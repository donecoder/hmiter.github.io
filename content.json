{"pages":[{"title":"黄敏","text":"生于89年，男，湖南人，毕业于吉首大学，本科学历。现就职于芒果TV，主要负责芒果多元化的开发管理工作。 主要从事Java开发，了解Python、PHP、NodeJS，写过shell和python脚本。独立开发过微信、安卓应用，擅长后端开发，小至SSH，大到分布式，技术涉略比较杂，也算一个小小全栈吧！在管理方面只能说是马马虎虎吧！负责开发过几个项目，成功上线6、7个地方。 技术前端：HTML、CSS、JavaScript、Ajax、JQuery、EasyUI、bootstrap后端：Java、Struts2、Spring MVC、Spring、Mybatis、FreeMarker、Quartz、Dubbo、HSF(DEAS)、ActiveMQ、RabbitMQ、Netty、WebService、WebSocket、Spring boot、Spring Cloud数据库：Oracle、MySQL和Mongodb缓存：Ehcache、Memcached、Redis应用服务器：Jetty、Tomcat、JBoss、Weblogic、Apache、Ngnix等。构建：ANT、Maven、Gradle、SVN、Git运维：Linux、shell，VMware开发工具：Eclipse、IDEA、PLSQL Developer、PowerDesigner、Rose其他：Android、Python、PHP、公众号、小程序、第三方支付 经历2012.07 – 2013.11 科盟软件有限公司2013.11 – 2017.05 北京智慧眼科技股份有限公司2017.05 – 2017.11 长沙东东科技有限公司2017.11 – 至今 湖南快乐阳光互动娱乐传媒有限公司(芒果TV) 箴言业精于勤荒于嬉","link":"/about/index.html"}],"posts":[{"title":"Oracle在Linux下的安装","text":"安装环境：系统是 centos6.5","link":"/2017/03/18/Database/Oracle在Linux下的安装/"},{"title":"memcached在Linux下的安装","text":"安装环境：系统是 centos6.5 libevent安装打开memcached官网的下载界面http://memcached.org/downloads，看到有如下提示： Debian/Ubuntu: apt-get install libevent-dev Redhat/Centos: yum install libevent-devel 说明memcached依赖于libevent 。 yum安装1yum install libevent-devel 源码安装下载网址：http://libevent.org/123456#解压 tar -zxvf libevent-2.0.22-stable.tar.gz #配置 ./configure –prefix=/usr/local/libevent #安装 make &amp;&amp; make install 下载memcachedmemcached下载网址： http://memcached.org/downloads 或者 wget http://memcached.org/latest 解压该版本的memcached没有使用gzip压缩，所以不能加-g1tar -xvf memcached-1.4.25.tar.gz 编译通过 ./configure –help可以参考编译参数，可以看出，我们可以通过 –with-libevent来指定libevent安装目录。 编译命令如下：1./configure --prefix=/usr/local/memcached --with-libevent=/usr/local/libevent/ 安装执行如下命令1make &amp;&amp; make install 安装完成后，可以看到/usr/local目录下多了一个memcached目录。 启动1./memcached -vv -u nobody","link":"/2017/03/18/Database/memcached在Linux下的安装/"},{"title":"Redis在Linux下的安装","text":"基本知识1、 Redis的数据类型：字符串、列表（lists）、集合（sets）、有序集合（sorts sets）、哈希表（hashs） 2、 Redis和memcache相比的独特之处：（1）redis可以用来做存储（storge）、而memcache是来做缓存（cache）。这个特点主要是因为其有“持久化”功能（2）存储的数据有“结构”，对于memcache来说，存储的数据，只有一种类型——“字符串”，而redis则可以存储字符串、链表、集合、有序集合、哈序结构 3、 持久化的两种方式：Redis将数据存储于内存中，或被配置为使用虚拟内存。实现数据持久化的两种方式：（1）使用截图的方式，将内存中的数据不断写入磁盘（性能高，但可能会引起一定程度的数据丢失）（2）使用类似mysql的方式，记录每次更新的日志 4、 Redis的主从同步：对提高读取性能非常有益 5、Redis服务端的默认端口是6379 下载安装环境：系统是 centos6.5 下载地址：http://download.redis.io/releases/redis-3.2.1.tar.gz上传到Linux。 解压1tar -zxvf redis-3.2.1.tar.gz 编译源程序123456cd redis-3.2.1make#编译完成之后cd srcmake install PREFIX=/usr/local/redis 配置文件1cp redis.conf /usr/local/redis/bin/ 服务启动和停止12cd /usr/local/redis/bin/./redis-server redis.conf 默认情况，Redis不是在后台运行，我们需要把redis放在后台运行123vim redis.conf#将daemonize的值改为yesdaemonize yes 客户端连接1./redis-cli 停止服务1./redis-cli shutdown redis开机自启123vim /etc/rc.local#加入redis启动脚本/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.conf 其他redis-benchmark：redis性能测试工具 redis-check-aof：检查aof日志的工具 redis-check-dump：检查rdb日志的工具 redis-cli：连接用的客户端 redis-server：redis服务进程 Redis的配置daemonize：如需要在后台运行，把该项的值改为yes pdifile：把pid文件放在/var/run/redis.pid，可以配置到其他地址 bind：指定redis只接收来自该IP的请求，如果不设置，那么将处理所有请求，在生产环节中最好设置该项 port：监听端口，默认为6379 timeout：设置客户端连接时的超时时间，单位为秒 loglevel：等级分为4级，debug，revbose，notice和warning。生产环境下一般开启notice logfile：配置log文件地址，默认使用标准输出，即打印在命令行终端的端口上 database：设置数据库的个数，默认使用的数据库是0 save：设置redis进行数据库镜像的频率 rdbcompression：在进行镜像备份时，是否进行压缩 dbfilename：镜像备份文件的文件名 dir：数据库镜像备份的文件放置的路径 slaveof：设置该数据库为其他数据库的从数据库 masterauth：当主数据库连接需要密码验证时，在这里设定 requirepass：设置客户端连接后进行任何其他指定前需要使用的密码 maxclients：限制同时连接的客户端数量 maxmemory：设置redis能够使用的最大内存 appendonly：开启appendonly模式后，redis会把每一次所接收到的写操作都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态 appendfsync：设置appendonly.aof文件进行同步的频率 vm_enabled：是否开启虚拟内存支持 vm_swap_file：设置虚拟内存的交换文件的路径 vm_max_momery：设置开启虚拟内存后，redis将使用的最大物理内存的大小，默认为0 vm_page_size：设置虚拟内存页的大小 vm_pages：设置交换文件的总的page数量 vm_max_thrrads：设置vm IO同时使用的线程数量","link":"/2017/03/18/Database/Redis在Linux下的安装/"},{"title":"Oracle查看表空间常用语句","text":"查看表空间使用率12345678910111213select a.tablespace_name, a.bytes / 1024 / 1024 \"Sum MB\", (a.bytes - b.bytes) / 1024 / 1024 \"used MB\", b.bytes / 1024 / 1024 \"free MB\", round(((a.bytes - b.bytes) / a.bytes) * 100, 2) \"percent_used\" from (select tablespace_name, sum(bytes) bytes from dba_data_files group by tablespace_name) a, (select tablespace_name, sum(bytes) bytes, max(bytes) largest from dba_free_space group by tablespace_name) b where a.tablespace_name = b.tablespace_name order by ((a.bytes - b.bytes) / a.bytes) desc 查看用户、表空间使用率123456789101112select a.username,b.* from dba_users a left join (select a.tablespace_name, a.bytes / 1024 / 1024 \"Sum MB\", (a.bytes - b.bytes) / 1024 / 1024 \"used MB\", b.bytes / 1024 / 1024 \"free MB\", round(((a.bytes - b.bytes) / a.bytes) * 100, 2) \"percent_used\" from (select tablespace_name, sum(bytes) bytes from dba_data_files group by tablespace_name) a, (select tablespace_name, sum(bytes) bytes, max(bytes) largest from dba_free_space group by tablespace_name) b where a.tablespace_name = b.tablespace_name) b on a.default_tablespace = b.tablespace_name; 删除用户和数据文件12345--步骤一： 删除userdrop user ×× cascade--说明： 删除了user，只是删除了该user下的schema objects，是不会删除相应的tablespace的。--步骤二： 删除tablespaceDROP TABLESPACE tablespace_name INCLUDING CONTENTS AND DATAFILES; 改变数据文件大小1alter database datafile '/u01/oracle/oradata/aeye/sysaux01.dbf' resize 2G;","link":"/2017/05/30/Database/Oracle查看表空间常用语句/"},{"title":"mysql事件（计划任务）","text":"例子：123456789101112131415161718192021222324252627282930313233343536373839DELIMITER $$-- 1、设置 全局变量event_scheduler开启，使用event必须开启-- SET GLOBAL event_scheduler = ON$$ -- required for event to execute but not create CREATE /*[DEFINER = { user | CURRENT_USER }]*/ EVENT `start_distribution`.`event_account_checking` ON SCHEDULE EVERY 1 DAY STARTS '2017-09-04 06:00:00' ON COMPLETION NOT PRESERVE ENABLE COMMENT '每天自动统计对账数据' /* uncomment the example below you want to use */ -- scheduleexample 1: run once 执行一次 -- AT 'YYYY-MM-DD HH:MM.SS'/CURRENT_TIMESTAMP { + INTERVAL 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] } -- scheduleexample 2: run at intervals forever after creation 创建后每多久执行一次 -- EVERY 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] -- scheduleexample 3: specified start time, end time and interval for execution 指定开始开始、结束时间和间隔时间执行任务。 /*EVERY 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] STARTS CURRENT_TIMESTAMP/'YYYY-MM-DD HH:MM.SS' { + INTERVAL 1[HOUR|MONTH|WEEK|DAY|MINUTE|...] } ENDS CURRENT_TIMESTAMP/'YYYY-MM-DD HH:MM.SS' { + INTERVAL 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] } */ /*[ON COMPLETION [NOT] PRESERVE] -- 执行后删除(NOT)还是保留 [ENABLE | DISABLE] -- 创建时，event的章台 [COMMENT 'comment']*/ -- 注释DO BEGIN call pro_account_checking(); END$$DELIMITER ;","link":"/2017/09/04/Database/mysql事件（计划任务）/"},{"title":"mysql存储过程","text":"简介存储过程是可编程的函数，在数据库中创建并保存，可以由SQL语句和控制结构组成。当想要在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟，它允许控制数据的访问方式。 存储过程是数据库的一个重要的功能，MySQL 5.0以前并不支持存储过程，这使得MySQL在应用上大打折扣。好在MySQL 5.0开始支持存储过程，这样即可以大大提高数据库的处理速度，同时也可以提高数据库编程的灵活性。 存储过程的优点(1).增强SQL语言的功能和灵活性：存储过程可以用控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。 (2).标准组件式编程：存储过程被创建后，可以在程序中被多次调用，而不必重新编写该存储过程的SQL语句。而且数据库专业人员可以随时对存储过程进行修改，对应用程序源代码毫无影响。 (3).较快的执行速度：如果某一操作包含大量的Transaction-SQL代码或分别被多次执行，那么存储过程要比批处理的执行速度快很多。因为存储过程是预编译的。在首次运行一个存储过程时查询，优化器对其进行分析优化，并且给出最终被存储在系统表中的执行计划。而批处理的Transaction-SQL语句在每次运行时都要进行编译和优化，速度相对要慢一些。 (4).减少网络流量：针对同一个数据库对象的操作（如查询、修改），如果这一操作所涉及的Transaction-SQL语句被组织进存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大减少网络流量并降低了网络负载。 (5).作为一种安全机制来充分利用：通过对执行某一存储过程的权限进行限制，能够实现对相应的数据的访问权限的限制，避免了非授权用户对数据的访问，保证了数据的安全。 例子1234567891011121314151617181920212223242526272829303132333435363738394041424344DELIMITER $$ USE `start_user`$$ DROP PROCEDURE IF EXISTS `rob_test`$$ CREATE /*[DEFINER = { user | CURRENT_USER }]*/ -- 0、定义存储过程名称和参数、返回值 -- in 为输入参数；out 为返回值，inout 即是参数也是返回值 PROCEDURE `start_user`.`rob_test`( out msg VARCHAR(50) ) /* LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT 'string' */ BEGIN -- 1、声明变量 DECLARE userId int; DECLARE i int; DECLARE phoneId BIGint; -- 2、赋值，也可以用DEFAULT，如：DECLARE i int DEFAULT 0; SET i=0; SET phoneId= 11010000002; -- 3、业务逻辑，此处用循环插入数据 while i&lt;5 do INSERT INTO `start_user`.`user` (`nickName`, `gender`, `mobile`, `signatureText`, `avatarUrl`, `userType`, `isUserGroup`, `status`, `channel`, `inviteGuestPermission`, `reported`, `flag`, `appId`, `loginBundleId`) VALUES(CONCAT('机器人',i),'MALE',CONCAT(phoneId,''),NULL,NULL,'NORMAL',NULL,'1',NULL,NULL,NULL,NULL,NULL,NULL); SELECT LAST_INSERT_ID() INTO userId ; insert into `start_distribution`.`dis_org_startuser` ( `orgId`, `agentId`, `cooperatorId`, `opcenterId`, `startUserId`, `startUserType`, `inviteUserId`) values('999992348','999992347','999992346','999992345',userId,'ANCHOR',NULL); set i=i+1; set phoneId= phoneId+1; end while; -- 4、结束循环，设定返回值 set msg ='执行成功'; END$$DELIMITER ; 语法CREATE PROCEDURE 过程名([[IN|OUT|INOUT] 参数名 数据类型[,[IN|OUT|INOUT] 参数名 数据类型…]]) [特性 …] 过程体 结束符DELIMITER 定义语句结束符。首先将结束符变为“$$”,在完成存储过程之后再将结束符改为默认的“;”。 参数存储过程根据需要可能会有输入、输出、输入输出参数，如果有多个参数用”,”分割开。MySQL存储过程的参数用在存储过程的定义，共有三种参数类型,IN,OUT,INOUT: IN：参数的值必须在调用存储过程时指定，在存储过程中修改该参数的值不能被返回，为默认值OUT:该值可在存储过程内部被改变，并可返回INOUT:调用时指定，并且可被改变和返回 过程体过程体的开始与结束使用BEGIN与END进行标识。 变量赋值语法：SET 变量名 = 变量值 [,变量名= 变量值 …] 用户变量用户变量一般以@开头注意：滥用用户变量会导致程序难以理解及管理 注释MySQL存储过程可使用两种风格的注释：双杠：–，该风格一般用于单行注释C风格： 一般用于多行注释 调用用call和你过程名以及一个括号，括号里面根据需要，加入参数，参数包括输入参数、输出参数、输入输出参数。 控制语句条件语句IF-THEN-ELSE语句1234567891011121314151617DROP PROCEDURE IF EXISTS proc3;DELIMITER //CREATE PROCEDURE proc3(IN parameter int) BEGIN DECLARE var int; SET var=parameter+1; IF var=0 THEN INSERT INTO t VALUES (17); END IF ; IF parameter=0 THEN UPDATE t SET s1=s1+1; ELSE UPDATE t SET s1=s1+2; END IF ; END ; //DELIMITER ; CASE-WHEN-THEN-ELSE语句12345678910111213141516DELIMITER // CREATE PROCEDURE proc4 (IN parameter INT) BEGIN DECLARE var INT; SET var=parameter+1; CASE var WHEN 0 THEN INSERT INTO t VALUES (17); WHEN 1 THEN INSERT INTO t VALUES (18); ELSE INSERT INTO t VALUES (19); END CASE ; END ; //DELIMITER ; 循环语句WHILE-DO…END-WHILE123456789101112DELIMITER // CREATE PROCEDURE proc5() BEGIN DECLARE var INT; SET var=0; WHILE var&lt;6 DO INSERT INTO t VALUES (var); SET var=var+1; END WHILE ; END; //DELIMITER ; REPEAT…END REPEAT此语句的特点是执行操作后检查结果12345678910111213DELIMITER // CREATE PROCEDURE proc6 () BEGIN DECLARE v INT; SET v=0; REPEAT INSERT INTO t VALUES(v); SET v=v+1; UNTIL v&gt;=5 END REPEAT; END; //DELIMITER ; LOOP…END LOOP12345678910111213141516171819202122232425DELIMITER // CREATE PROCEDURE proc7 () BEGIN DECLARE v INT; SET v=0; LOOP_LABLE:LOOP INSERT INTO t VALUES(v); SET v=v+1; IF v &gt;=5 THEN LEAVE LOOP_LABLE; END IF; END LOOP; END; //DELIMITER ;``` #### LABLES标号标号可以用在begin repeat while 或者loop 语句前，语句标号只能在合法的语句前面使用。可以跳出循环，使运行指令达到复合语句的最后一步。#### ITERATE迭代通过引用复合语句的标号,来从新开始复合语句### ITERATE DELIMITER // CREATE PROCEDURE proc8() BEGIN DECLARE v INT; SET v=0; LOOP_LABLE:LOOP IF v=3 THEN SET v=v+1; ITERATE LOOP_LABLE; END IF; INSERT INTO t VALUES(v); SET v=v+1; IF v&gt;=5 THEN LEAVE LOOP_LABLE; END IF; END LOOP; END; //DELIMITER ;```","link":"/2017/08/18/Database/mysql存储过程/"},{"title":"Java配置中心","text":"阿里：Diamondhttps://github.com/hengyunabc/xdiamondhttps://yq.aliyun.com/articles/6058 携程：Apollohttps://github.com/ctripcorp/apollo/wiki/Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%BB%8B%E7%BB%8Dhttps://github.com/ctripcorp/apollo/wiki/Quick-Start Spring: Spring Cload Confighttps://github.com/spring-cloud/spring-cloud-config","link":"/2017/08/08/Java/Java配置中心/"},{"title":"mysql在Linux下的安装","text":"安装环境：系统是 centos6.5 下载下载地址：http://dev.mysql.com/downloads/mysql/5.6.html#downloads下载版本：我这里选择的5.6.33，通用版，linux下64位 也可以直接复制64位的下载地址，通过命令下载：wget http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz 解压1234#解压tar -zxvf mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz#复制解压后的mysql目录cp -r mysql-5.6.33-linux-glibc2.5-x86_64 /usr/local/mysql 添加用户组和用户1234#添加用户组groupadd mysql#添加用户mysql 到用户组mysqluseradd -g mysql mysql 安装1234567891011121314151617181920212223242526272829303132cd /usr/local/mysql/mkdir ./data/mysqlchown -R mysql:mysql ././scripts/mysql_install_db --user=mysql --datadir=/usr/local/mysql/data/mysqlcp support-files/mysql.server /etc/init.d/mysqldchmod 755 /etc/init.d/mysqldcp support-files/my-default.cnf /etc/my.cnf#修改启动脚本vi /etc/init.d/mysqld #修改项：basedir=/usr/local/mysql/datadir=/usr/local/mysql/data/mysql #启动服务service mysqld start #测试连接./mysql/bin/mysql -uroot #加入环境变量，编辑 /etc/profile，这样可以在任何地方用mysql命令了export PATH=$PATH:/usr/local/mysql//binsource /etc/profile #启动mysqlservice mysqld start#关闭mysqlservice mysqld stop#查看运行状态service mysqld status 错误sqlyog连接时，报1130错误.是由于没有给远程连接的用户权限问题解决1:更改 ‘mysql’数据库‘user’表‘host’项，从‘localhost’改成‘%’。1234use mysql;select 'host' from user where user='root'; update user set host = '%' where user ='root';flush privileges; 解决2：直接授权1GRANT ALL PRIVILEGES ON *.* TO ‘root’@'%’ IDENTIFIED BY ‘youpassword’ WITH GRANT OPTION; 安装时的一些错误-bash: ./scripts/mysql_install_db: /usr/bin/perl: bad interpreter: 没有那个文件或目录1解决： yum -y install perl perl-devel Installing MySQL system tables…./bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory1解决：yum -y install libaio-devel #其他 配置环境变量12vi + /etc/profileexport PATH=....:/usr/local/mysql/bin Linux表名大小写问题编辑/etc/my.cnf,在[mysqld]下面添加如下：12[mysqld]lower_case_table_names=1 乱码问题编辑/etc/my.cnf,在[mysqld]下面添加如下：123[mysqld]...character-set-server=utf8","link":"/2017/03/18/Database/mysql在Linux下的安装/"},{"title":"FreeMarker简单介绍","text":"","link":"/2017/05/30/Java/FreeMarker简单介绍/"},{"title":"Velocity简单介绍","text":"","link":"/2017/05/30/Java/Velocity简单介绍/"},{"title":"Logger4j详解","text":"一、介绍Log4j是Apache的一个开源项目，通过使用Log4j，我们可以控制日志信息输送的目的地是控制台、文件、GUI组件、甚至是套接口服务 器、NT的事件记录器、UNIX Syslog守护进程等；我们也可以控制每一条日志的输出格式；通过定义每一条日志信息的级别，我们能够更加细致地控制日志的生成过程。 Log4j由三个重要的组件构成：日志信息的优先级（Loggers），日志信息的输出目的地（Appenders），日志信息的输出格式（Layouts）。日志信息的优先级从高到低有ERROR、WARN、 INFO、DEBUG，分别用来指定这条日志信息的重要程度；日志信息的输出目的地指定了日志将打印到控制台还是文件中；而输出格式则控制了日志信息的显示内容。 二、配置文件其实您也可以完全不使用配置文件，而是在代码中配置Log4j环境。但是，使用配置文件将使您的应用程序更加灵活。Log4j支持两种配置文件格式，一种是XML格式的文件，一种是properties格式的文件。下面我们介绍使用properties格式做为配置文件的方法：示例：log4j.rootLogger=INFO, A1log4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-4r %-5p [%t] %37c %3x - %m%n 1. 配置根Logger，其语法为：log4j.rootLogger = [ level ] , appenderName, appenderName, …其中，level 是日志记录的优先级，分为OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL或者您定义的级别。Log4j建议只使用四个级别，优先级从高到低分别是ERROR、WARN、INFO、DEBUG。通过在这里定义的级别，您可以控制到应用程序中相应级别的日志信息的开关。比如在这里定义了INFO级别，则应用程序中所有DEBUG级别的日志信息将不被打印出来。 appenderName就是指定日志信息输出到哪个地方。您可以同时指定多个输出目的地。 2. 配置日志信息输出目的地Appender，其语法为：log4j.appender.appenderName = package+appender_class_namelog4j.appender.appenderName.option1 = value1…log4j.appender.appenderName.option = valueN其中，Log4j提供的appender有以下几种：org.apache.log4j.ConsoleAppender（控制台），org.apache.log4j.FileAppender（文件），org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件），org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件），org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方) org.apache.log4j.jdbc.JDBCAppender(将日志写入数据库) (1).ConsoleAppender选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。Target=System.err：默认情况下是：System.out,指定输出控制台(2).FileAppender 选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。(3).DailyRollingFileAppender 选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。DatePattern=’.’yyyy-ww:每周滚动一次文件，即每周产生一个新的文件。当然也可以指定按月、周、天、时和分。即对应的格式如下：1)’.’yyyy-MM: 每月2)’.’yyyy-ww: 每周3)’.’yyyy-MM-dd: 每天4)’.’yyyy-MM-dd-a: 每天两次5)’.’yyyy-MM-dd-HH: 每小时6)’.’yyyy-MM-dd-HH-mm: 每分钟(4).RollingFileAppender 选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。MaxFileSize=100KB: 后缀可以是KB, MB 或者是 GB. 在日志文件到达该大小时，将会自动滚动，即将原来的内容移到mylog.log.1文件。MaxBackupIndex=2:指定可以产生的滚动文件的最大数。 (5). JDBCApperder选项 URL=jdbc:mysql://localhost:3306/test：指定日志写入的数据库链接driver=com.mysql.jdbc.Driver：指定数据库驱动user=root：指定数据库的用户名password=123：指定数据库的登录密码sql=insert into tb_log (message) values(‘=[%-5p] %d(%r) –&gt; [%t] %l: %m %x %n’)：指定写入数据库的执行语句 3. 配置日志信息的布局，其语法为：log4j.appender.appenderName.layout = package+layout_class_namelog4j.appender.appenderName.layout.option1 = value1…log4j.appender.appenderName.layout.option = valueN其中，Log4j提供的layout有以下几种：org.apache.log4j.HTMLLayout（以HTML表格形式布局），org.apache.log4j.PatternLayout（可以灵活地指定布局模式），org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串），org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息） 4、输出格式设置在配置文件中可以通过log4j.appender.A1.layout.ConversionPattern设置日志输出格式。参数：%p: 输出日志信息优先级，即DEBUG，INFO，WARN，ERROR，FATAL,%d: 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，比如：%d{yyy MMM dd HH:mm:ss,SSS}，输出类似：2002年10月18日 22：10：28，921 %r: 输出自应用启动到输出该log信息耗费的毫秒数%c: 输出日志信息所属的类目，通常就是所在类的全名%t: 输出产生该日志事件的线程名%l: 输出日志事件的发生位置，相当于%C.%M(%F:%L)的组合,包括类目名、发生的线程，以及在代码中的行数。举例：Testlog4.main(TestLog4.java:10) %x: 输出和当前线程相关联的NDC(嵌套诊断环境),尤其用到像java servlets这样的多客户多线程的应用中。%%: 输出一个”%”字符%F: 输出日志消息产生时所在的文件名称%L: 输出代码中的行号%m: 输出代码中指定的消息,产生的日志具体信息%n: 输出一个回车换行符，Windows平台为”\\r\\n”，Unix平台为”\\n”输出日志信息换行可以在%与模式字符之间加上修饰符来控制其最小宽度、最大宽度、和文本的对齐方式。如：1)%20c：指定输出category的名称，最小的宽度是20，如果category的名称小于20的话，默认的情况下右对齐。2)%-20c:指定输出category的名称，最小的宽度是20，如果category的名称小于20的话，”-”号指定左对齐。3)%.30c:指定输出category的名称，最大的宽度是30，如果category的名称大于30的话，就会将左边多出的字符截掉，但小于30的话也不会有空格。4)%20.30c:如果category的名称小于20就补空格，并且右对齐，如果其名称长于30字符，就从左边交远销出的字符截掉。 三、如何在不同的模块中输出不同的日志用户基础信息模块路径为：com.test.user它下面有个类：com.test.user.service.impl.UserInfoprivate Log log = LogFactory.getLog(UserInfo.class); 方法1：在log4j.properties中加入:log4j.logger.com.test.user=info,userLog,stdoutlog4j.appender.userLog=org.apache.log4j.FileAppenderlog4j.appender.userLog.File=../logs/userinfo.loglog4j.appender.userLog.Append=truelog4j.appender.userLog.Threshold=infolog4j.appender.userLog.layout=org.apache.log4j.PatternLayoutlog4j.appender.userLog.layout.ConversionPattern==%d %p [%c] - %m%n 注：也就是让com.test.user模块下所有的logger使用log4j.appender.userLog所做的配置。 方法2：自定义“别名”private Log log = LogFactory.getLog(“userInfoLog”);然后在log4j.properties中加入:log4j.logger.userInfoLog=info,userLog,stdoutlog4j.appender.userLog=org.apache.log4j.FileAppenderlog4j.appender.userLog.File=../logs/userinfo.loglog4j.appender.userLog.Append=truelog4j.appender.userLog.Threshold=infolog4j.appender.userLog.layout=org.apache.log4j.PatternLayoutlog4j.appender.userLog.layout.ConversionPattern==%d %p [%c] - %m%n 注：也就是在用logger时给它一个自定义的名字(如这里的”userInfoLog”)，然后在log4j.properties中做出相应配置即可。，在这种模式下，即使在同一个类中也能定义多个不同输出的log. 在类中调用代码如下：private Log loggerError = LogFactory.getLog(“userErrorLog”);private Log loggerInfo = LogFactory.getLog(“userInfoLog”); 自定义的日志默认是同时输出到log4j.rootLogger所配置的日志中的，如何能只让它们输出到自己指定的日志中呢？log4j.additivity.userInfoLog = false它用来设置是否同时输出到log4j.rootLogger所配置的日志中，设为false就不会输出到其它地方啦！注意这里的”userInfoLog”是你在程序中给logger起的那个自定义的名字！如果你说，我只是不想同时输出这个日志到log4j.rootLogger所配置的logfile中，stdout里我还想同时输出呢！如：log4j.logger.userInfoLog=DEBUG, userLog, stdout 三、加载log4j.properties文件1、spring方式加载，配置与web.xml中：Spring加载log4j.properties，它提供了一个Log4jConfigListener，本身就能通过web.xml配置从指定位置加载log4j配置文件和log4j的输出路径，要注意的是 Log4jConfigListener必须要在Spring的Listener之前。 web.xml12345678910111213141516171819202122232425&lt;!-- 设置由Spring载入的Log4j配置文件位置 --&gt;&lt;context-param&gt;&lt;param-name&gt;log4jConfigLocation&lt;/param-name&gt;&lt;param-value&gt;WEB-INF/classes/log4j.properties&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- Spring刷新Log4j配置文件变动的间隔,单位为毫秒 --&gt;&lt;context-param&gt;&lt;param-name&gt;log4jRefreshInterval&lt;/param-name&gt;&lt;param-value&gt;10000&lt;/param-value&gt;&lt;/context-param&gt;&lt;listener&gt;&lt;listener-class&gt;org.springframework.web.util.Log4jConfigListener&lt;/listener-class&gt;&lt;/listener&gt; 2、可以通过资源类对资源文件进行加载，与使用为一体123456789public class Logger4JTest { public static void main(String[] args) { PropertyConfigurator.configure(\" D:/log/log4j.properties \"); Logger logger = Logger.getLogger(Logger4JTest.class); logger.debug(\" debug \"); logger.error(\" error \"); } } 四、在程序中的使用在程序中使用Log4j之前，首先要将commons-logging.jar和logging-log4j-1.2.9.jar导入到classpath中，并将log4j.properties放于src根目录中。接下来就可以使用了。 1.得到记录器使用Log4j，第一步就是获取日志记录器，这个记录器将负责控制日志信息。其语法为：public static Logger getLogger( String name)，通过指定的名字获得记录器，如果必要的话，则为这个名字创建一个新的记录器。Name一般取本类的名字，比如：static Logger logger = Logger.getLogger ( ServerWithLog4j.class.getName () ) ; 2.插入记录信息（格式化日志信息）当上两个必要步骤执行完毕，您就可以轻松地使用不同优先级别的日志记录语句插入到您想记录日志的任何地方，其语法如下：logger.debug ( Object message ) ;logger.info ( Object message ) ;logger.warn ( Object message ) ;logger.error ( Object message ) ;","link":"/2017/12/27/Java/Logger4j详解/"},{"title":"lambda与函数式","text":"前言Lambda表达式是Java SE 8才引进的新特性。对于只申明一个函数的接口，它提供了一个简单和简洁的编写方式。 语法1(参数...) -&gt; { 代码块 } 有三种格式：123(params) -&gt; expression(params) -&gt; statement(params) -&gt; { statements } 函数式接口像Comparator这样的只有一个抽象方法的接口，叫做函数式接口（Functional Interface）。与Comparator类似，其他函数式接口的唯一的抽象方法也可以用lambda来表示。 我们看一下Comparator的源码，发现其多了一个@FunctionalInterface的注解，用来表明它是一个函数式接口。标记了该注解的接口有且仅有一个抽象方法，否则会报编译错误。 再看一下其他的仅有一个抽象方法的接口，比如Runnable和Callable，发现也都在Java 8之后加了@FunctionalInterface注解。对于Runnable来说，接口定义如下：1234@FunctionalInterfacepublic interface Runnable { public abstract void run();} 不难推测，其lambda的写法应该是 () -&gt; { body }，它不接收任何参数，方法体中也无return返回值，用起来像这样：1new Thread(() -&gt; {doSomething();}); 此外，随lambda一同增加的还有一个java.util.function包，其中定义了一些常见的函数式接口的。比如： Function，接受一个输入参数，返回一个结果。参数与返回值的类型可以不同，我们之前的map方法内的lambda就是表示这个函数式接口的； Consumer，接受一个输入参数并且无返回的操作。比如我们针对数据流的每一个元素进行打印，就可以用基于Consumer的lambda； Supplier，无需输入参数，只返回结果。看接口名就知道是发挥了对象工厂的作用； Predicate，接受一个输入参数，返回一个布尔值结果。比如我们在对数据流中的元素进行筛选的时候，就可以用基于Predicate的lambda； … 例子Runnable Lambda我们可以使用Lambda表达式写一个Runnable测试程序：123456789101112131415161718public class RunnableTest { public static void main(String[] args) { //匿名内部类 Runnable r1 = new Runnable() { @Override public void run() { System.out.println(\"hello Runnable 1!\"); } }; //Lambda Runnable r2 = ()-&gt; System.out.println(\"hello Runnable 2\"); r1.run(); r2.run(); }} Comparator Lambda下面是java.util.Comparator的例子： 1234567891011enum Gender { MALE, FEMALE }public class Person { private String givenName; private String surName; private int age; private Gender gender; private String eMail; private String phone; private String address;} 123456789101112131415161718192021222324252627282930313233import java.util.Collections;import java.util.Comparator;import java.util.List;public class ComparatorTest { public static void main(String[] args) { List&lt;Person&gt; personList = Person.createShortList(); //匿名内部类 Collections.sort(personList, new Comparator&lt;Person&gt;() { @Override public int compare(Person o1, Person o2) { return o1.getSurName().compareTo(o2.getSurName()); } }); for(Person p: personList){ p.printName(); } //lambda 1 Collections.sort(personList, (Person o1, Person o2)-&gt;o1.getSurName().compareTo(o2.getSurName())); for(Person p: personList){ p.printName(); } //lambda 2 Collections.sort(personList, (o1, o2)-&gt;o1.getSurName().compareTo(o2.getSurName())); for(Person p: personList){ p.printName(); } }} 从上面的lambda 1和lambda 2中的参数可以看到，我们传入的o1, o2可以不用指定它的类型，编译器能够自动判断。（为什么？java.lang.Comparator接口只有一个方法） Listener Lambda最后，我们再看一下ActionListenter的例子： 123456789101112131415161718192021222324import javax.swing.*;import java.awt.*;import java.awt.event.ActionEvent;import java.awt.event.ActionListener;public class ListenerTest { public static void main(String[] args) { JButton testButton = new JButton(\"button\"); testButton.addActionListener(new ActionListener() { @Override public void actionPerformed(ActionEvent e) { System.out.println(\"click button, 匿名内部类\"); } }); testButton.addActionListener(event -&gt; System.out.println(\"click button, lambda\")); JFrame frame = new JFrame(\"test\"); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.add(testButton, BorderLayout.CENTER); frame.pack(); frame.setVisible(true); }} 通过上面的程序我们看到，lambda表达式作为一个参数传进方法中。","link":"/2018/11/13/Java/lambda与函数式/"},{"title":"NIO框架的一点想法","text":"不管是什么NIO框架。本身其实都是对Java底层的一种在封装。封装一套更简便，更易于扩展的一套东西以方便开发者使用。所以性能上也许会有所差异，但是绝对没有java和C++之间这么多。(代码写的太烂的除外，不过想要使用java写出很烂的代码也比较困难。)这些框架在性能方面差别不会超过1%。 Mina和Netty开始。因为这两个NIO框架的创作者是同一个人Trustin Lee （韩国人）。GitHub主页地址 ：https://github.com/trustin。尽管创作者现在已经不专注与开发了。但是框架的后续开发和继承，可以说都是符合最开始的设定的。两个框架的架构设计思路基本一致。 Netty从某种程度上讲是Mina的延伸和扩展。解决了一些Mina上的设计缺陷，也优化了一下Mina上面的设计理念。 另一方面Netty相比较Mina更容易学习。API更简单。详细的范例源码和API文档。更活跃的论坛和社区。更高的代码更新维护速度。 我想不出什么理由来不选择Netty。 xSocket：是一个轻量级的基于nio的服务器框架用于开发高性能、可扩展、多线程的服务器。该框架封装了线程处理、异步读/写等方面。（只是对Java的NIO做了最简单的封装，以便于开发使用。） Grizzly ： 是一种应用程序框架，专门解决编写成千上万用户访问服务器时候产生的各种问题。使用JAVA NIO作为基础，并隐藏其编程的复杂性。容易使用的高性能的API。带来非阻塞socketd到协议处理层。利用高性能的缓冲和缓冲管理使用高性能的线程池。","link":"/2017/12/27/Java/NIO框架的一点想法/"},{"title":"Android常用部署属性","text":"第一类：通用属性1234567891011android:id 设置IDandroid:layout_width 该组件的宽度,wrap_content|fill_parent|match_parentandroid:layout_height 该组件的高度，wrap_content|fill_parent|match_parentandroid:layout_gravity 该组件在父组件的对其方式，bottom|left|top|rightandroid:orientation 布局组件的内部子组件的排列方式，horizontal|verticalandroid:gravity 布局组件的内部子组件的对其方式,bottom|left|top|rightandroid:text 设置显示的文本内容，通过、string.xml文件引用android:textColor 设置字体颜色，通过colors.xml资源来引用android:textStyle 设置字体风格，normal(无效果)|bold(加粗)|italic(斜体)android:textSize 字体大小，单位一般是用sp！android:background 设置主键的背景图片 第二类:属性值为true或false12345678android:layout_centerHrizontal 水平居中android:layout_centerVertical 垂直居中android:layout_centerInparent 相对于父元素完全居中。android:layout_alignParentBottom 贴紧父元素的下边缘android:layout_alignParentLeft 贴紧父元素的左边缘android:layout_alignParentRight 贴紧父元素的右边缘android:layout_alignParentTop 贴紧父元素的上边缘android:layout_alignWithParentIfMissing 如果对应的兄弟元素找不到的话就以父元素做参照物 第三类：属性值必须为id的引用名“@id/id-name”12345678android:layout_below 在某元素的下方android:layout_above 在某元素的的上方android:layout_toLeftOf 在某元素的左边android:layout_toRightOf 在某元素的右边android:layout_alignTop 本元素的上边缘和某元素的的上边缘对齐android:layout_alignLeft 本元素的左边缘和某元素的的左边缘对齐android:layout_alignBottom 本元素的下边缘和某元素的的下边缘对齐android:layout_alignRight 本元素的右边缘和某元素的的右边缘对齐 第四类：属性值为具体的像素值，如30dip，40px12345678910android:layout_margin 离某元素上下左右的的距离android:layout_marginTop 离某元素上边缘的距离android:layout_marginRight 离某元素右边缘的距离android:layout_marginBottom 离某元素底边缘的距离android:layout_marginLeft 离某元素左边缘的距离android:padding 指该控件内部内容距离该控件上下左右边缘的边距android:paddingTop 指该控件内部内容距离该控件上边缘的边距android:paddingRight 指该控件内部内容距离该控件左边缘的边距android:paddingBottom 指该控件内部内容距离该控件下边缘的边距android:paddingLeft 指该控件内部内容距离该控件右边缘的边距","link":"/2017/03/18/Android/Android常用部署属性/"},{"title":"Linux时间和时区","text":"如果你的 Linux 系统时区配置不正确，必需要手动调整到正确的当地时区。NTP 对时间的同步处理只计算当地时间与 UTC 时间的偏移量，因此配置一个 NTP 对时间进行同步并不能解决时区不正确的问题。所以大家在用了国外云计算服务商如 Microsoft Azure 或其它 VPS、虚拟机时，需要注意是否与中国大陆的时区一致。 时间123456# 查询时间date# 修改时间date -s \"2018-01-03 15:36:25\"# 查看时区时间ls -l /etc/localtime 时区/etc/localtime是用来描述本机时间，而 /etc/timezone是用来描述本机所属的时区.1234# 修改时区tzselect# 查看时区timedatectl Linux 用户一个多用户系统，每个用户都可以配置自己所需的时区，你可以为自己新增一个 TZ 环境变量：1export TZ='Asia/Shanghai' 执行完成之后需要重新登录系统或刷新 ~/.bashrc 生效。1source ~/.bashrc 更改Linux系统时区要更改 Linux 系统整个系统范围的时区可以使用如下命令：12sudo rm -f /etc/localtimesudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 注意：/usr/share/zoneinfo/Asia/Shanghai 中的具体时区请用自己获取到的 TZ 值进行替换。","link":"/2018/01/03/Linux/Linux时间和时区/"},{"title":"Spring常用注解","text":"Java原生注解@Retention：注解说明,这种类型的注解会被保留到那个阶段1.RetentionPolicy.SOURCE —— 这种类型的Annotations只在源代码级别保留,编译时就会被忽略2.RetentionPolicy.CLASS —— 这种类型的Annotations编译时被保留,在class文件中存在,但JVM将会忽略3.RetentionPolicy.RUNTIME —— 这种类型的Annotations将被JVM保留,所以他们能在运行时被JVM或其他使用反射机制的代码所读取和使用. @Documented：作用是在生成javadoc文档的时候将该注解也写入到文档中。javadoc默认是不会将注解写到文档中的。 @Target:注解的使用的目标 @Target(ElementType.TYPE) //接口、类、枚举、注解 @Target(ElementType.FIELD) //字段、枚举的常量 @Target(ElementType.METHOD) //方法 @Target(ElementType.PARAMETER) //方法参数 @Target(ElementType.CONSTRUCTOR) //构造函数 @Target(ElementType.LOCAL_VARIABLE)//局部变量 @Target(ElementType.ANNOTATION_TYPE)//注解 @Target(ElementType.PACKAGE) ///包 @Inherited：它指明被注解的类的所有属性将会自动继承到它的子类中. #java EE规范@Resource：为目标bean指定协作者Bean@Inject：为目标bean指定协作者Bean@Named：为目标bean指定协作者Bean@Qualifier：@PostConstruct：在构造函数执行完成之后执行。@PreDestory：bean销毁之前的执行方法。 Spring注解@Component :标注一个普通的spring Bean类@Service :标注一个业务逻辑组件类。@Repository :标注一个DAO组件类。@Controller:标注一个控制器组件类。 @EnableWebMvc：开启MVC 方法级别@ResetController：组合注解，组合了@Controller和@RequestBody@RequestMapping：配置URI和方法之间的映射 @GetMapping @PostMapping @PutMapping@RequestHeader 注解，可以把Request请求header部分的值绑定到方法的参数上。如：@RequestHeader(“Accept-Encoding”) String encoding@CookieValue 可以把Request header中关于cookie的值绑定到方法的参数上。如：@CookieValue(“JSESSIONID”) String cookie @CrossOrigin:实现跨域访问。 参数@PathVariable：接受路径参数，如someUrl/{paramId}, 这时的paramId可通过 @Pathvariable注解绑定它传过来的值到方法的参数上。@RequestParam：A） 常用来处理简单类型的绑定，通过Request.getParameter() 获取的String可直接转换为简单类型的情况（ String–&gt; 简单类型的转换操作由ConversionService配置的转换器来完成）；因为使用request.getParameter()方式获取参数，所以可以处理get 方式中queryString的值，也可以处理post方式中 body data的值；B）用来处理Content-Type: 为 application/x-www-form-urlencoded编码的内容，提交方式GET、POST；C) 该注解有两个属性： value、required； value用来指定要传入值的id名称，required用来指示参数是否必须绑定；@RequestBody：该注解常用来处理Content-Type: 不是application/x-www-form-urlencoded编码的内容，例如application/json, application/xml等；它是通过使用HandlerAdapter 配置的HttpMessageConverters来解析post data body，然后绑定到相应的bean上的。因为配置有FormHttpMessageConverter，所以也可以用来处理 application/x-www-form-urlencoded的内容，处理完的结果放在一个MultiValueMap里，这种情况在某些特殊需求下使用 @InitBinder：用来设置WebDataBinder，WebDataBinder用来自动绑定前台请求的参数到Model中。@ExceptionHandler：用于全局处理控制器里的异常。@ModelAttribute该注解有两个用法，一个是用于方法上，一个是用于参数上；用于方法上时： 通常用来在处理@RequestMapping之前，为请求绑定需要从后台查询的model；用于参数上时： 用来通过名称对应，把相应名称的值绑定到注解的参数bean上；要绑定的值来源于： @SessionAttribute:该注解用来绑定HttpSession中的attribute对象的值，便于在方法中的参数里使用。@RequestAttribute：可以被用于访问由过滤器或拦截器创建的、预先存在的请求属性 @ResponseBody：支持将返回值放在response的body体内 @Scope：注解也可以指定Bean实例的作用域。取值：Singleton(默认)|Prototype|Request|Session|GlobalSession@Autowired：为目标bean指定协作者Bean@Value：为属性注入值 @Aspect：声明一个切面@PointCut：定义拦截规则@After：标注一个之后建言@Before：标注一个之前建言@Around：标注一个运行时建言 @Transcational：事物@Cacheable：数据缓存 @EnableScheduling：开启计划任务@Scheduled：声明这是一个计划任务 @Import：since4.2。导入普通的java类,并将其声明成一个bean@Configuration：声明一个配置类@ComponentScan：制定Spring扫描包路径。@Bean 注解在方法上，声明当前方法的返回@Profile：可以注解在类或方法上。 在不同环境下使用不同配置提供支持。@Conditional：根据满足某一个特定条件创建一个特定的Bean。 Spring boot@SpringBootApplication ：会根据类路径中jar包自动进行相关配置。@EnableAutoConfiguration@ConfigurationProperties：加载一个properties文件。 @ConditionalOnBean:当容器里有指定的Bean的条件下@ConditionalOnClass:当类路径下有指定的类的条件下。@ConditionalOnExpression：基于SPEl表达式作为判断条件。@ConditionalOnJava：基于JVM版本作为判断条件。@ConditionalOnJndi:在JNDI存在的条件下查找指定的位置。@ConditionalOnMissingBean：当容器里没有指定Bean的条件下。@ConditionalOnMissingClass:当类路径下没有指定的类的条件下。@ConditionalOnWebApplication:当前项目是web项目的条件下。@ConditionalOnNotWebApplication:当前项目部是WEb项目的条件下。@ConditionalOnProperty:指定的属性是否有指定的值。@ConditionalOnResource：类路径是否有指定的值。@ConditionalOnSingleCandidate:当指定首选的Bean。","link":"/2017/04/20/Spring/Spring常用注解/"},{"title":"spring声明式事务配置","text":"上周要一个同事开发一个模块，他说事物死活不起作用。我看了一下，大致主要配置如下： 12345678910111213141516171819202122232425262728&lt;!-- 配置事务管理 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"businessDataSource\" /&gt;&lt;/bean&gt;&lt;!-- 事务相关控制配置：例如配置事务的传播机制 --&gt;&lt;tx:advice id=\"fortressAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"get*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"query*\" propagation=\"SUPPORTS\" read-only=\"false\" /&gt; &lt;tx:method name=\"count*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"save*\" propagation=\"REQUIRED\" read-only=\"false\" /&gt; &lt;tx:method name=\"remove*\" propagation=\"REQUIRED\" read-only=\"false\" /&gt; &lt;tx:method name=\"modify*\" propagation=\"REQUIRED\" read-only=\"false\" /&gt; &lt;tx:method name=\"*\" propagation=\"SUPPORTS\" read-only=\"true\" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 事务控制切入点在service层|第一个 * —— 通配 任意返回值类型||第二个 * —— 通配包com.mindasoft.fortress.service下的任意class||第三个 * —— 通配包com.mindasoft.fortress.service下的任意class的任意方法||第四个 .. —— 通配 办法可以有0个或多个参数--&gt;&lt;aop:config&gt; &lt;aop:pointcut id=\"allFortressMethod\" expression=\"execution(* com.mindasoft.fortress.service.*.*(..))\" /&gt; &lt;aop:advisor advice-ref=\"fortressAdvice\" pointcut-ref=\"allFortressMethod\" /&gt;&lt;/aop:config&gt; 配置是这样没错，他的配置也问题，但是为什么会不起作用呢？Debug进行，发现根本就没有进入事物处理。为什么会这样？看配置web.xml1234567891011121314151617181920212223&lt;!-- Spring MVC 子Context配置 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:/config/servlet.web.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;*.html&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- Spring root Context监听配置 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:/config/**/*.bean.xml&lt;/param-value&gt;&lt;/context-param&gt; Spring和SpringMVC一起使用时，ContextLoaderListener会初始化一个Context，这个Context是RootContext，而DispatcherServlet也会初始化一个Context，简称WebContext。WebContext是RootContext的子集。当mvc有自己的bean时便不再去向父context要bean。 所以，在servlet.web.xml 和*.bean.xml 当中，各自的component-scan配置要指定相应位置，否则会导致bean混乱，从导致声明事务无效。如下：servlet.web.xml1&lt;context:component-scan base-package=\"com.mindasoft.*.controllers\" /&gt; *.bean.xml12&lt;context:component-scan base-package=\"com.mindasoft.*.dao\" /&gt;&lt;context:component-scan base-package=\"com.mindasoft.*.service\" /&gt; 当然你也可以使用DispatcherServlet加载全部的配置文件或者将AOP配置复制到servlet.web.xml中。 这个修改了之后还是不行？？看了下他的代码，他是自己写了一个Exception。然后断点跟踪源代码TransactionAspectSupport：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Handle a throwable, completing the transaction. * We may commit or roll back, depending on the configuration. * @param txInfo information about the current transaction * @param ex throwable encountered */protected void completeTransactionAfterThrowing(TransactionInfo txInfo, Throwable ex) { if (txInfo != null &amp;&amp; txInfo.hasTransaction()) { if (logger.isTraceEnabled()) { logger.trace(\"Completing transaction for [\" + txInfo.getJoinpointIdentification() + \"] after exception: \" + ex); } if (txInfo.transactionAttribute.rollbackOn(ex)) { try { txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()); } catch (TransactionSystemException ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); ex2.initApplicationException(ex); throw ex2; } catch (RuntimeException ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); throw ex2; } catch (Error err) { logger.error(\"Application exception overridden by rollback error\", ex); throw err; } } else { // We don't roll back on this exception. // Will still roll back if TransactionStatus.isRollbackOnly() is true. try { txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); } catch (TransactionSystemException ex2) { logger.error(\"Application exception overridden by commit exception\", ex); ex2.initApplicationException(ex); throw ex2; } catch (RuntimeException ex2) { logger.error(\"Application exception overridden by commit exception\", ex); throw ex2; } catch (Error err) { logger.error(\"Application exception overridden by commit error\", ex); throw err; } } }} 上面的方法中有这么一段txInfo.transactionAttribute.rollbackOn(ex)，这里是判断是否需要执行回滚操作的，跟踪rollbackOn方法最后会执行到DefaultTransactionAttribute中的rollbackOn方法。12345678/** * The default behavior is as with EJB: rollback on unchecked exception. * Additionally attempt to rollback on Error. * &lt;p&gt;This is consistent with TransactionTemplate's default behavior. */public boolean rollbackOn(Throwable ex) { return (ex instanceof RuntimeException || ex instanceof Error);} 到这里，应该都清楚了。。。自己主动抛异常Exception是不对的。这里只捕获运行时异常RuntimeException 及Error，所以我们测试时不可以直接抛Exception，而应该换成RuntimeException 。当然。也可在xml中指定rollback-for。123456789101112&lt;!-- 事务相关控制配置：例如配置事务的传播机制 --&gt;&lt;tx:advice id=\"fortressAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"get*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"query*\" propagation=\"SUPPORTS\" read-only=\"false\" /&gt; &lt;tx:method name=\"count*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"save*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"Exception\"/&gt; &lt;tx:method name=\"remove*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"Exception\"/&gt; &lt;tx:method name=\"modify*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"Exception\"/&gt; &lt;tx:method name=\"*\" propagation=\"SUPPORTS\" read-only=\"true\" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;","link":"/2017/03/19/Spring/spring声明式事务配置/"},{"title":"Linux查看物理CPU个数、核数、逻辑CPU个数","text":"CPU总核数 = 物理CPU个数 每颗物理CPU的核数总逻辑CPU数 = 物理CPU个数 每颗物理CPU的核数 * 超线程数 查看CPU信息（型号）1234567891011121314[root@AAA ~]# cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 24 Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz# 查看物理CPU个数[root@AAA ~]# cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l2# 查看每个物理CPU中core的个数(即核数)[root@AAA ~]# cat /proc/cpuinfo| grep \"cpu cores\"| uniqcpu cores : 6# 查看逻辑CPU的个数[root@AAA ~]# cat /proc/cpuinfo| grep \"processor\"| wc -l24","link":"/2018/02/07/Linux/Linux查看物理CPU个数、核数、逻辑CPU个数/"},{"title":"Centos7.3防火墙配置","text":"firewall防火墙1、查看firewall服务状态123456789101112131415查看状态： systemctl status firewalld 启动： systemctl start firewalld停止： systemctl stop firewalld禁用： systemctl enable firewalld启用： systemctl disable firewalld启动一个服务：systemctl start firewalld.service关闭一个服务：systemctl stop firewalld.service重启一个服务：systemctl restart firewalld.service显示一个服务的状态：systemctl status firewalld.service在开机时启用一个服务：systemctl enable firewalld.service在开机时禁用一个服务：systemctl disable firewalld.service查看服务是否开机启动：systemctl is-enabled firewalld.service查看已启动的服务列表：systemctl list-unit-files|grep enabled查看启动失败的服务列表：systemctl --failed 2、查看firewall的状态1firewall-cmd --state 3、开启、重启、关闭、firewalld.service服务123456# 开启service firewalld start# 重启service firewalld restart# 关闭service firewalld stop 4、查看防火墙规则123456789101112firewall-cmd --list-all -zone=public查看版本： firewall-cmd --version查看帮助： firewall-cmd --help显示状态： firewall-cmd --state查看所有打开的端口： firewall-cmd --zone=public --list-ports更新防火墙规则： firewall-cmd --reload查看区域信息: firewall-cmd --get-active-zones查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0拒绝所有包：firewall-cmd --panic-on取消拒绝状态： firewall-cmd --panic-off查看是否拒绝： firewall-cmd --query-panic 5、查询、开放、关闭端口 12345678# 查询端口是否开放firewall-cmd --query-port=8080/tcp # 开放80端口firewall-cmd --permanent --add-port=8080/tcp --zone=public# 移除端口firewall-cmd --permanent --remove-port=8080/tcp --zone=public#重启防火墙(修改配置后要重启防火墙)firewall-cmd --reload 参数解释1、firwall-cmd：是Linux提供的操作firewall的一个工具；2、–permanent：表示设置为持久；3、–add-port：标识添加的端口； iptables防火墙1、安装123sudo yum install iptables-servicessudo systemctl enable iptables &amp;&amp; sudo systemctl enable ip6tablessudo systemctl start iptables &amp;&amp; sudo systemctl start ip6tables 2、启用、禁用1234 #最后重启防火墙使配置生效systemctl restart iptables.service# 设置防火墙开机启动systemctl enable iptables.service 3、编辑配置文件1vi /etc/sysconfig/iptables 12345678910111213141516# sampleconfiguration for iptables service# you can edit thismanually or use system-config-firewall# please do not askus to add additional ports/services to this default configuration*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT[0:0]:OUTPUT ACCEPT[0:0]-A INPUT -m state--state RELATED,ESTABLISHED -j ACCEPT-A INPUT -p icmp -jACCEPT-A INPUT -i lo -jACCEPT-A INPUT -p tcp -mstate --state NEW -m tcp --dport 22 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -jACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 8080-j ACCEPT-A INPUT -j REJECT--reject-with icmp-host-prohibited-A FORWARD -jREJECT --reject-with icmp-host-prohibitedCOMMIT 4、开启、停止123456# 查看防火墙状态： service iptables status# 开启防火墙：service iptables start# 关闭防火墙：service iptables stop","link":"/2018/01/02/Linux/Centos7.3防火墙配置/"},{"title":"Hexo博客（二）更换主题和相关设置","text":"更换主题icarus1、下载icarus主题地址：https://github.com/ppoffice/hexo-theme-icarus2、更换主题icarus，修改Hexo的_config.yml里面的theme如下：1theme: icarus 搜索插件主题已经集成了，主题的_config.yml1234search: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false 默认配置不用改。但是需要安装hexo-generator-json-content。命令如下：1$ npm install -S hexo-generator-json-content 打赏在国外比较流行打赏，在中国嘛…有总比没有好，万一有人打赏呢？是吧。在主题配置文件_config.yml的comment上面添加:123#donate 打赏donate: true # options: true , falsedonate_message: 如果您觉得文章不错,可以请我喝一杯咖啡！ 在layout添加文件jdonate.ejs，内容如下：123456789101112131415161718192021222324252627&lt;! -- 添加捐赠图标 --&gt;&lt;div class =\"post-donate\"&gt; &lt;div id=\"donate_board\" class=\"donate_bar center\"&gt; &lt;a id=\"btn_donate\" class=\"btn_donate\" href=\"javascript:;\" title=\"打赏\"&gt;&lt;/a&gt; &lt;span class=\"donate_txt\"&gt; &lt;%=theme.donate_message%&gt; &lt;/span&gt; &lt;br&gt; &lt;/div&gt; &lt;div id=\"donate_guide\" class=\"donate_bar center hidden\" &gt; &lt;!-- 支付宝打赏图案 --&gt; &lt;img src=\"/img/zhifubao.png\" alt=\"支付宝打赏\" &gt; &lt;!-- 微信打赏图案 --&gt; &lt;img src=\"/img/weixin.png\" alt=\"微信打赏\" &gt; &lt;/div&gt; &lt;script type=\"text/javascript\"&gt; document.getElementById('btn_donate').onclick = function(){ if($('#donate_guide').hasClass('hidden')){ $('#donate_guide').removeClass('hidden'); }else{ $('#donate_guide').addClass('hidden'); } } &lt;/script&gt;&lt;/div&gt;&lt;! -- 添加捐赠图标 --&gt; 在主题source\\css_partial目录新增donate.styl：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647.donate_bar { text-align: center; background: #fff --margin-top: 5%}.donate_bar a.btn_donate { display: inline-block; width: 82px; height: 82px; margin-left: auto; margin-right: auto; background: url(../img/btn_reward.gif)no-repeat; -webkit-transition: background 0s; -moz-transition: background 0s; -o-transition: background 0s; -ms-transition: background 0s; transition: background 0s}.donate_bar a.btn_donate:hover { background-position: 0 -82px}.donate_bar .donate_txt { display: block; color: #9d9d9d; font: 14px/2 \"Microsoft Yahei\"}.donate_bar.hidden{ display: none}.post-donate{ --margin-top: 80px;}#donate_guide{ height: 320px; width: 100%; margin: 0 auto;}#donate_guide img{ height: 300px; height: 300px;} btn_reward.gif直接拿我的，然后放在img目录吧。最后修改layout\\common\\article.ejs，添加如下：1234567&lt;footer class=\"article-footer\"&gt; &lt;% if (!index &amp;&amp; theme.donate){ %&gt; &lt;%- partial('donate') %&gt; &lt;% } %&gt; &lt;%- partial('share/index', { post: post }) %&gt; &lt;%- partial('comment/counter', { post: post }) %&gt;&lt;/footer&gt; 需要在加入icarus/css/style.styl计入@import ‘_partial/donate’ 添加评论这个主题的评论已经集成了，并不需要我们进行手工导入，要求你有多说或者disqus的账号。1234comment: disqus: # enter disqus shortname here duoshuo: # enter duoshuo shortname here youyan: # enter youyan uid here disqus是国外的，不适用。我们选择duoshuo。在多说注册账号：http://duoshuo.com/创建站点后在主题目录的_config.yml中填写shortname： duoshuo: 多说设置的名称 分享主题已经集成，只需要配置即可。启用jiathis，如下12# Share 分享share: jiathis # options: jiathis, bdshare, addtoany, default 百度/谷歌验证站点为什么要验证站点了，因为要搜索引擎进行收录，说白了就是让别人更容易搜索到你的网站，仅此而已。首先需要到百度/谷歌站长统计中注册，以及验证：百度站长工具Google网站管理员工具地址注册完后，进行输入相应的网站地址，然后选择html验证，将代码加入以下路径layout/_partial/head.ejs：12345&lt;head&gt; &lt;meta name=\"baidu-site-verification\" content=\"xxxx\" /&gt; &lt;meta name=\"google-site-verification\" content=\"xxxx\" /&gt; &lt;meta charset=\"utf-8\"&gt; .... 然后发布到github中，再进行验证即可。 添加站长统计我们通过站长统计来及时查看我们个人网站的浏览情况。我寻找cnzz，需要先注册：站长统计1、在theme的_config.yml中的末尾添加以下：12# CNZZ web_idcnzz: CNZZ的web_id 2、在主题目录的layout/common添加文件为cnzz.ejs，内容如下：123&lt;% if (theme.cnzz){ %&gt;Analyse with &lt;script src=\"http://s23.cnzz.com/z_stat.php?id=&lt;%= theme.cnzz %&gt;&amp;web_id=&lt;%= theme.cnzz %&gt;\" language=\"JavaScript\"&gt;&lt;/script&gt;&lt;% } %&gt; 如需其他形式的，请参数CNZZ上的代码。3、最后进行显示，在路径layout/common/footer.ejs里面添加1...PPOffice&lt;/a&gt;.&lt;%- partial('cnzz') %&gt; 提醒：注意在_config.xml中添加web_id， 自定义widget在layout/widget 自定义模板即可，例如：1234567891011121314151617&lt;% if (site.tags.length) { %&gt;&lt;div class=\"card widget\"&gt; &lt;div class=\"card-content\"&gt; &lt;h3 class=\"menu-label\"&gt; &lt;%= __('widget.announcement') %&gt; &lt;/h3&gt; &lt;p class=\"board\"&gt; 欢迎来访问！&lt;br&gt; QQ：150045153&lt;br&gt; 微信：150045153&lt;br&gt; 邮箱：hmiter@sina.com&lt;br&gt;&lt;br&gt; 欢迎交流与分享经验! &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;% } %&gt; categories和tags页面不显示解决办法默认是没有 categories 和 tags 的需要12hexo new page \"tags\" hexo new page \"categories\" 编辑 /tags/index.md /categories/index.md12345type: \"tags\"layout: \"tags\"type: \"categories\"layout: \"categories\"","link":"/2017/02/06/hexo/Hexo博客（二）更换主题和相关设置/"},{"title":"JavaSE源码分析-Integer源码分析","text":"Integer 类在对象中包装了一个基本类型 int 的值。Integer 类型的对象包含一个 int 类型的字段。 类定义1public final class Integer extends Number implements Comparable&lt;Integer&gt; 从类定义中我们可以知道以下几点： Integer类不能被继承 Integer类实现了Comparable接口，所以可以用compareTo进行比较并且Integer对象只能和Integer类型的对象进行比较，不能和其他类型比较（至少调用compareTo方法无法比较）。 Integer继承了Number类，所以该类可以调用longValue、floatValue、doubleValue等系列方法返回对应的类型的值。 Numbers实现Serializable接口，所以Integer也支持序列化和反序列化。 属性私有属性Integer类中定义了以下几个私有属性：12private final int value;private static final long serialVersionUID = 1360826667806852920L; value属性就是Integer对象中真正保存int值的。 serialVersionUID和序列化有关。Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常(InvalidCastException)。 公共属性12345678910//值为 （－（2的31次方）） 的常量，它表示 int 类型能够表示的最小值。@Native public static final int MIN_VALUE = 0x80000000;//值为 （（2的31次方）－1） 的常量，它表示 int 类型能够表示的最大值。@Native public static final int MAX_VALUE = 0x7fffffff; //表示基本类型 int 的 Class 实例。public static final Class&lt;Integer&gt; TYPE = (Class&lt;Integer&gt;) Class.getPrimitiveClass(\"int\");//用来以二进制补码形式表示 int 值的比特位数。@Native public static final int SIZE = 32;//用来以二进制补码形式表示 int 值的字节数。1.8以后才有public static final int BYTES = SIZE / Byte.SIZE; 以上属性可直接使用，因为他们已经定义成publis static fianl能用的时候尽量使用他们，这样不仅能使代码有很好的可读性，也能提高性能节省资源。 装箱拆箱12345678public class IntegerLearning { public static void main(String[] args) { Integer i = 100; int i2 = i; System.out.println(i); }} 上面代码，编译后，我们通过jdk自带的javap命令工具对IntegerLearning.class 进行分析javap -v IntegerLearning.class123456789101112131415public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: bipush 100 2: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 5: astore_1 6: aload_1 7: invokevirtual #3 // Method java/lang/Integer.intValue:()I 10: istore_2 11: getstatic #4 // Field java/lang/System.out:Ljava/io/PrintStream; 14: aload_1 15: invokevirtual #5 // Method java/io/PrintStream.println:(Ljava/lang/Object;)V 18: return 从上面可以看出Integer i = 100; 编译器会转成 Integer i = Integer.valueOf(100);int i2 = i 自动拆箱用的是 Integer.intValue 内部类123456789101112131415161718192021222324252627282930313233private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) { try { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } catch( NumberFormatException nfe) { // If the property cannot be parsed into an int, ignore it. } } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; } private IntegerCache() {}} Integer内部有一个内部类IntegerCache，用来缓存缓存以支持-128和127（包括）之间的值的自动装箱的对象标识语义。可以看出，Integer实例化时，IntegerCache中的 Integer cache[] 缓存了 -128 和127（包括）之间的值，下面会介绍如何使用了缓存的内部类。 方法构造方法Integer提供了两个构造方法： 123456789//构造一个新分配的 Integer 对象，它表示指定的 int 值。public Integer(int value) { this.value = value;}//构造一个新分配的 Integer 对象，它表示 String 参数所指示的 int 值。public Integer(String s) throws NumberFormatException { this.value = parseInt(s, 10);} 从构造方法中我们可以知道，初始化一个Integer对象的时候只能创建一个十进制的整数。 String转Integerparse方法在String的构造方法中，使用了parseInt，我们来看下全部的parse方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// 使用进制基数radix（2,8,10,16,27,36）解析字符串Spublic static int parseInt(String s, int radix) throws NumberFormatException{ if (s == null) { throw new NumberFormatException(\"null\"); } if (radix &lt; Character.MIN_RADIX) { throw new NumberFormatException(\"radix \" + radix + \" less than Character.MIN_RADIX\"); } if (radix &gt; Character.MAX_RADIX) { throw new NumberFormatException(\"radix \" + radix + \" greater than Character.MAX_RADIX\"); } int result = 0; boolean negative = false; int i = 0, len = s.length(); int limit = -Integer.MAX_VALUE; int multmin; int digit; if (len &gt; 0) { // 判断字符串开通是否以 + - 开头 char firstChar = s.charAt(0); if (firstChar &lt; '0') { // Possible leading \"+\" or \"-\" if (firstChar == '-') { negative = true; limit = Integer.MIN_VALUE; } else if (firstChar != '+') throw NumberFormatException.forInputString(s); if (len == 1) // Cannot have lone \"+\" or \"-\" throw NumberFormatException.forInputString(s); i++; } // 循环遍历字符串每个字符转成相应进制数字 multmin = limit / radix; while (i &lt; len) { // Accumulating negatively avoids surprises near MAX_VALUE digit = Character.digit(s.charAt(i++),radix); if (digit &lt; 0) { throw NumberFormatException.forInputString(s); } if (result &lt; multmin) { throw NumberFormatException.forInputString(s); } result *= radix; if (result &lt; limit + digit) { throw NumberFormatException.forInputString(s); } result -= digit; } } else { throw NumberFormatException.forInputString(s); } return negative ? result : -result;}public static int parseInt(String s) throws NumberFormatException { return parseInt(s,10);}public static int parseUnsignedInt(String s)public static int parseUnsignedInt(String s, int radix) parseUnsignedInt 解析无符号的字符串，在内部原理调用的是parseInt 或者Long.parseLong。这里就不在说明。 valueOf方法12345678910111213public static Integer valueOf(int i) { if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);}public static Integer valueOf(String s, int radix) throws NumberFormatException { return Integer.valueOf(parseInt(s,radix));}public static Integer valueOf(String s) throws NumberFormatException { return Integer.valueOf(parseInt(s, 10));} valueOf(int i)使用缓存内部类IntegerCache,如果是-128和127之间的数，不是重新new的类，而直接返回缓存的Integer类。 getInteger方法1234567891011121314151617181920212223public static Integer getInteger(String nm) { return getInteger(nm, null);}public static Integer getInteger(String nm, int val) { Integer result = getInteger(nm, null); return (result == null) ? Integer.valueOf(val) : result;}public static Integer getInteger(String nm, Integer val) { String v = null; try { v = System.getProperty(nm); } catch (IllegalArgumentException | NullPointerException e) { } if (v != null) { try { return Integer.decode(v); } catch (NumberFormatException e) { } } return val;} 从上面可以看出getInteger 实际上调用的是Integer.decode方法。那我们来看看decode1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 通过字符串前面几位字符得出 是否为 十进制，十六进制和八进制数，而以其规则解析成Integer类public static Integer decode(String nm) throws NumberFormatException { int radix = 10; int index = 0; boolean negative = false; Integer result; if (nm.length() == 0) throw new NumberFormatException(\"Zero length string\"); char firstChar = nm.charAt(0); // Handle sign, if present if (firstChar == '-') { negative = true; index++; } else if (firstChar == '+') index++; // Handle radix specifier, if present if (nm.startsWith(\"0x\", index) || nm.startsWith(\"0X\", index)) { index += 2; radix = 16; } else if (nm.startsWith(\"#\", index)) { index ++; radix = 16; } else if (nm.startsWith(\"0\", index) &amp;&amp; nm.length() &gt; 1 + index) { index ++; radix = 8; } if (nm.startsWith(\"-\", index) || nm.startsWith(\"+\", index)) throw new NumberFormatException(\"Sign character in wrong position\"); try { result = Integer.valueOf(nm.substring(index), radix); result = negative ? Integer.valueOf(-result.intValue()) : result; } catch (NumberFormatException e) { // If number is Integer.MIN_VALUE, we'll end up here. The next line // handles this case, and causes any genuine format error to be // rethrown. String constant = negative ? (\"-\" + nm.substring(index)) : nm.substring(index); result = Integer.valueOf(constant, radix); } return result;} 我们又发现decode 底层调用的是Integer.valueOf。从而得出： 所有将String转成Integer的方法都是基于parseInt方法实现的。简单看一下以上部分方法的调用栈。123456getInteger(...) ---&gt; ---&gt;Integer.decode(...)---&gt;Integer.valueOf(...)---&gt;parseInt(...)``` ## Integer转String### toString public String toString() { return toString(value);} public static String toString(int i) { if (i == Integer.MIN_VALUE) return “-2147483648”; int size = (i &lt; 0) ? stringSize(-i) + 1 : stringSize(i); char[] buf = new char[size]; getChars(i, size, buf); return new String(buf, true);}12 public static String toString(int i, int radix)public static String toBinaryString(int i)public static String toHexString(int i)public static String toOctalString(int i)public static String toUnsignedString(int i)public static String toUnsignedString(int i, int radix)```","link":"/2018/10/24/javase/Integer源码分析/"},{"title":"JavaSE源码分析-String源码分析","text":"Integer 类在对象中包装了一个基本类型 int 的值。Integer 类型的对象包含一个 int 类型的字段。 类定义1public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence{} 从该类的声明中我们可以看出String是final类型的，表示该类不能被继承，同时该类实现了三个接口：java.io.Serializable、 Comparable、 CharSequence 属性私有属性Integer类中定义了以下几个私有属性：1234private final char value[];private int hash;private static final long serialVersionUID = -6849794470754667710L;private static final ObjectStreamField[] serialPersistentFields = new ObjectStreamField[0]; value[]这是一个字符数组，并且是final类型，他用于存储字符串内容，从fianl这个关键字中我们可以看出，String的内容一旦被初始化了是不能被更改的。 虽然有这样的例子： String s = “a”; s = “b” 但是，这并不是对s的修改，而是重新指向了新的字符串， 从这里我们也能知道，String其实就是用char[]实现的。 hash缓存字符串的hash Code，默认值为 0 因为String实现了Serializable接口，所以支持序列化和反序列化支持。Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常(InvalidCastException)。 12345678public class StringLearning { public static void main(String[] args) { String s1 = \"abc\"; System.out.println(s1); String s2 = new String(\"edf\"); System.out.println(s2); }} 上面代码，编译后，我们通过jdk自带的javap命令工具对StringLearning.class 进行分析javap -v StringLearning.class12345678910111213141516171819public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=3, args_size=1 0: ldc #2 // String abc 2: astore_1 3: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 6: aload_1 7: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 10: new #5 // class java/lang/String 13: dup 14: ldc #6 // String edf 16: invokespecial #7 // Method java/lang/String.\"&lt;init&gt;\":(Ljava/lang/String;)V 19: astore_2 20: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 23: aload_2 24: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 27: return ldc表示将一个常量加载到操作数栈。 #2 从常量池中取出”abc”的引用，加载到操作数栈中 。在编译启动过程中,字符串字面量就会被存入常量池astore_1 保存一个变量，就是s116: invokespecial #7 是调用String的构造方法。 由以上可以知道，String s1 = “abc”中s1只是引用了常量池的值。String s2 = new String(“edf”);的s2是指向了一个String对象。那么：1234String s1 = \"abc\";String s2 = \"abc\";String s3 = new String(\"abc\");String s4 = new String(\"abc\"); s1 == s2 答案：trues1 == s3 答案：falses3 == s4 答案：falses1.equals(s2) 答案：trues1.equals(s3) 答案：trues3.equals(s4) 答案：true 方法构造方法String类作为一个java.lang包中比较常用的类,自然有很多重载的构造方法.在这里介绍几种典型的构造方法:1234public String(String original) { this.value = original.value; this.hash = original.hash;} 我们知道，其实String就是使用字符数组（char[]）实现的。所以我们可以使用一个字符数组来创建一个String，那么这里值得注意的是，当我们使用字符数组创建String的时候，会用到Arrays.copyOf方法和Arrays.copyOfRange方法。这两个方法是将原有的字符数组中的内容逐一的复制到String中的字符数组中。同样，我们也可以用一个String类型的对象来初始化一个String。这里将直接将源String中的value和hash两个属性直接赋值给目标String。因为String一旦定义之后是不可以改变的，所以也就不用担心改变源String的值会影响到目标String的值。 当然，在使用字符数组来创建一个新的String对象的时候，不仅可以使用整个字符数组，也可以使用字符数组的一部分，只要多传入两个参数int offset和int count就可以了。123public String(char value[]) { this.value = Arrays.copyOf(value, value.length);} 在Java中，String实例中保存有一个char[]字符数组，char[]字符数组是以unicode码来存储的，String 和 char 为内存形式，byte是网络传输或存储的序列化形式。所以在很多传输和存储的过程中需要将byte[]数组和String进行相互转化。所以，String提供了一系列重载的构造方法来将一个字符数组转化成String，提到byte[]和String之间的相互转换就不得不关注编码问题。String(byte[] bytes, Charset charset)是指通过charset来解码指定的byte数组，将其解码成unicode的char[]数组，够造成新的String。 这里的bytes字节流是使用charset进行编码的，想要将他转换成unicode的char[]数组，而又保证不出现乱码，那就要指定其解码方式 同样使用字节数组来构造String也有很多种形式，按照是否指定解码方式分的话可以分为两种：12345String(byte bytes[]) String(byte bytes[], int offset, int length)String(byte bytes[], Charset charset)String(byte bytes[], String charsetName)String(byte bytes[], int offset, int length, Charset charset)String(byte bytes[], int offset, int length, String charsetName) 如果我们在使用byte[]构造String的时候，使用的是下面这四种构造方法(带有charsetName或者charset参数)的一种的话，那么就会使用StringCoding.decode方法进行解码，使用的解码的字符集就是我们指定的charsetName或者charset。 我们在使用byte[]构造String的时候，如果没有指明解码使用的字符集的话，那么StringCoding的decode方法首先调用系统的默认编码格式，如果没有指定编码格式则默认使用ISO-8859-1编码格式进行编码操作。主要体现代码如下：123456789101112131415161718192021static char[] decode(byte[] ba, int off, int len) { String csn = Charset.defaultCharset().name(); try { // use charset name decode() variant which provides caching. return decode(csn, ba, off, len); } catch (UnsupportedEncodingException x) { warnUnsupportedCharset(csn); } try { return decode(\"ISO-8859-1\", ba, off, len); } catch (UnsupportedEncodingException x) { // If this code is hit during VM initialization, MessageUtils is // the only way we will be able to get any kind of error message. MessageUtils.err(\"ISO-8859-1 charset not available: \" + x.toString()); // If we can not find ISO-8859-1 (a required encoding) then things // are seriously wrong with the installation. System.exit(1); return null; }} 使用StringBuffer和StringBuider构造一个String作为String的两个“兄弟”，StringBuffer和StringBuider也可以被当做构造String的参数。123456789public String(StringBuffer buffer) { synchronized(buffer) { this.value = Arrays.copyOf(buffer.getValue(), buffer.length()); }}public String(StringBuilder builder) { this.value = Arrays.copyOf(builder.getValue(), builder.length());} 当然，这两个构造方法是很少用到的，至少我从来没有使用过，因为当我们有了StringBuffer或者StringBuilfer对象之后可以直接使用他们的toString方法来得到String。关于效率问题，Java的官方文档有提到说使用StringBuilder的toString方法会更快一些，原因是StringBuffer的toString方法是synchronized的，在牺牲了效率的情况下保证了线程安全。123456 public String toString() { // Create a copy, don't share the array return new String(value, 0, count); }this.value = Arrays.copyOfRange(value, offset, offset+count); 其他方法","link":"/2018/10/23/javase/String源码分析/"},{"title":"Nginx在Linux下的安装","text":"系统平台：CentOS release 6.6 (Final) 64位。 安装编译工具及库文件1yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 首先要安装 PCREPCRE 作用是让 Nginx 支持 Rewrite 功能。 1、下载 PCRE 安装包，下载地址： http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz1[root@ngnix src]# wget http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz 2、解压安装包:1[root@ngnix src]# tar zxvf pcre-8.35.tar.gz 3、进入安装包目录1[root@ngnix src]# cd pcre-8.35 4、编译安装12[root@ngnix pcre-8.35]# ./configure[root@ngnix pcre-8.35]# make &amp;&amp; make install 5、查看pcre版本1[root@ngnix pcre-8.35]# pcre-config --version 安装Nginx1、下载 Nginx，下载地址：http://nginx.org/download/nginx-1.6.2.tar.gz1[root@ngnix src]# wget http://nginx.org/download/nginx-1.6.2.tar.gz 2、解压安装包1[root@ngnix src]# tar zxvf nginx-1.6.2.tar.gz 3、进入安装包目录1[root@ngnix src]# cd nginx-1.6.2 4、编译安装123[root@ngnix nginx-1.6.2]# ./configure --prefix=/usr/local/webserver/nginx --with-http_stub_status_module --with-http_ssl_module --with-pcre=/usr/local/src/pcre-8.35[root@ngnix nginx-1.6.2]# make[root@ngnix nginx-1.6.2]# make install 常用编译选项说明: nginx大部分常用模块，编译时./configure –help以–without开头的都默认安装。 –prefix=PATH ： 指定nginx的安装目录。默认 /usr/local/nginx –conf-path=PATH ： 设置nginx.conf配置文件的路径。nginx允许使用不同的配置文件启动，通过命令行中的-c选项。默认为prefix/conf/nginx.conf –user=name： 设置nginx工作进程的用户。安装完成后，可以随时在nginx.conf配置文件更改user指令。默认的用户名是nobody。–group=name类似 –with-pcre ： 设置PCRE库的源码路径，如果已通过yum方式安装，使用–with-pcre自动找到库文件。使用–with-pcre=PATH时，需要从PCRE网站下载pcre库的源码（版本4.4 – 8.30）并解压，剩下的就交给Nginx的./configure和make来完成。perl正则表达式使用在location指令和 ngx_http_rewrite_module模块中。 –with-zlib=PATH ： 指定 zlib（版本1.1.3 – 1.2.5）的源码解压目录。在默认就启用的网络传输压缩模块ngx_http_gzip_module时需要使用zlib 。 –with-http_ssl_module ： 使用https协议模块。默认情况下，该模块没有被构建。前提是openssl与openssl-devel已安装 –with-http_stub_status_module ： 用来监控 Nginx 的当前状态 –with-http_realip_module ： 通过这个模块允许我们改变客户端请求头中客户端IP地址值(例如X-Real-IP 或 X-Forwarded-For)，意义在于能够使得后台服务器记录原始客户端的IP地址 –add-module=PATH ： 添加第三方外部模块，如nginx-sticky-module-ng或缓存模块。每次添加新的模块都要重新编译（Tengine可以在新加入module时无需重新编译）5、查看nginx版本1[root@ngnix nginx-1.6.2]# /usr/local/webserver/nginx/sbin/nginx -v 到此，nginx安装完成。 Nginx 配置创建 Nginx 运行使用的用户 www：12[root@ngnix conf]# /usr/sbin/groupadd www [root@ngnix conf]# /usr/sbin/useradd -g www www 配置nginx.conf ，将/usr/local/webserver/nginx/conf/nginx.conf替换为以下内容1[root@ngnix conf]# cat /usr/local/webserver/nginx/conf/nginx.conf 显示如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374user www www;worker_processes 2; #设置值和CPU核心数一致error_log /usr/local/webserver/nginx/logs/nginx_error.log crit; #日志位置和日志级别pid /usr/local/webserver/nginx/nginx.pid;#Specifies the value for maximum file descriptors that can be opened by this process.worker_rlimit_nofile 65535;events{ use epoll; worker_connections 65535;}http{ include mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" $http_x_forwarded_for'; #charset gb2312; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 8m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/html text/xml text/javascript application/x-javascript application/javascript text/css text/plain image/png image/jpeg image/gif; gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #下面是server虚拟主机的配置 server { listen 80;#监听端口 server_name localhost;#域名 index index.html index.htm index.php; root /usr/local/webserver/nginx/html;#站点目录 location ~ .*\\.(php|php5)?$ { #fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|ico)$ { expires 30d; # access_log off; } location ~ .*\\.(js|css)?$ { expires 15d; # access_log off; } access_log off; }} 检查配置文件ngnix.conf的正确性命令：1[root@ngnix conf]# /usr/local/webserver/nginx/sbin/nginx -t Nginx命令1234/usr/local/webserver/nginx/sbin/nginx # 启动 Nginx/usr/local/webserver/nginx/sbin/nginx -s reload # 重新载入配置文件/usr/local/webserver/nginx/sbin/nginx -s reopen # 重启 Nginx/usr/local/webserver/nginx/sbin/nginx -s stop # 停止 Nginx","link":"/2017/12/15/server/Nginx安装/"},{"title":"符号++的原理","text":"我们在使用java程序时，会使用带 i++ 这样一个表达式，那么他的底层原理是什么呢？ 先来看下面这段代码：12345678910111213141516171819202122232425public class IntegerPlusPlusLearning implements Runnable{ public static Integer i = new Integer(0); @Override public void run() { while (true){ synchronized (i){ if(i &lt; 200){ i++; System.out.println(i); }else{ break; } } } } public static void main(String[] args) { Thread t1 = new Thread(new IntegerPlusPlusLearning()); Thread t2 = new Thread(new IntegerPlusPlusLearning()); t1.start(); t2.start(); }} 期待的运行结果：12345123...200 顺序 输出 i=1到i=200。可惜，这是错误的答案。 它的结果可能会出现无序的，或者重复、缺少的情况。 底层原理分析1、 分析编译后的IntegerPlusPlusLearning.class文件，发现 i++ 在虚拟机中的执行原理。 通过jdk自带的javap命令工具，对IntegerPlusPlusLearning.class 进行分析javap -v IntegerPlusPlusLearning.class 可以看到输出内容中(如下图)，JVM执行 i++ 的内部逻辑。12345678910111213141516171819202122232425public void run(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=5, args_size=1 0: getstatic #2 // Field i:Ljava/lang/Integer; 3: dup 4: astore_1 5: monitorenter 6: getstatic #2 // Field i:Ljava/lang/Integer; 9: invokevirtual #3 // Method java/lang/Integer.intValue:()I 12: sipush 200 15: if_icmpge 52 18: getstatic #2 // Field i:Ljava/lang/Integer; 21: astore_2 22: getstatic #2 // Field i:Ljava/lang/Integer; 25: invokevirtual #3 // Method java/lang/Integer.intValue:()I 28: iconst_1 29: iadd 30: invokestatic #4 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 33: dup 34: putstatic #2 // Field i:Ljava/lang/Integer; 37: astore_3 38: aload_2 39: pop 22: getstatic —– 获取i的值，Integer类型。25: invokevirtual —– 这里表示调用了，Integer.intValue29: iadd —– 表示i加130: invokestatic —– 调用Integer.valueOf34: putstatic —– 赋值 这个逻辑，通过代码表达出来就是这句“i = Integer.valueOf(i.intValue() + 1)；” 2、 查看Integer的源码，当变量i的值发生变化后，发现 Integer.valueOf 每次都是新对象。123456/** Integer源码取自jdk1.8 */public static Integer valueOf(int i) { if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);} 所以最后，代码中 i 变量，是一个Integer对象，当代码中两个线程执行 i++时，实际运行时的 i++ 的实现逻辑是这样的：1i = Integer.valueOf(i.intValue() + 1)； 而Integer.valueOf每次是返回一个新的Integer对象，所以，我们的synchronized实际上没有起到你预想中的效果。","link":"/2018/11/08/javase/符号++的原理/"},{"title":"Nginx配置文件详解","text":"Nginx配置文件nginx.conf中文详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335#定义Nginx运行的用户和用户组user www www;#nginx进程数，建议设置为等于CPU总核心数。worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /usr/local/nginx/logs/error.log info;#进程pid文件pid /usr/local/nginx/logs/nginx.pid;#指定进程可以打开的最大描述符：数目#工作模式与连接数上限#这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。#现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。worker_rlimit_nofile 65535;events{ #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #4096 #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误. open_file_cache_errors on;} #设定http服务器，利用它的反向代理功能提供负载均衡支持http{ #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; #负载均衡配置 upstream piao.jd.com { #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; } #nginx的upstream目前支持4种方式的分配 #1、轮询（默认） #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 #upstream bakend { # server 192.168.0.14; # server 192.168.0.15; #} #2、weight #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 #例如： #upstream bakend { # server 192.168.0.14 weight=10; # server 192.168.0.15 weight=10; #} #2、ip_hash #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 #例如： #upstream bakend { # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; #} #3、fair（第三方） #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 #upstream backend { # server server1; # server server2; # fair; #} #4、url_hash（第三方） #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 #upstream backend { # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; #} #tips: #upstream bakend{#定义负载均衡设备的Ip及设备状态}{ # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060; # server 127.0.0.1:7070 backup; #} #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; #每个设备的状态设置为: #1.down表示单前的server暂时不参与负载 #2.weight为weight越大，负载的权重就越大。 #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 #4.fail_timeout:max_fails次失败后，暂停的时间。 #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 #虚拟主机的配置 server { #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.jd.com jd.com; index index.html index.htm index.php; root /data/www/jd; #对******进行负载均衡 # ~ 波浪线表示执行一个正则匹配，区分大小写 # ~* 表示执行一个正则匹配，不区分大小写 # ^~ 表示普通字符匹配，如果该选项匹配，只匹配该选项，不匹配别的选项，一般用来匹配目录 # = 进行普通字符精确匹配 location ~ .*.(php|php5)?$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ { expires 10d; } #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ { expires 1h; } #日志格式设定 #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； #$remote_user：用来记录客户端用户名称； #$time_local： 用来记录访问时间与时区； #$request： 用来记录请求的url与http协议； #$status： 用来记录请求状态；成功是200， #$body_bytes_sent ：记录发送给客户端文件主体内容大小； #$http_referer：用来记录从那个页面链接访问过来的； #$http_user_agent：记录客户浏览器的相关信息； #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 log_format access '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" $http_x_forwarded_for'; #定义本虚拟主机的访问日志 access_log /usr/local/nginx/logs/host.access.log main; access_log /usr/local/nginx/logs/host.access.404.log log404; #对 \"/\" 启用反向代理 location / { proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 128k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #后端服务器连接的超时时间_发起握手等候响应超时时间 #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #后端服务器数据回传时间(代理发送超时) #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的设置 #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; } #设定查看Nginx状态的地址 location /NginxStatus { stub_status on; access_log on; auth_basic \"NginxStatus\"; auth_basic_user_file confpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 } #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; } #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| pdf|xls|mp3|wma)$ { expires 15d; } location ~ .*.(js|css)?$ { expires 1h; } }}######Nginx配置文件nginx.conf中文详解#####","link":"/2017/12/15/server/Nginx配置文件详解/"},{"title":"mysql安装成服务及自启动","text":"windows1、解压该压缩包，生成3分tomcat 分别命名为 tomcat1,tomcat2,tomcat3 2、进入tomcat1/conf/目录，修改server.xml的端口。 3、进入tomcat1/bin目录，修改 service.bat 4、修改SERVICE_NAME，如下123rem Set default Service nameset SERVICE_NAME=Tomcat7set DISPLAYNAME=Apache Tomcat 7.0 %SERVICE_NAME% 5、打开CMD命令控制台，进入tomcat1/bin目录，执行服务安装命令1service.bat install 注意：不要在环境变量设置CATALINA_HOME和CATALINA_HOME，否则无法生效！ 同理安装其他Tomcat。 PS：删除服务1service.bat uninstall linux简单自启动：1vim /etc/rc.local 在 exit 0 之前添加启动命令：1/home/tomcat/bin/startup.sh","link":"/2017/04/20/server/tomcat安装成服务及自启动/"},{"title":"idea的使用","text":"1 修改对应的配置信息(缓存)地址由于我家里的电脑C盘被我设置得超级小,然后Idea默认的各种系统配置,最主要是缓存的地址,修改 ${idea.home}/bin/idea.properties 修改下面几个值. 123456789101112131415. #--------------------------------------------------------------------- 16. # Uncomment this option if you want to customize path to IDE config folder. Make sure you're using forward slashes 17. #--------------------------------------------------------------------- 18. idea.config.path=D:/dev_soft/IntelliJ IDEA 12.0.1/bin/.IntelliJIdea/config 19. 20. #--------------------------------------------------------------------- 21. # Uncomment this option if you want to customize path to IDE system folder. Make sure you're using forward slashes 22. #--------------------------------------------------------------------- 23. idea.system.path=D:/dev_soft/IntelliJ IDEA 12.0.1/bin/.IntelliJIdea/system 24. 25. #--------------------------------------------------------------------- 26. # Uncomment this option if you want to customize path to user installed plugins folder. Make sure you're using forward slashes 27. #--------------------------------------------------------------------- 28. idea.plugins.path=D:/dev_soft/IntelliJ IDEA 12.0.1/bin/.IntelliJIdea/config/plugins 2 修改快捷键 key/map 选择eclipse ,选择copy成自定义 (我还是习惯用eclipse的快捷键) 3 配置修改1、修改主题 File | Settings | Appearance &amp; Behavior | Appearance ： Theme选择 Darcula2、显示行号：Settings-&gt;Editor-&gt;Appearance标签项，勾选Show line numbers3、选择字体大小：File | Settings | Editor | Font 154、Tab换成字符串 ：File | Settings | Editor | Code Style | Java –&gt; Use tab charactor3、生成Serializable ID ，setting–&gt;Editor–&gt;Inspactions–&gt;Java | Serialization issues | Serializable class without ‘serialVersionUID’ 打上勾4、maven 工程 unable to read the metadata file for artifact 问题 :setting-&gt;maven-&gt;always update snapshot 打开,然后重新import change就搞定了. 4、代码TemplatesFile | Settings | Editor | File and Code Templates –&gt; Includes–&gt;File Header/** Company：MGTV User: huangmin DateTime: ${DATE} ${TIME}*/ File | Settings | Editor | Live Templates添加Templates group ，再添加 Live Template。private static final Logger LOGGER = LoggerFactory.getLogger($CLASS$.class);点击$CLASS$ ，点击edit variables，选择getClassName() 5、常用插件Mybatis自动转换对象插件 generateO2O 快捷键 alt+insert 快捷键提示插件Key Promoter大小写转换插件 UpperLowerCapitalize : 安装后快捷键alt+P全部大写 alt+L全部小写 alt+C开头字母大写查看maven的依赖树 Maven Helper 6、常用快捷键fori/sout/psvm+Tab.for+Tab.var+Tab Top #10切来切去：Ctrl+TabTop #9选你所想：Ctrl+WTop #8代码生成：Template/Postfix +TabTop #7发号施令：Ctrl+Shift+ATop #6无处藏身：Shift+ShiftTop #5自动完成：Ctrl+Shift+EnterTop #4创造万物：Alt+Insert 太难割舍，前三名并列吧！Top #1智能补全：Ctrl+Shift+SpaceTop #1自我修复：Alt+EnterTop #1重构一切：Ctrl+Shift+Alt+T","link":"/2017/12/05/tools/idea的使用/"},{"title":"markdown语法","text":"链接1.语法：1[淘宝网](http://www.taobao.com/) 2.例子：淘宝网 图片1.语法：1![图片标题](http://mat1.gtimg.com/pingjs/ext2020/qqindex2018/dist/img/qq_logo_2x.png) 2.例子： 标题1.语法：123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 tips：几个 # 就是几级标题，最小到六级 斜体1234 *这是斜体* *[这是斜体链接](http://www.taobao.com)* *斜体,[这是斜体链接](http://www.taobao.com)*tips：斜体和链接可以混用 为字体加颜色12345 这是&lt;label style=\"color:red\"&gt;红色&lt;/label&gt;字体 这是&lt;label style=\"color:green\"&gt;绿色&lt;/label&gt;字体 这是&lt;label style=\"color:yellow\"&gt;黄色&lt;/label&gt;字体 这是&lt;label style=\"color:blue\"&gt;蓝色&lt;/label&gt;字体&lt;label style=\"color:red\"&gt;tips:修改color为对应的颜色英文字母即可，复杂的颜色不要想了，况且大家也用不到&lt;/label&gt; 例子： 这是红色字体 这是绿色字体 这是黄色字体 这是蓝色字体 为自体加粗1**加粗**字体 例子： 加粗字体 Email1Email:&lt;yabing.zyb@alibaba-inc.com&gt; 例子： Email:yabing.zyb@alibaba-inc.com 无序排列123* list1* list2* list3 有序排列1231. list12. list23. list3 分割线123***---- - - - tips: 三种都一样 内容块1&gt; 这里的内容在内容块中 例子： 这里的内容在内容块中 代码块1234567```java public class Demo{ public static void main(String[] args) } }``` 例子： 12345public class Demo{ public static void main(String[] args){ }} 内容框1在上一行内容缩进的基础上再缩进四个空格 换行1需要换行&lt;br&gt;这是换行后的下一行 例子：需要换行这是换行后的下一行 中划线1~~中划线~~ 例子：中划线 添加脚注12这是脚注[^1][^1]: 这是脚注说明，会在文章的末尾显示. 这是脚注[^1][^1]: 这是脚注说明，会在文章的末尾显示. 表格默认表格：1234First Header | Second Header | Third Header------------ | ------------- | ------------Content Cell | Content Cell | Content CellContent Cell | Content Cell | Content Cell 例子： First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell 左右浮动表格：1234First Header | Second Header | Third Header:----------- | :-----------: | -----------:Left | Center | RightLeft | Center | Right 例子： First Header Second Header Third Header Left Center Right Left Center Right tips:默认向左对齐","link":"/2018/11/06/tools/markdown语法/"},{"title":"SSL-https配置","text":"主流证书格式介绍一般来说，主流的Web服务软件，通常都基于两种基础密码库：OpenSSL和Java。 Tomcat、Weblogic、JBoss等，使用Java提供的密码库。通过Java的Keytool工具，生成Java Keystore（JKS）格式的证书文件。 Apache、Nginx等，使用OpenSSL提供的密码库，生成PEM、KEY、CRT等格式的证书文件。 BM的产品，如Websphere、IBM Http Server（IHS）等，使用IBM产品自带的iKeyman工具，生成KDB格式的证书文件。 微软Windows Server中的Internet Information Services（IIS），使用Windows自带的证书库生成PFX格式的证书文件。 如果您在工作中遇到带有后缀扩展名的证书文件，可以简单用如下方法区分： .DER .CER : 这样的证书文件是二进制格式，只含有证书信息，不包含私钥。 .CRT : 这样的文件可以是二进制格式，也可以是文本格式，一般均为文本格式，功能与.DER/*.CER相同。 .PEM : 一般是文本格式，可以放证书或私钥，或者两者都包含。 .PEM如果只包含私钥，那一般用 *.KEY代替。 .PFX .P12 是二进制格式，同时含证书和私钥，一般有密码保护。 怎么判断是文本格式还是二进制？ 用记事本打开，如果是规则的数字字母，如—–BEGIN CERTIFICATE—–MIIE5zCCA8+gAwIBAgIQN+whYc2BgzAogau0dc3PtzANBgkqh……—–END CERTIFICATE—–就是文本的，上面的BEGIN CERTIFICATE，说明这是一个证书如果是—–BEGIN RSA PRIVATE KEY—–，说明这是一个私钥 这些证书格式之间是可以互相转换的以下提供了一些证书之间的转换方法： 将JKS转换成PFX 可以使用Keytool工具，将JKS格式转换为PFX格式。 keytool -importkeystore -srckeystore D:\\server.jks -destkeystore D:\\server.pfx -srcstoretype JKS -deststoretype PKCS12 将PFX转换为JKS 可以使用Keytool工具，将PFX格式转换为JKS格式。 keytool -importkeystore -srckeystore D:\\server.pfx -destkeystore D:\\server.jks -srcstoretype PKCS12 -deststoretype JKS 将PEM/KEY/CRT转换为PFX 使用OpenSSL工具，可以将密钥文件KEY和公钥文件CRT转化为PFX文件。 将密钥文件KEY和公钥文件CRT放到OpenSSL目录下，打开OpenSSL执行以下命令： openssl pkcs12 -export -out server.pfx -inkey server.key -in server.crt 将PFX转换为PEM/KEY/CRT 使用OpenSSL工具，可以将PFX文件转化为密钥文件KEY和公钥文件CRT。 将PFX文件放到OpenSSL目录下，打开OpenSSL执行以下命令： openssl pkcs12 -in server.pfx -nodes -out server.pem openssl rsa -in server.pem -out server.key openssl x509 -in server.pem -out server.crt 请注意 此步骤是专用于使用keytool生成私钥和CSR申请证书，并且获取到pem格式证书公钥的情况下做分离私钥使用的，所以在实际部署证书时请使用此步骤分离出来的私钥和申请下来的公钥证书做匹配使用。 云盾证书服务统一使用 PEM 格式的数字证书文件。 pem证书转为jks证书第一步：pem(需要私钥) 转为 .pfx1openssl pkcs12 -export -out server.pfx -inkey private.key -in server.pem 第二步：.pfx 转为 .jks1keytool -importkeystore -srckeystore server.pfx -destkeystore server.jks -srcstoretype PKCS12 -deststoretype JKS tomcat配置1234567&lt;Connector protocol=\"org.apache.coyote.http11.Http11NioProtocol\" port=\"443\" SSLEnabled=\"true\" maxThreads=\"150\" scheme=\"https\" secure=\"true\" keystoreFile=\"/home/websoft/key/server.jks\" keystorePass=\"123456\" clientAuth=\"false\" sslProtocol=\"TLS\" ciphers=\"TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA256\" /&gt; ngnix配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869server { listen 80; listen 443 ssl; # ssl on; #在同一个server{}里配置同时开启http和https时，不需要开启此项！ server_name dev.cmop.mgtv.com; root /home/websoft/nginx/html; ssl_certificate \"/home/websoft/key/dev/full_chain.pem\"; ssl_certificate_key \"/home/websoft/key/dev/private.key\"; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1;#指定密码为openssl支持的格式 ssl_ciphers HIGH:!aNULL:!MD5;#密码加密方式 ssl_prefer_server_ciphers on; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { }}server { listen 80; listen 443 ssl; # ssl on; #在同一个server{}里配置同时开启http和https时，不需要开启此项！ server_name book.cmop.mgtv.com; root /home/websoft/nginx/html; ssl_certificate \"/home/websoft/key/book/full_chain.pem\"; ssl_certificate_key \"/home/websoft/key/book/private.key\"; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1;#指定密码为openssl支持的格式 ssl_ciphers HIGH:!aNULL:!MD5;#密码加密方式 ssl_prefer_server_ciphers on; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { }}server { listen 80; listen 443 ssl; server_name mango.m.lrts.me; ssl_certificate \"/data/nginx/key/mango.m.lrts.me_bundle.crt\"; ssl_certificate_key \"/data/nginx/key/mango.m.lrts.me.key\"; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #启用TLS1.1、TLS1.2要求OpenSSL1.0.1及以上版本，若您的OpenSSL版本低于要求，请使用 ssl_protocols TLSv1; ssl_ciphers HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:1m; location / { proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; index index.html; proxy_pass http://127.0.0.1:3000/; }}","link":"/2018/01/08/server/SSL-https配置/"},{"title":"JavaSE源码分析-HashMap源码剖析","text":"HashMap简介HashMap是基于哈希表实现的，每一个元素是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。HashMap 实现了Serializable接口，因此它支持序列化，实现了Cloneable接口，能被克隆。 HashMap源码剖析HashMap的源码如下(加入了比较详细的注释)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754package java.util; import java.io.*; public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable { // 默认的初始容量（容量为HashMap中槽的数目）是16，且实际容量必须是2的整数次幂。 static final int DEFAULT_INITIAL_CAPACITY = 16; // 最大容量（必须是2的幂且小于2的30次方，传入容量过大将被这个值替换） static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 存储数据的Entry数组，长度是2的幂。 // HashMap采用链表法解决冲突，每一个Entry本质上是一个单向链表 transient Entry[] table; // HashMap的底层数组中已用槽的数量 transient int size; // HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*加载因子） int threshold; // 加载因子实际大小 final float loadFactor; // HashMap被改变的次数 transient volatile int modCount; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); // HashMap的最大容量只能是MAXIMUM_CAPACITY if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //加载因此不能小于0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); // 找出“大于initialCapacity”的最小的2的幂 int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; // 设置“加载因子” this.loadFactor = loadFactor; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(capacity * loadFactor); // 创建Entry数组，用来保存数据 table = new Entry[capacity]; init(); } // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } // 默认构造函数。 public HashMap() { // 设置“加载因子”为默认加载因子0.75 this.loadFactor = DEFAULT_LOAD_FACTOR; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // 创建Entry数组，用来保存数据 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); } // 包含“子Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) { this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); // 将m中的全部元素逐个添加到HashMap中 putAllForCreate(m); } //求hash值的方法，重新计算hash值 static int hash(int h) { h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } // 返回h在数组中的索引值，这里用&amp;代替取模，旨在提升效率 // h &amp; (length-1)保证返回值的小于length static int indexFor(int h, int length) { return h &amp; (length-1); } public int size() { return size; } public boolean isEmpty() { return size == 0; } // 获取key对应的value public V get(Object key) { if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; //判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; } //没找到则返回null return null; } // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() { for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) return e.value; } return null; } // HashMap是否包含key public boolean containsKey(Object key) { return getEntry(key) != null; } // 返回“键为key”的键值对 final Entry&lt;K,V&gt; getEntry(Object key) { // 获取哈希值 // HashMap将“key为null”的元素存储在table[0]位置，“key不为null”的则调用hash()计算哈希值 int hash = (key == null) ? 0 : hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } return null; } // 将“key-value”添加到HashMap中 public V put(K key, V value) { // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; } // putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) { for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; } // 创建HashMap对应的“添加方法”， // 它和put()不同。putForCreate()是内部方法，它被构造函数等调用，用来创建HashMap // 而put()是对外提供的往HashMap中添加元素的方法。 private void putForCreate(K key, V value) { int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); // 若该HashMap表中存在“键值等于key”的元素，则替换该元素的value值 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { e.value = value; return; } } // 若该HashMap表中不存在“键值等于key”的元素，则将该key-value添加到HashMap中 createEntry(hash, key, value, i); } // 将“m”中的全部元素都添加到HashMap中。 // 该方法被内部的构造HashMap的方法所调用。 private void putAllForCreate(Map&lt;? extends K, ? extends V&gt; m) { // 利用迭代器将元素逐个添加到HashMap中 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) { Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); putForCreate(e.getKey(), e.getValue()); } } // 重新调整HashMap的大小，newCapacity是调整后的容量 void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; //如果就容量已经达到了最大值，则不能再扩容，直接返回 if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); } // 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) { src[j] = null; do { Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } } } // 将\"m\"的全部元素都添加到HashMap中 public void putAll(Map&lt;? extends K, ? extends V&gt; m) { // 有效性判断 int numKeysToBeAdded = m.size(); if (numKeysToBeAdded == 0) return; // 计算容量是否足够， // 若“当前阀值容量 &lt; 需要的容量”，则将容量x2。 if (numKeysToBeAdded &gt; threshold) { int targetCapacity = (int)(numKeysToBeAdded / loadFactor + 1); if (targetCapacity &gt; MAXIMUM_CAPACITY) targetCapacity = MAXIMUM_CAPACITY; int newCapacity = table.length; while (newCapacity &lt; targetCapacity) newCapacity &lt;&lt;= 1; if (newCapacity &gt; table.length) resize(newCapacity); } // 通过迭代器，将“m”中的元素逐个添加到HashMap中。 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) { Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); put(e.getKey(), e.getValue()); } } // 删除“键为key”元素 public V remove(Object key) { Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value); } // 删除“键为key”的元素 final Entry&lt;K,V&gt; removeEntryForKey(Object key) { // 获取哈希值。若key为null，则哈希值为0；否则调用hash()进行计算 int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中“键为key”的元素 // 本质是“删除单向链表中的节点” while (e != null) { Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; } prev = e; e = next; } return e; } // 删除“键值对” final Entry&lt;K,V&gt; removeMapping(Object o) { if (!(o instanceof Map.Entry)) return null; Map.Entry&lt;K,V&gt; entry = (Map.Entry&lt;K,V&gt;) o; Object key = entry.getKey(); int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中的“键值对e” // 本质是“删除单向链表中的节点” while (e != null) { Entry&lt;K,V&gt; next = e.next; if (e.hash == hash &amp;&amp; e.equals(entry)) { modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; } prev = e; e = next; } return e; } // 清空HashMap，将所有的元素设为null public void clear() { modCount++; Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) tab[i] = null; size = 0; } // 是否包含“值为value”的元素 public boolean containsValue(Object value) { // 若“value为null”，则调用containsNullValue()查找 if (value == null) return containsNullValue(); // 若“value不为null”，则查找HashMap中是否有值为value的节点。 Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false; } // 是否包含null值 private boolean containsNullValue() { Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (e.value == null) return true; return false; } // 克隆一个HashMap，并返回Object对象 public Object clone() { HashMap&lt;K,V&gt; result = null; try { result = (HashMap&lt;K,V&gt;)super.clone(); } catch (CloneNotSupportedException e) { // assert false; } result.table = new Entry[table.length]; result.entrySet = null; result.modCount = 0; result.size = 0; result.init(); // 调用putAllForCreate()将全部元素添加到HashMap中 result.putAllForCreate(this); return result; } // Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括\"哈希值(h)\", \"键(k)\", \"值(v)\", \"下一节点(n)\" Entry(int h, K k, V v, Entry&lt;K,V&gt; n) { value = v; next = n; key = k; hash = h; } public final K getKey() { return key; } public final V getValue() { return value; } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) { Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; } return false; } // 实现hashCode() public final int hashCode() { return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); } public final String toString() { return getKey() + \"=\" + getValue(); } // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) { } // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) { } } // 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) { // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); } // 创建Entry。将“key-value”插入指定位置。 void createEntry(int hash, K key, V value, int bucketIndex) { // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); size++; } // HashIterator是HashMap迭代器的抽象出来的父类，实现了公共了函数。 // 它包含“key迭代器(KeyIterator)”、“Value迭代器(ValueIterator)”和“Entry迭代器(EntryIterator)”3个子类。 private abstract class HashIterator&lt;E&gt; implements Iterator&lt;E&gt; { // 下一个元素 Entry&lt;K,V&gt; next; // expectedModCount用于实现fast-fail机制。 int expectedModCount; // 当前索引 int index; // 当前元素 Entry&lt;K,V&gt; current; HashIterator() { expectedModCount = modCount; if (size &gt; 0) { // advance to first entry Entry[] t = table; // 将next指向table中第一个不为null的元素。 // 这里利用了index的初始值为0，从0开始依次向后遍历，直到找到不为null的元素就退出循环。 while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; } } public final boolean hasNext() { return next != null; } // 获取下一个元素 final Entry&lt;K,V&gt; nextEntry() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); Entry&lt;K,V&gt; e = next; if (e == null) throw new NoSuchElementException(); // 注意！！！ // 一个Entry就是一个单向链表 // 若该Entry的下一个节点不为空，就将next指向下一个节点; // 否则，将next指向下一个链表(也是下一个Entry)的不为null的节点。 if ((next = e.next) == null) { Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; } current = e; return e; } // 删除当前元素 public void remove() { if (current == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); Object k = current.key; current = null; HashMap.this.removeEntryForKey(k); expectedModCount = modCount; } } // value的迭代器 private final class ValueIterator extends HashIterator&lt;V&gt; { public V next() { return nextEntry().value; } } // key的迭代器 private final class KeyIterator extends HashIterator&lt;K&gt; { public K next() { return nextEntry().getKey(); } } // Entry的迭代器 private final class EntryIterator extends HashIterator&lt;Map.Entry&lt;K,V&gt;&gt; { public Map.Entry&lt;K,V&gt; next() { return nextEntry(); } } // 返回一个“key迭代器” Iterator&lt;K&gt; newKeyIterator() { return new KeyIterator(); } // 返回一个“value迭代器” Iterator&lt;V&gt; newValueIterator() { return new ValueIterator(); } // 返回一个“entry迭代器” Iterator&lt;Map.Entry&lt;K,V&gt;&gt; newEntryIterator() { return new EntryIterator(); } // HashMap的Entry对应的集合 private transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet = null; // 返回“key的集合”，实际上返回一个“KeySet对象” public Set&lt;K&gt; keySet() { Set&lt;K&gt; ks = keySet; return (ks != null ? ks : (keySet = new KeySet())); } // Key对应的集合 // KeySet继承于AbstractSet，说明该集合中没有重复的Key。 private final class KeySet extends AbstractSet&lt;K&gt; { public Iterator&lt;K&gt; iterator() { return newKeyIterator(); } public int size() { return size; } public boolean contains(Object o) { return containsKey(o); } public boolean remove(Object o) { return HashMap.this.removeEntryForKey(o) != null; } public void clear() { HashMap.this.clear(); } } // 返回“value集合”，实际上返回的是一个Values对象 public Collection&lt;V&gt; values() { Collection&lt;V&gt; vs = values; return (vs != null ? vs : (values = new Values())); } // “value集合” // Values继承于AbstractCollection，不同于“KeySet继承于AbstractSet”， // Values中的元素能够重复。因为不同的key可以指向相同的value。 private final class Values extends AbstractCollection&lt;V&gt; { public Iterator&lt;V&gt; iterator() { return newValueIterator(); } public int size() { return size; } public boolean contains(Object o) { return containsValue(o); } public void clear() { HashMap.this.clear(); } } // 返回“HashMap的Entry集合” public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() { return entrySet0(); } // 返回“HashMap的Entry集合”，它实际是返回一个EntrySet对象 private Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet0() { Set&lt;Map.Entry&lt;K,V&gt;&gt; es = entrySet; return es != null ? es : (entrySet = new EntrySet()); } // EntrySet对应的集合 // EntrySet继承于AbstractSet，说明该集合中没有重复的EntrySet。 private final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; { public Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() { return newEntryIterator(); } public boolean contains(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;K,V&gt; e = (Map.Entry&lt;K,V&gt;) o; Entry&lt;K,V&gt; candidate = getEntry(e.getKey()); return candidate != null &amp;&amp; candidate.equals(e); } public boolean remove(Object o) { return removeMapping(o) != null; } public int size() { return size; } public void clear() { HashMap.this.clear(); } } // java.io.Serializable的写入函数 // 将HashMap的“总的容量，实际容量，所有的Entry”都写入到输出流中 private void writeObject(java.io.ObjectOutputStream s) throws IOException { Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = (size &gt; 0) ? entrySet0().iterator() : null; // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); // Write out number of buckets s.writeInt(table.length); // Write out size (number of Mappings) s.writeInt(size); // Write out keys and values (alternating) if (i != null) { while (i.hasNext()) { Map.Entry&lt;K,V&gt; e = i.next(); s.writeObject(e.getKey()); s.writeObject(e.getValue()); } } } private static final long serialVersionUID = 362498820763181265L; // java.io.Serializable的读取函数：根据写入方式读出 // 将HashMap的“总的容量，实际容量，所有的Entry”依次读出 private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException { // Read in the threshold, loadfactor, and any hidden stuff s.defaultReadObject(); // Read in number of buckets and allocate the bucket array; int numBuckets = s.readInt(); table = new Entry[numBuckets]; init(); // Give subclass a chance to do its thing. // Read in size (number of Mappings) int size = s.readInt(); // Read the keys and values, and put the mappings in the HashMap for (int i=0; i&lt;size; i++) { K key = (K) s.readObject(); V value = (V) s.readObject(); putForCreate(key, value); } } // 返回“HashMap总的容量” int capacity() { return table.length; } // 返回“HashMap的加载因子” float loadFactor() { return loadFactor; } } 几点总结1、首先要清楚HashMap的存储结构，如下图所示：图中，紫色部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 2、首先看链表中节点的数据结构：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括\"哈希值(h)\", \"键(k)\", \"值(v)\", \"下一节点(n)\" Entry(int h, K k, V v, Entry&lt;K,V&gt; n) { value = v; next = n; key = k; hash = h; } public final K getKey() { return key; } public final V getValue() { return value; } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) { Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; } return false; } // 实现hashCode() public final int hashCode() { return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); } public final String toString() { return getKey() + \"=\" + getValue(); } // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) { } // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) { } } 它的结构元素除了key、value、hash外，还有next，next指向下一个节点。另外，这里覆写了equals和hashCode方法来保证键值对的独一无二。3、HashMap共有四个构造方法。构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16），加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。下面说下加载因子，如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。另外，无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方4、HashMap中key和value都允许为null。5、要重点分析下HashMap中用的最多的两个方法put和get。先从比较简单的get方法着手，源码如下： 12345678910111213141516171819202122232425262728// 获取key对应的value public V get(Object key) { if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; /判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; } 没找到则返回null return null; } // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() { for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) return e.value; } return null; } 首先，如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中，当然不一定是存放在头结点table[0]中。如果key不为null，则先求的key的hash值，根据hash值找到在table中的索引，在该索引对应的单链表中查找是否有键值对的key与目标key相等，有就返回对应的value，没有则返回null。put方法稍微复杂些，代码如下：12345678910111213141516171819202122232425 // 将“key-value”添加到HashMap中 public V put(K key, V value) { // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; } 如果key为null，则将其添加到table[0]对应的链表中，putForNullKey的源码如下：123456789101112131415// putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) { for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; } 如果key不为null，则同样先求出key的hash值，根据hash值得出在table中的索引，而后遍历对应的单链表，如果单链表中存在与目标key相等的键值对，则将新的value覆盖旧的value，比将旧的value返回，如果找不到与目标key相等的键值对，或者该单链表为空，则将该键值对插入到改单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有addEntry方法实现的，它的源码如下： 1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) { // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); } 注意这里倒数第三行的构造方法，将key-value键值对赋给table[bucketIndex]，并将其next指向元素e，这便将key-value放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次put键值对的时候，总是将新的该键值对放在table[bucketIndex]处（即头结点处）。两外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。6、关于扩容。上面我们看到了扩容的方法，resize方法，它的源码如下：12345678910111213141516// 重新调整HashMap的大小，newCapacity是调整后的单位 void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); } 很明显，是新建了一个HashMap的底层数组，而后调用transfer方法，将就HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。transfer方法的源码如下： 123456789101112131415161718// 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) { src[j] = null; do { Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } } } 很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap的时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。 7、注意containsKey方法和containsValue方法。前者直接可以通过key的哈希值将搜索范围定位到指定索引对应的链表，而后者要对哈希数组的每个链表进行搜索。 8、我们重点来分析下求hash值和索引值的方法，这两个方法便是HashMap设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。计算哈希值的方法如下：1234static int hash(int h) { h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } 它只是一个数学公式，IDK这样设计对hash值的计算，自然有它的好处，至于为什么这样设计，我们这里不去追究，只要明白一点，用的位的操作使hash值的计算效率很高。 由hash值找到对应索引的方法如下：123static int indexFor(int h, int length) { return h &amp; (length-1); } 这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法），Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&amp;(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&amp;(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&amp;(length-1)的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&amp;(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。","link":"/2018/11/23/javase/HashMap源码剖析/"},{"title":"Hexo博客（一）在GitHub搭建博客","text":"1 Git准备1.1 申请GitHub1、进入https://github.com/ 2、点击”New repository”，新建一个仓库。 注意：输入Repository name:yourname.github.io(yourname与你的注册用户名一致,这个就是你博客的域名了) 3、启用GitHub Page 点击右边的“Setting”菜单进入设置,点击”Launch automatic page generator”，然后再点击点击底部的”Continue to layouts”。最后点击”Publish page”,发布github默认生成的一个静态站点。 4、验证邮箱 点击右上角个人设置的“Setting”菜单进入设置，再点击Emails,点击”Send verification Email”发送验证邮件。进入你的邮箱，查收验证邮件进行验证。 1.2 安装Git1.2.1下载https://git-for-windows.github.io/ 1.2.2安装 安装过程中，询问是否修改环境变量，选择“Use Git Bash Only”. 即只在msysGit提供的Shell (NOTE: 这个步骤最好选择第二项“Use Git from the Windows Command Prompt”， 这样在Windows的命令行cmd中也可以运行git命令了。这样会对以后的一些操作带来方便，比如Win7下安装配置gVim) 配置行结束标记，保持默认“Checkout Windows-style, commit Unix-style line endings”. 1.2.3中文乱码问题解决方法ls 不能显示中文目录 解决办法：在git/git-completion.bash中增加一行【4】： 1alias ls='ls --show-control-chars --color=auto' 另外，Git Shell 不支持 ls -l的缩写形式ll，也为其添加一个alias 1alias ll='ls -l' 1.2.4 运行 Git 前的配置 配置你个人的用户名称和电子邮件地址,打开git bash。 12$git config --global user.name \"xxx\"$git config --global user.email xxx@example.com 配置GitHub SSH （1）首先使用 ssh-keygen 生成 SSH 密钥123456cd ~/.ssh/Administrator@THINKPAD ~/.ssh ls known_hostsssh-keygen -t rsa -C \"youremail@example.com\" 有提示，直接回车即可。生成key以后检查下”~”目录下的.ssh目录下是否多了2个文件 。 把 id_rsa.pub 中的全部内容复制，包括最后的一个换行。或者用命令：1clip &lt; ~/.ssh/id_rsa.pub （2）配置Github SSH。 登陆GitHub-&gt;Settings-&gt;“SSH Keys”，然后，点“Add SSH Key”，起个Title，在Key文本框里粘贴id_rsa.pub文件的内容，点“Add Key”。 （3）测试是否可以连接到github1234ssh git@github.comHi imsofter! You've successfully authenticated, but GitHub does not provide shell access.Connection to github.com closed. 现在可以将代码推上github上了。 2 安装node.js下载：http://nodejs.org/download/ 可以下载 node-v4.2.1-x64.msi 安装时直接保持默认配置即可。 3 安装Hexo关于Hexo的安装配置过程，请以官方Hexo给出的步骤为准,大致如下： 在电脑上E盘新建一个blog文件夹,该文件夹用于存放你的博客文件,然后右键单击选择“Git Bash” 3.1 Installation打开Git命令行，执行如下命令1$ npm install hexo-cli g 3.2 Quick Start1. Setup your blog 在电脑中建立一个名字叫「Hexo」的文件夹（比如我建在了E:\\Hexo），然后在此文件夹中右键打开Git Bash。执行下面的命令12345$ cd /e/blog$ hexo init[info] Copying data[info] You are almost done! Don't forget to run `npm install` before you start blogging with Hexo! Hexo随后会自动在目标文件夹建立网站所需要的文件。 然后按照提示，运行 npm install（在 /e/blog下）1npm install 会在e:\\blog目录中生成 node_modules。 安装其它插件12345678npm install hexo-server --savenpm install hexo-admin --savenpm install hexo-generator-archive --savenpm install hexo-generator-feed --savenpm install hexo-generator-search --savenpm install hexo-generator-tag --savenpm install hexo-deployer-git --savenpm install hexo-generator-sitemap --save 2. Start the server 运行下面的命令（在 /E/Hexo下）12$ hexo server[info] Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 表明Hexo Server已经启动了，在浏览器中打开 http://localhost:4000/，这时可以看到Hexo已为你生成了一篇blog。 你可以按Ctrl+C 停止Server。 3. Create a new post 新打开一个git bash命令行窗口，cd到/e/blog下，执行下面的命令12$ hexo new \"My New Post\"[info] File created at e:\\blog\\source\\_posts\\My-New-Post.md 刷新http://localhost:4000/，可以发现已生成了一篇新文章 “My New Post”。 NOTE： 有一个问题，发现 “My New Post” 被发了2遍，在Hexo server所在的git bash窗口也能看到create了2次。1234$ hexo server[info] Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.[create] e:\\blog\\source\\_posts\\My-New-Post.md[create] e:\\blog\\source\\_posts\\My-New-Post.md 经验证，在hexo new “My New Post” 时，如果按Ctrl+C将hexo server停掉，就不会出现发2次的问题了。 所以，在hexo new文章时，需要stop server。 4. Generate static files 执行下面的命令，将markdown文件生成静态网页。 该命令执行完后，会在 E:\\blog\\public\\ 目录下生成一系列html，css等文件。 5. 编辑文章 hexo new “My New Post”会在E:\\blog\\source_posts目录下生成一个markdown文件：My-New-Post.md 可以使用一个支持markdown语法的编辑器（比如 Sublime Text 2）来编辑该文件。 6. 部署到Github 运行 npm install hexo-deployer-git --save 安装git支持 部署到Github前需要配置_config.yml文件，首先找到下面的内容1234# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: 然后将它们修改为123456# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: github repository: git@github.com:hmiter/hmiter.github.io.git branch: master NOTE1: Repository：必须是SSH形式的url（git@github.com:hmiter/hmiter.github.io.git），而不能是HTTPS形式的url（https://github.com/hmiter/hmiter.github.io.git），否则会出现错误：123$ hexo deploy[info] Start deploying: github[error] https://github.com/hmiter/hmiter.github.io is not a valid repositor URL! 使用SSH url，如果电脑没有开放SSH 端口，会致部署失败。12fatal: Could not read from remote repository.Please make sure you have the correct access rights and the repository exists. NOTE2： 如果你是为一个项目制作网站，那么需要把branch设置为gh-pages。 NOTE3： hexo3.0以上的版本type为git 7. 测试 当部署完成后，在浏览器中打开http://hmiter.github.io/（https://hmiter.github.io/） ，正常显示网页，表明部署成功。 8. 总结：部署步骤 每次部署的步骤，可按以下三步来进行。123hexo cleanhexo generatehexo deploy 9. 总结：本地调试 在执行下面的命令后，12$ hexo g #生成$ hexo s #启动本地服务，进行文章预览调试 浏览器输入http://localhost:4000，查看搭建效果。此后的每次变更_config.yml 文件或者新建文件都可以先用此命令调试，尤其是当你想调试新添加的主题时。 可以用简化的一条命令1hexo s -g 10.hexo命令缩写hexo支持命令缩写，如下所示。hexo g等价于hexo generate1234hexo g：hexo generatehexo c：hexo cleanhexo s：hexo serverhexo d：hexo deploy","link":"/2017/01/04/hexo/Hexo博客（一）在GitHub搭建博客/"},{"title":"JavaSE源码分析-Hashtable源码剖析","text":"Hashtable简介Hashtable同样是基于哈希表实现的，同样每个元素是一个key-value对，其内部也是通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。Hashtable也是JDK1.0引入的类，是线程安全的，能用于多线程环境中。Hashtable同样实现了Serializable接口，它支持序列化，实现了Cloneable接口，能被克隆。 HashTable源码剖析Hashtable的源码的很多实现都与HashMap差不多，源码如下（加入了比较详细的注释）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789package java.util; import java.io.*; public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable { // 保存key-value的数组。 // Hashtable同样采用单链表解决冲突，每一个Entry本质上是一个单向链表 private transient Entry[] table; // Hashtable中键值对的数量 private transient int count; // 阈值，用于判断是否需要调整Hashtable的容量（threshold = 容量*加载因子） private int threshold; // 加载因子 private float loadFactor; // Hashtable被改变的次数，用于fail-fast机制的实现 private transient int modCount = 0; // 序列版本号 private static final long serialVersionUID = 1421746759512286392L; // 指定“容量大小”和“加载因子”的构造函数 public Hashtable(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal Load: \"+loadFactor); if (initialCapacity==0) initialCapacity = 1; this.loadFactor = loadFactor; table = new Entry[initialCapacity]; threshold = (int)(initialCapacity * loadFactor); } // 指定“容量大小”的构造函数 public Hashtable(int initialCapacity) { this(initialCapacity, 0.75f); } // 默认构造函数。 public Hashtable() { // 默认构造函数，指定的容量大小是11；加载因子是0.75 this(11, 0.75f); } // 包含“子Map”的构造函数 public Hashtable(Map&lt;? extends K, ? extends V&gt; t) { this(Math.max(2*t.size(), 11), 0.75f); // 将“子Map”的全部元素都添加到Hashtable中 putAll(t); } public synchronized int size() { return count; } public synchronized boolean isEmpty() { return count == 0; } // 返回“所有key”的枚举对象 public synchronized Enumeration&lt;K&gt; keys() { return this.&lt;K&gt;getEnumeration(KEYS); } // 返回“所有value”的枚举对象 public synchronized Enumeration&lt;V&gt; elements() { return this.&lt;V&gt;getEnumeration(VALUES); } // 判断Hashtable是否包含“值(value)” public synchronized boolean contains(Object value) { //注意，Hashtable中的value不能是null， // 若是null的话，抛出异常! if (value == null) { throw new NullPointerException(); } // 从后向前遍历table数组中的元素(Entry) // 对于每个Entry(单向链表)，逐个遍历，判断节点的值是否等于value Entry tab[] = table; for (int i = tab.length ; i-- &gt; 0 ;) { for (Entry&lt;K,V&gt; e = tab[i] ; e != null ; e = e.next) { if (e.value.equals(value)) { return true; } } } return false; } public boolean containsValue(Object value) { return contains(value); } // 判断Hashtable是否包含key public synchronized boolean containsKey(Object key) { Entry tab[] = table; //计算hash值，直接用key的hashCode代替 int hash = key.hashCode(); // 计算在数组中的索引值 int index = (hash &amp; 0x7FFFFFFF) % tab.length; // 找到“key对应的Entry(链表)”，然后在链表中找出“哈希值”和“键值”与key都相等的元素 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { return true; } } return false; } // 返回key对应的value，没有的话返回null public synchronized V get(Object key) { Entry tab[] = table; int hash = key.hashCode(); // 计算索引值， int index = (hash &amp; 0x7FFFFFFF) % tab.length; // 找到“key对应的Entry(链表)”，然后在链表中找出“哈希值”和“键值”与key都相等的元素 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { return e.value; } } return null; } // 调整Hashtable的长度，将长度变成原来的2倍+1 protected void rehash() { int oldCapacity = table.length; Entry[] oldMap = table; //创建新容量大小的Entry数组 int newCapacity = oldCapacity * 2 + 1; Entry[] newMap = new Entry[newCapacity]; modCount++; threshold = (int)(newCapacity * loadFactor); table = newMap; //将“旧的Hashtable”中的元素复制到“新的Hashtable”中 for (int i = oldCapacity ; i-- &gt; 0 ;) { for (Entry&lt;K,V&gt; old = oldMap[i] ; old != null ; ) { Entry&lt;K,V&gt; e = old; old = old.next; //重新计算index int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = newMap[index]; newMap[index] = e; } } } // 将“key-value”添加到Hashtable中 public synchronized V put(K key, V value) { // Hashtable中不能插入value为null的元素！！！ if (value == null) { throw new NullPointerException(); } // 若“Hashtable中已存在键为key的键值对”， // 则用“新的value”替换“旧的value” Entry tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { V old = e.value; e.value = value; return old; } } // 若“Hashtable中不存在键为key的键值对”， // 将“修改统计数”+1 modCount++; // 若“Hashtable实际容量” &gt; “阈值”(阈值=总的容量 * 加载因子) // 则调整Hashtable的大小 if (count &gt;= threshold) { rehash(); tab = table; index = (hash &amp; 0x7FFFFFFF) % tab.length; } //将新的key-value对插入到tab[index]处（即链表的头结点） Entry&lt;K,V&gt; e = tab[index]; tab[index] = new Entry&lt;K,V&gt;(hash, key, value, e); count++; return null; } // 删除Hashtable中键为key的元素 public synchronized V remove(Object key) { Entry tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; //从table[index]链表中找出要删除的节点，并删除该节点。 //因为是单链表，因此要保留带删节点的前一个节点，才能有效地删除节点 for (Entry&lt;K,V&gt; e = tab[index], prev = null ; e != null ; prev = e, e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { modCount++; if (prev != null) { prev.next = e.next; } else { tab[index] = e.next; } count--; V oldValue = e.value; e.value = null; return oldValue; } } return null; } // 将“Map(t)”的中全部元素逐一添加到Hashtable中 public synchronized void putAll(Map&lt;? extends K, ? extends V&gt; t) { for (Map.Entry&lt;? extends K, ? extends V&gt; e : t.entrySet()) put(e.getKey(), e.getValue()); } // 清空Hashtable // 将Hashtable的table数组的值全部设为null public synchronized void clear() { Entry tab[] = table; modCount++; for (int index = tab.length; --index &gt;= 0; ) tab[index] = null; count = 0; } // 克隆一个Hashtable，并以Object的形式返回。 public synchronized Object clone() { try { Hashtable&lt;K,V&gt; t = (Hashtable&lt;K,V&gt;) super.clone(); t.table = new Entry[table.length]; for (int i = table.length ; i-- &gt; 0 ; ) { t.table[i] = (table[i] != null) ? (Entry&lt;K,V&gt;) table[i].clone() : null; } t.keySet = null; t.entrySet = null; t.values = null; t.modCount = 0; return t; } catch (CloneNotSupportedException e) { throw new InternalError(); } } public synchronized String toString() { int max = size() - 1; if (max == -1) return \"{}\"; StringBuilder sb = new StringBuilder(); Iterator&lt;Map.Entry&lt;K,V&gt;&gt; it = entrySet().iterator(); sb.append('{'); for (int i = 0; ; i++) { Map.Entry&lt;K,V&gt; e = it.next(); K key = e.getKey(); V value = e.getValue(); sb.append(key == this ? \"(this Map)\" : key.toString()); sb.append('='); sb.append(value == this ? \"(this Map)\" : value.toString()); if (i == max) return sb.append('}').toString(); sb.append(\", \"); } } // 获取Hashtable的枚举类对象 // 若Hashtable的实际大小为0,则返回“空枚举类”对象； // 否则，返回正常的Enumerator的对象。 private &lt;T&gt; Enumeration&lt;T&gt; getEnumeration(int type) { if (count == 0) { return (Enumeration&lt;T&gt;)emptyEnumerator; } else { return new Enumerator&lt;T&gt;(type, false); } } // 获取Hashtable的迭代器 // 若Hashtable的实际大小为0,则返回“空迭代器”对象； // 否则，返回正常的Enumerator的对象。(Enumerator实现了迭代器和枚举两个接口) private &lt;T&gt; Iterator&lt;T&gt; getIterator(int type) { if (count == 0) { return (Iterator&lt;T&gt;) emptyIterator; } else { return new Enumerator&lt;T&gt;(type, true); } } // Hashtable的“key的集合”。它是一个Set，没有重复元素 private transient volatile Set&lt;K&gt; keySet = null; // Hashtable的“key-value的集合”。它是一个Set，没有重复元素 private transient volatile Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet = null; // Hashtable的“key-value的集合”。它是一个Collection，可以有重复元素 private transient volatile Collection&lt;V&gt; values = null; // 返回一个被synchronizedSet封装后的KeySet对象 // synchronizedSet封装的目的是对KeySet的所有方法都添加synchronized，实现多线程同步 public Set&lt;K&gt; keySet() { if (keySet == null) keySet = Collections.synchronizedSet(new KeySet(), this); return keySet; } // Hashtable的Key的Set集合。 // KeySet继承于AbstractSet，所以，KeySet中的元素没有重复的。 private class KeySet extends AbstractSet&lt;K&gt; { public Iterator&lt;K&gt; iterator() { return getIterator(KEYS); } public int size() { return count; } public boolean contains(Object o) { return containsKey(o); } public boolean remove(Object o) { return Hashtable.this.remove(o) != null; } public void clear() { Hashtable.this.clear(); } } // 返回一个被synchronizedSet封装后的EntrySet对象 // synchronizedSet封装的目的是对EntrySet的所有方法都添加synchronized，实现多线程同步 public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() { if (entrySet==null) entrySet = Collections.synchronizedSet(new EntrySet(), this); return entrySet; } // Hashtable的Entry的Set集合。 // EntrySet继承于AbstractSet，所以，EntrySet中的元素没有重复的。 private class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; { public Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() { return getIterator(ENTRIES); } public boolean add(Map.Entry&lt;K,V&gt; o) { return super.add(o); } // 查找EntrySet中是否包含Object(0) // 首先，在table中找到o对应的Entry链表 // 然后，查找Entry链表中是否存在Object public boolean contains(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry entry = (Map.Entry)o; Object key = entry.getKey(); Entry[] tab = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry e = tab[index]; e != null; e = e.next) if (e.hash==hash &amp;&amp; e.equals(entry)) return true; return false; } // 删除元素Object(0) // 首先，在table中找到o对应的Entry链表 // 然后，删除链表中的元素Object public boolean remove(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;K,V&gt; entry = (Map.Entry&lt;K,V&gt;) o; K key = entry.getKey(); Entry[] tab = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index], prev = null; e != null; prev = e, e = e.next) { if (e.hash==hash &amp;&amp; e.equals(entry)) { modCount++; if (prev != null) prev.next = e.next; else tab[index] = e.next; count--; e.value = null; return true; } } return false; } public int size() { return count; } public void clear() { Hashtable.this.clear(); } } // 返回一个被synchronizedCollection封装后的ValueCollection对象 // synchronizedCollection封装的目的是对ValueCollection的所有方法都添加synchronized，实现多线程同步 public Collection&lt;V&gt; values() { if (values==null) values = Collections.synchronizedCollection(new ValueCollection(), this); return values; } // Hashtable的value的Collection集合。 // ValueCollection继承于AbstractCollection，所以，ValueCollection中的元素可以重复的。 private class ValueCollection extends AbstractCollection&lt;V&gt; { public Iterator&lt;V&gt; iterator() { return getIterator(VALUES); } public int size() { return count; } public boolean contains(Object o) { return containsValue(o); } public void clear() { Hashtable.this.clear(); } } // 重新equals()函数 // 若两个Hashtable的所有key-value键值对都相等，则判断它们两个相等 public synchronized boolean equals(Object o) { if (o == this) return true; if (!(o instanceof Map)) return false; Map&lt;K,V&gt; t = (Map&lt;K,V&gt;) o; if (t.size() != size()) return false; try { // 通过迭代器依次取出当前Hashtable的key-value键值对 // 并判断该键值对，存在于Hashtable中。 // 若不存在，则立即返回false；否则，遍历完“当前Hashtable”并返回true。 Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); while (i.hasNext()) { Map.Entry&lt;K,V&gt; e = i.next(); K key = e.getKey(); V value = e.getValue(); if (value == null) { if (!(t.get(key)==null &amp;&amp; t.containsKey(key))) return false; } else { if (!value.equals(t.get(key))) return false; } } } catch (ClassCastException unused) { return false; } catch (NullPointerException unused) { return false; } return true; } // 计算Entry的hashCode // 若 Hashtable的实际大小为0 或者 加载因子&lt;0，则返回0。 // 否则，返回“Hashtable中的每个Entry的key和value的异或值 的总和”。 public synchronized int hashCode() { int h = 0; if (count == 0 || loadFactor &lt; 0) return h; // Returns zero loadFactor = -loadFactor; // Mark hashCode computation in progress Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) for (Entry e = tab[i]; e != null; e = e.next) h += e.key.hashCode() ^ e.value.hashCode(); loadFactor = -loadFactor; // Mark hashCode computation complete return h; } // java.io.Serializable的写入函数 // 将Hashtable的“总的容量，实际容量，所有的Entry”都写入到输出流中 private synchronized void writeObject(java.io.ObjectOutputStream s) throws IOException { // Write out the length, threshold, loadfactor s.defaultWriteObject(); // Write out length, count of elements and then the key/value objects s.writeInt(table.length); s.writeInt(count); for (int index = table.length-1; index &gt;= 0; index--) { Entry entry = table[index]; while (entry != null) { s.writeObject(entry.key); s.writeObject(entry.value); entry = entry.next; } } } // java.io.Serializable的读取函数：根据写入方式读出 // 将Hashtable的“总的容量，实际容量，所有的Entry”依次读出 private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException { // Read in the length, threshold, and loadfactor s.defaultReadObject(); // Read the original length of the array and number of elements int origlength = s.readInt(); int elements = s.readInt(); // Compute new size with a bit of room 5% to grow but // no larger than the original size. Make the length // odd if it's large enough, this helps distribute the entries. // Guard against the length ending up zero, that's not valid. int length = (int)(elements * loadFactor) + (elements / 20) + 3; if (length &gt; elements &amp;&amp; (length &amp; 1) == 0) length--; if (origlength &gt; 0 &amp;&amp; length &gt; origlength) length = origlength; Entry[] table = new Entry[length]; count = 0; // Read the number of elements and then all the key/value objects for (; elements &gt; 0; elements--) { K key = (K)s.readObject(); V value = (V)s.readObject(); // synch could be eliminated for performance reconstitutionPut(table, key, value); } this.table = table; } private void reconstitutionPut(Entry[] tab, K key, V value) throws StreamCorruptedException { if (value == null) { throw new java.io.StreamCorruptedException(); } // Makes sure the key is not already in the hashtable. // This should not happen in deserialized version. int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { throw new java.io.StreamCorruptedException(); } } // Creates the new entry. Entry&lt;K,V&gt; e = tab[index]; tab[index] = new Entry&lt;K,V&gt;(hash, key, value, e); count++; } // Hashtable的Entry节点，它本质上是一个单向链表。 // 也因此，我们才能推断出Hashtable是由拉链法实现的散列表 private static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { // 哈希值 int hash; K key; V value; // 指向的下一个Entry，即链表的下一个节点 Entry&lt;K,V&gt; next; // 构造函数 protected Entry(int hash, K key, V value, Entry&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } protected Object clone() { return new Entry&lt;K,V&gt;(hash, key, value, (next==null ? null : (Entry&lt;K,V&gt;) next.clone())); } public K getKey() { return key; } public V getValue() { return value; } // 设置value。若value是null，则抛出异常。 public V setValue(V value) { if (value == null) throw new NullPointerException(); V oldValue = this.value; this.value = value; return oldValue; } // 覆盖equals()方法，判断两个Entry是否相等。 // 若两个Entry的key和value都相等，则认为它们相等。 public boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; return (key==null ? e.getKey()==null : key.equals(e.getKey())) &amp;&amp; (value==null ? e.getValue()==null : value.equals(e.getValue())); } public int hashCode() { return hash ^ (value==null ? 0 : value.hashCode()); } public String toString() { return key.toString()+\"=\"+value.toString(); } } private static final int KEYS = 0; private static final int VALUES = 1; private static final int ENTRIES = 2; // Enumerator的作用是提供了“通过elements()遍历Hashtable的接口” 和 “通过entrySet()遍历Hashtable的接口”。 private class Enumerator&lt;T&gt; implements Enumeration&lt;T&gt;, Iterator&lt;T&gt; { // 指向Hashtable的table Entry[] table = Hashtable.this.table; // Hashtable的总的大小 int index = table.length; Entry&lt;K,V&gt; entry = null; Entry&lt;K,V&gt; lastReturned = null; int type; // Enumerator是 “迭代器(Iterator)” 还是 “枚举类(Enumeration)”的标志 // iterator为true，表示它是迭代器；否则，是枚举类。 boolean iterator; // 在将Enumerator当作迭代器使用时会用到，用来实现fail-fast机制。 protected int expectedModCount = modCount; Enumerator(int type, boolean iterator) { this.type = type; this.iterator = iterator; } // 从遍历table的数组的末尾向前查找，直到找到不为null的Entry。 public boolean hasMoreElements() { Entry&lt;K,V&gt; e = entry; int i = index; Entry[] t = table; /* Use locals for faster loop iteration */ while (e == null &amp;&amp; i &gt; 0) { e = t[--i]; } entry = e; index = i; return e != null; } // 获取下一个元素 // 注意：从hasMoreElements() 和nextElement() 可以看出“Hashtable的elements()遍历方式” // 首先，从后向前的遍历table数组。table数组的每个节点都是一个单向链表(Entry)。 // 然后，依次向后遍历单向链表Entry。 public T nextElement() { Entry&lt;K,V&gt; et = entry; int i = index; Entry[] t = table; /* Use locals for faster loop iteration */ while (et == null &amp;&amp; i &gt; 0) { et = t[--i]; } entry = et; index = i; if (et != null) { Entry&lt;K,V&gt; e = lastReturned = entry; entry = e.next; return type == KEYS ? (T)e.key : (type == VALUES ? (T)e.value : (T)e); } throw new NoSuchElementException(\"Hashtable Enumerator\"); } // 迭代器Iterator的判断是否存在下一个元素 // 实际上，它是调用的hasMoreElements() public boolean hasNext() { return hasMoreElements(); } // 迭代器获取下一个元素 // 实际上，它是调用的nextElement() public T next() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); return nextElement(); } // 迭代器的remove()接口。 // 首先，它在table数组中找出要删除元素所在的Entry， // 然后，删除单向链表Entry中的元素。 public void remove() { if (!iterator) throw new UnsupportedOperationException(); if (lastReturned == null) throw new IllegalStateException(\"Hashtable Enumerator\"); if (modCount != expectedModCount) throw new ConcurrentModificationException(); synchronized(Hashtable.this) { Entry[] tab = Hashtable.this.table; int index = (lastReturned.hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index], prev = null; e != null; prev = e, e = e.next) { if (e == lastReturned) { modCount++; expectedModCount++; if (prev == null) tab[index] = e.next; else prev.next = e.next; count--; lastReturned = null; return; } } throw new ConcurrentModificationException(); } } } private static Enumeration emptyEnumerator = new EmptyEnumerator(); private static Iterator emptyIterator = new EmptyIterator(); // 空枚举类 // 当Hashtable的实际大小为0；此时，又要通过Enumeration遍历Hashtable时，返回的是“空枚举类”的对象。 private static class EmptyEnumerator implements Enumeration&lt;Object&gt; { EmptyEnumerator() { } // 空枚举类的hasMoreElements() 始终返回false public boolean hasMoreElements() { return false; } // 空枚举类的nextElement() 抛出异常 public Object nextElement() { throw new NoSuchElementException(\"Hashtable Enumerator\"); } } // 空迭代器 // 当Hashtable的实际大小为0；此时，又要通过迭代器遍历Hashtable时，返回的是“空迭代器”的对象。 private static class EmptyIterator implements Iterator&lt;Object&gt; { EmptyIterator() { } public boolean hasNext() { return false; } public Object next() { throw new NoSuchElementException(\"Hashtable Iterator\"); } public void remove() { throw new IllegalStateException(\"Hashtable Iterator\"); } } } 几点总结针对Hashtable，我们同样给出几点比较重要的总结，但要结合与HashMap的比较来总结。1、二者的存储结构和解决冲突的方法都是相同的。 2、HashTable在不指定容量的情况下的默认容量为11，而HashMap为16，Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。 3、Hashtable中key和value都不允许为null，而HashMap中key和value都允许为null（key只能有一个为null，而value则可以有多个为null）。但是如果在Hashtable中有类似put(null,null)的操作，编译同样可以通过，因为key和value都是Object类型，但运行时会抛出NullPointerException异常，这是JDK的规范规定的。我们来看下ContainsKey方法和ContainsValue的源码： 12345678910111213141516171819202122232425262728293031323334353637383940// 判断Hashtable是否包含“值(value)” public synchronized boolean contains(Object value) { //注意，Hashtable中的value不能是null， // 若是null的话，抛出异常! if (value == null) { throw new NullPointerException(); } // 从后向前遍历table数组中的元素(Entry) // 对于每个Entry(单向链表)，逐个遍历，判断节点的值是否等于value Entry tab[] = table; for (int i = tab.length ; i-- &gt; 0 ;) { for (Entry&lt;K,V&gt; e = tab[i] ; e != null ; e = e.next) { if (e.value.equals(value)) { return true; } } } return false; } public boolean containsValue(Object value) { return contains(value); } // 判断Hashtable是否包含key public synchronized boolean containsKey(Object key) { Entry tab[] = table; /计算hash值，直接用key的hashCode代替 int hash = key.hashCode(); // 计算在数组中的索引值 int index = (hash &amp; 0x7FFFFFFF) % tab.length; // 找到“key对应的Entry(链表)”，然后在链表中找出“哈希值”和“键值”与key都相等的元素 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { return true; } } return false; } 很明显，如果value为null，会直接抛出NullPointerException异常，但源码中并没有对key是否为null判断，有点小不解！不过NullPointerException属于RuntimeException异常，是可以由JVM自动抛出的，也许对key的值在JVM中有所限制吧。 4、Hashtable扩容时，将容量变为原来的2倍加1，而HashMap扩容时，将容量变为原来的2倍。 5、Hashtable计算hash值，直接用key的hashCode()，而HashMap重新计算了key的hash值，Hashtable在求hash值对应的位置索引时，用取模运算，而HashMap在求位置索引时，则用与运算，且这里一般先用hash&amp;0x7FFFFFFF后，再对length取模，&amp;0x7FFFFFFF的目的是为了将负的hash值转化为正值，因为hash值有可能为负数，而&amp;0x7FFFFFFF后，只有符号外改变，而后面的位都不变。","link":"/2018/11/23/javase/Hashtable源码剖析/"},{"title":"JavaSE源码分析-ConcurrentHashMap源码剖析","text":"ConcurrentHashMap简介因为多线程环境下，使用Hashmap进行put操作会引起死循环，导致CPU利用率接近100%，所以在并发情况下不能使用HashMap。 HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。 ConcurrentHashMap本质上是一个Segment数组，而一个Segment实例又包含若干个桶，每个桶中都包含一条由若干个 HashEntry 对象链接起来的链表。总的来说，ConcurrentHashMap的高效并发机制是通过以下三方面来保证的(具体细节见后文阐述)： 通过锁分段技术保证并发环境下的写操作； 通过 HashEntry的不变性、Volatile变量的内存可见性和加锁重读机制保证高效、安全的读操作； 通过不加锁和加锁两种方案控制跨段操作的的安全性。 ConcurrentHashMap类中包含两个静态内部类 HashEntry 和 Segment，其中 HashEntry 用来封装具体的K/V对，是个典型的四元组；Segment 用来充当锁的角色，每个 Segment 对象守护整个ConcurrentHashMap的若干个桶 (可以把Segment看作是一个小型的哈希表)，其中每个桶是由若干个 HashEntry 对象链接起来的链表。总的来说，一个ConcurrentHashMap实例中包含由若干个Segment实例组成的数组，而一个Segment实例又包含由若干个桶，每个桶中都包含一条由若干个 HashEntry 对象链接起来的链表。特别地，ConcurrentHashMap 在默认并发级别下会创建16个Segment对象的数组，如果键能均匀散列，每个 Segment 大约守护整个散列表中桶总数的 1/16。 类定义ConcurrentHashMap 继承了AbstractMap并实现了ConcurrentMap接口，12345public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable { ...} 成员变量定义与HashMap相比，ConcurrentHashMap 增加了两个属性用于定位段，分别是 segmentMask 和 segmentShift。此外，不同于HashMap的是，ConcurrentHashMap底层结构是一个Segment数组，而不是Object数组，具体源码如下：123456789101112131415/** * Mask value for indexing into segments. The upper bits of a * key's hash code are used to choose the segment. */ final int segmentMask; // 用于定位段，大小等于segments数组的大小减 1，是不可变的 /** * Shift value for indexing within segments. */ final int segmentShift; // 用于定位段，大小等于32(hash值的位数)减去对segments的大小取以2为底的对数值，是不可变的 /** * The segments, each of which is a specialized hash table */ final Segment&lt;K,V&gt;[] segments; // ConcurrentHashMap的底层结构是一个Segment数组 ConcurrentHashMap 的数据结构本质上，ConcurrentHashMap就是一个Segment数组，而一个Segment实例则是一个小的哈希表。由于Segment类继承于ReentrantLock类，从而使得Segment对象能充当锁的角色，这样，每个 Segment对象就可以守护整个ConcurrentHashMap的若干个桶，其中每个桶是由若干个HashEntry 对象链接起来的链表。通过使用段(Segment)将ConcurrentHashMap划分为不同的部分，ConcurrentHashMap就可以使用不同的锁来控制对哈希表的不同部分的修改，从而允许多个修改操作并发进行, 这正是ConcurrentHashMap锁分段技术的核心内涵。进一步地，如果把整个ConcurrentHashMap看作是一个父哈希表的话，那么每个Segment就可以看作是一个子哈希表，如下图所示：注意，假设ConcurrentHashMap一共分为2^n个段，每个段中有2^m个桶，那么段的定位方式是将key的hash值的高n位与(2^n-1)相与。在定位到某个段后，再将key的hash值的低m位与(2^m-1)相与，定位到具体的桶位。 段的定义：SegmentSegment 类继承于 ReentrantLock 类，从而使得 Segment 对象能充当锁的角色。每个 Segment 对象用来守护它的成员对象 table 中包含的若干个桶。table 是一个由 HashEntry 对象组成的链表数组，table 数组的每一个数组成员就是一个桶。在Segment类中，count 变量是一个计数器，它表示每个 Segment 对象管理的 table 数组包含的 HashEntry 对象的个数，也就是 Segment 中包含的 HashEntry 对象的总数。特别需要注意的是，之所以在每个 Segment 对象中包含一个计数器，而不是在 ConcurrentHashMap 中使用全局的计数器，是对 ConcurrentHashMap 并发性的考虑：因为这样当需要更新计数器时，不用锁定整个ConcurrentHashMap。事实上，每次对段进行结构上的改变，如在段中进行增加/删除节点(修改节点的值不算结构上的改变)，都要更新count的值，此外，在JDK的实现中每次读取操作开始都要先读取count的值。特别需要注意的是，count是volatile的，这使得对count的任何更新对其它线程都是立即可见的。modCount用于统计段结构改变的次数，主要是为了检测对多个段进行遍历过程中某个段是否发生改变，这一点具体在谈到跨段操作时会详述。threashold用来表示段需要进行重哈希的阈值。loadFactor表示段的负载因子，其值等同于ConcurrentHashMap的负载因子的值。table是一个典型的链表数组，而且也是volatile的，这使得对table的任何更新对其它线程也都是立即可见的。段(Segment)的定义如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Segments are specialized versions of hash tables. This * subclasses from ReentrantLock opportunistically, just to * simplify some locking and avoid separate construction. */ static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable { /** * The number of elements in this segment's region. */ transient volatile int count; // Segment中元素的数量，可见的 /** * Number of updates that alter the size of the table. This is * used during bulk-read methods to make sure they see a * consistent snapshot: If modCounts change during a traversal * of segments computing size or checking containsValue, then * we might have an inconsistent view of state so (usually) * must retry. */ transient int modCount; //对count的大小造成影响的操作的次数（比如put或者remove操作） /** * The table is rehashed when its size exceeds this threshold. * (The value of this field is always &lt;tt&gt;(int)(capacity * * loadFactor)&lt;/tt&gt;.) */ transient int threshold; // 阈值，段中元素的数量超过这个值就会对Segment进行扩容 /** * The per-segment table. */ transient volatile HashEntry&lt;K,V&gt;[] table; // 链表数组 /** * The load factor for the hash table. Even though this value * is same for all segments, it is replicated to avoid needing * links to outer object. * @serial */ final float loadFactor; // 段的负载因子，其值等同于ConcurrentHashMap的负载因子 ... }我们知道，ConcurrentHashMap允许多个修改(写)操作并发进行，其关键在于使用了锁分段技术，它使用了不同的锁来控制对哈希表的不同部分进行的修改(写)，而 ConcurrentHashMap 内部使用段(Segment)来表示这些不同的部分。实际上，每个段实质上就是一个小的哈希表，每个段都有自己的锁(Segment 类继承了 ReentrantLock 类)。这样，只要多个修改(写)操作发生在不同的段上，它们就可以并发进行。下图是依次插入 ABC 三个 HashEntry节点后，Segment 的结构示意图：## 基本元素：HashEntryHashEntry用来封装具体的键值对，是个典型的四元组。与HashMap中的Entry类似，HashEntry也包括同样的四个域，分别是key、hash、value和next。不同的是，在HashEntry类中，key，hash和next域都被声明为final的，value域被volatile所修饰，因此HashEntry对象几乎是不可变的，这是ConcurrentHashmap读操作并不需要加锁的一个重要原因。next域被声明为final本身就意味着我们不能从hash链的中间或尾部添加或删除节点，因为这需要修改next引用值，因此所有的节点的修改只能从头部开始。对于put操作，可以一律添加到Hash链的头部。但是对于remove操作，可能需要从中间删除一个节点，这就需要将要删除节点的前面所有节点整个复制(重新new)一遍，最后一个节点指向要删除结点的下一个结点(这在谈到ConcurrentHashMap的删除操作时还会详述)。特别地，由于value域被volatile修饰，所以其可以确保被读线程读到最新的值，这是ConcurrentHashmap读操作并不需要加锁的另一个重要原因。实际上，ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁。HashEntry代表hash链中的一个节点，其结构如下所示：1234567891011121314151617181920212223242526272829303132/** * ConcurrentHashMap 中的 HashEntry 类 * * ConcurrentHashMap list entry. Note that this is never exported * out as a user-visible Map.Entry. * * Because the value field is volatile, not final, it is legal wrt * the Java Memory Model for an unsynchronized reader to see null * instead of initial value when read via a data race. Although a * reordering leading to this is not likely to ever actually * occur, the Segment.readValueUnderLock method is used as a * backup in case a null (pre-initialized) value is ever seen in * an unsynchronized access method. */ static final class HashEntry&lt;K,V&gt; { final K key; // 声明 key 为 final 的 final int hash; // 声明 hash 值为 final 的 volatile V value; // 声明 value 被volatile所修饰 final HashEntry&lt;K,V&gt; next; // 声明 next 为 final 的 HashEntry(K key, int hash, HashEntry&lt;K,V&gt; next, V value) { this.key = key; this.hash = hash; this.next = next; this.value = value; } @SuppressWarnings(\"unchecked\") static final &lt;K,V&gt; HashEntry&lt;K,V&gt;[] newArray(int i) { return new HashEntry[i]; } }与HashMap类似，在ConcurrentHashMap中，如果在散列时发生碰撞，也会将碰撞的 HashEntry 对象链成一个链表。由于HashEntry的next域是final的，所以新节点只能在链表的表头处插入。下图是在一个空桶中依次插入 A，B，C 三个 HashEntry 对象后的结构图(由于只能在表头插入，所以链表中节点的顺序和插入的顺序相反)：# 方法## 构造方法ConcurrentHashMap 一共提供了五个构造函数，其中默认无参的构造函数和参数为Map的构造函数 为 Java Collection Framework 规范的推荐实现，其余三个构造函数则是 ConcurrentHashMap 专门提供的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 构造一个具有默认初始容量(16)、默认负载因子(0.75)和默认并发级别(16)的空ConcurrentHashMappublic ConcurrentHashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);}public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) { this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); putAll(m);}public ConcurrentHashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);}public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, DEFAULT_CONCURRENCY_LEVEL); // 默认并发级别为16}public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; // 大小为 lg(ssize) int ssize = 1; // 段的数目，segments数组的大小(2的幂次方) while (ssize &lt; concurrencyLevel) { ++sshift; ssize &lt;&lt;= 1; } segmentShift = 32 - sshift; // 用于定位段 segmentMask = ssize - 1; // 用于定位段 this.segments = Segment.newArray(ssize); // 创建segments数组 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; // 总的桶数/总的段数 if (c * ssize &lt; initialCapacity) ++c; int cap = 1; // 每个段所拥有的桶的数目(2的幂次方) while (cap &lt; c) cap &lt;&lt;= 1; for (int i = 0; i &lt; this.segments.length; ++i) // 初始化segments数组 this.segments[i] = new Segment&lt;K,V&gt;(cap, loadFactor);}在这里，我们提到了三个非常重要的参数：初始容量、负载因子 和 并发级别，这三个参数是影响ConcurrentHashMap性能的重要参数。从上述源码我们可以看出，ConcurrentHashMap 也正是通过initialCapacity、loadFactor和concurrencyLevel这三个参数进行构造并初始化segments数组、段偏移量segmentShift、段掩码segmentMask和每个segment的。## 并发写操作在ConcurrentHashMap中，线程对映射表做读操作时，一般情况下不需要加锁就可以完成，对容器做结构性修改的操作(比如，put操作、remove操作等)才需要加锁。典型结构性修改操作包括put、remove和clear，下面我们首先以put操作为例说明对ConcurrentHashMap做结构性修改的过程。ConcurrentHashMap的put操作对应的源码如下：123456public V put(K key, V value) { if (value == null) throw new NullPointerException(); int hash = hash(key.hashCode()); return segmentFor(hash).put(key, hash, value, false);}从上面的源码我们看到，ConcurrentHashMap不同于HashMap，它既不允许key值为null，也不允许value值为null。此外，我们还可以看到，实际上我们对ConcurrentHashMap的put操作被ConcurrentHashMap委托给特定的段来实现。也就是说，当我们向ConcurrentHashMap中put一个Key/Value对时，首先会获得Key的哈希值并对其再次哈希，然后根据最终的hash值定位到这条记录所应该插入的段，定位段的segmentFor()方法源码如下：123final Segment&lt;K,V&gt; segmentFor(int hash) { return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask];}segmentFor()方法根据传入的hash值向右无符号右移segmentShift位，然后和segmentMask进行与操作就可以定位到特定的段。在这里，假设Segment的数量(segments数组的长度)是2的n次方(Segment的数量总是2的倍数，具体见构造函数的实现)，那么segmentShift的值就是32-n(hash值的位数是32)，而segmentMask的值就是2^n-1（写成二进制的形式就是n个1）。进一步地，我们就可以得出以下结论：根据key的hash值的高n位就可以确定元素到底在哪一个Segment中。紧接着，调用这个段的put()方法来将目标Key/Value对插到段中，段的put()方法的源码如下所示：12345678910111213141516171819202122232425262728293031V put(K key, int hash, V value, boolean onlyIfAbsent) { lock(); // 上锁 try { int c = count; if (c++ &gt; threshold) // ensure capacity rehash(); HashEntry&lt;K,V&gt;[] tab = table; // table是Volatile的 int index = hash &amp; (tab.length - 1); // 定位到段中特定的桶 HashEntry&lt;K,V&gt; first = tab[index]; // first指向桶中链表的表头 HashEntry&lt;K,V&gt; e = first; // 检查该桶中是否存在相同key的结点 while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue; if (e != null) { // 该桶中存在相同key的结点 oldValue = e.value; if (!onlyIfAbsent) e.value = value; // 更新value值 }else { // 该桶中不存在相同key的结点 oldValue = null; ++modCount; // 结构性修改，modCount加1 tab[index] = new HashEntry&lt;K,V&gt;(key, hash, first, value); // 创建HashEntry并将其链到表头 count = c; //write-volatile，count值的更新一定要放在最后一步(volatile变量) } return oldValue; // 返回旧值(该桶中不存在相同key的结点，则返回null) } finally { unlock(); // 在finally子句中解锁 }}从源码中首先可以知道，ConcurrentHashMap对Segment的put操作是加锁完成的。我们已经知道Segment是ReentrantLock的子类，因此Segment本身就是一种可重入的Lock，所以我们可以直接调用其继承而来的lock()方法和unlock()方法对代码进行上锁/解锁。需要注意的是，这里的加锁操作是针对某个具体的Segment，锁定的也是该Segment而不是整个ConcurrentHashMap。因为插入键/值对操作只是在这个Segment包含的某个桶中完成，不需要锁定整个ConcurrentHashMap。因此，其他写线程对另外15个Segment的加锁并不会因为当前线程对这个Segment的加锁而阻塞。故而 相比较于 HashTable 和由同步包装器包装的HashMap每次只能有一个线程执行读或写操作，ConcurrentHashMap 在并发访问性能上有了质的提高。在理想状态下，ConcurrentHashMap 可以支持 16 个线程执行并发写操作（如果并发级别设置为 16），及任意数量线程的读操作。 在将Key/Value对插入到Segment之前，首先会检查本次插入会不会导致Segment中元素的数量超过阈值threshold，如果会，那么就先对Segment进行扩容和重哈希操作，然后再进行插入。重哈希操作暂且不表，稍后详述。第8和第9行的操作就是定位到段中特定的桶并确定链表头部的位置。第12行的while循环用于检查该桶中是否存在相同key的结点，如果存在，就直接更新value值；如果没有找到，则进入21行生成一个新的HashEntry并且把它链到该桶中链表的表头，然后再更新count的值(由于count是volatile变量，所以count值的更新一定要放在最后一步)。 到此为止，除了重哈希操作，ConcurrentHashMap的put操作已经介绍完了。此外，在ConcurrentHashMap中，修改操作还包括putAll()和replace()。其中，putAll()操作就是多次调用put方法，而replace()操作实现要比put()操作简单得多，此不赘述。 重哈希操作上面叙述到，在ConcurrentHashMap中使用put操作插入Key/Value对之前，首先会检查本次插入会不会导致Segment中节点数量超过阈值threshold，如果会，那么就先对Segment进行扩容和重哈希操作。特别需要注意的是，ConcurrentHashMap的重哈希实际上是对ConcurrentHashMap的某个段的重哈希，因此ConcurrentHashMap的每个段所包含的桶位自然也就不尽相同。针对段进行rehash()操作的源码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364void rehash() { HashEntry&lt;K,V&gt;[] oldTable = table; // 扩容前的table int oldCapacity = oldTable.length; if (oldCapacity &gt;= MAXIMUM_CAPACITY) // 已经扩到最大容量，直接返回 return; /* * Reclassify nodes in each list to new Map. Because we are * using power-of-two expansion, the elements from each bin * must either stay at same index, or move with a power of two * offset. We eliminate unnecessary node creation by catching * cases where old nodes can be reused because their next * fields won't change. Statistically, at the default * threshold, only about one-sixth of them need cloning when * a table doubles. The nodes they replace will be garbage * collectable as soon as they are no longer referenced by any * reader thread that may be in the midst of traversing table * right now. */ // 新创建一个table，其容量是原来的2倍 HashEntry&lt;K,V&gt;[] newTable = HashEntry.newArray(oldCapacity&lt;&lt;1); threshold = (int)(newTable.length * loadFactor); // 新的阈值 int sizeMask = newTable.length - 1; // 用于定位桶 for (int i = 0; i &lt; oldCapacity ; i++) { // We need to guarantee that any existing reads of old Map can // proceed. So we cannot yet null out each bin. HashEntry&lt;K,V&gt; e = oldTable[i]; // 依次指向旧table中的每个桶的链表表头 if (e != null) { // 旧table的该桶中链表不为空 HashEntry&lt;K,V&gt; next = e.next; int idx = e.hash &amp; sizeMask; // 重哈希已定位到新桶 if (next == null) // 旧table的该桶中只有一个节点 newTable[idx] = e; else { // Reuse trailing consecutive sequence at same slot HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; // 寻找k值相同的子链，该子链尾节点与父链的尾节点必须是同一个 if (k != lastIdx) { lastIdx = k; lastRun = last; } } // JDK直接将子链lastRun放到newTable[lastIdx]桶中 newTable[lastIdx] = lastRun; // 对该子链之前的结点，JDK会挨个遍历并把它们复制到新桶中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) { int k = p.hash &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(p.key, p.hash, n, p.value); } } } } table = newTable; // 扩容完成} 其实JDK官方的注释已经解释的很清楚了。由于扩容是按照2的幂次方进行的，所以扩展前在同一个桶中的元素，现在要么还是在原来的序号的桶里，或者就是原来的序号再加上一个2的幂次方，就这两种选择。根据本文前面对HashEntry的介绍，我们知道链接指针next是final的，因此看起来我们好像只能把该桶的HashEntry链中的每个节点复制到新的桶中(这意味着我们要重新创建每个节点)，但事实上JDK对其做了一定的优化。因为在理论上原桶里的HashEntry链可能存在一条子链，这条子链上的节点都会被重哈希到同一个新的桶中，这样我们只要拿到该子链的头结点就可以直接把该子链放到新的桶中，从而避免了一些节点不必要的创建，提升了一定的效率。因此，JDK为了提高效率，它会首先去查找这样的一个子链，而且这个子链的尾节点必须与原hash链的尾节点是同一个，那么就只需要把这个子链的头结点放到新的桶中，其后面跟的一串子节点自然也就连接上了。对于这个子链头结点之前的结点，JDK会挨个遍历并把它们复制到新桶的链头(只能在表头插入元素)中。特别地，我们注意这段代码：12345678910for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; }}newTable[lastIdx] = lastRun; 在该代码段中，JDK直接将子链lastRun放到newTable[lastIdx]桶中，难道这个操作不会覆盖掉newTable[lastIdx]桶中原有的元素么？事实上，这种情形时不可能出现的，因为桶newTable[lastIdx]在子链添加进去之前压根就不会有节点存在，这还是因为table的大小是按照2的幂次方的方式去扩展的。假设原来table的大小是2^k大小，那么现在新table的大小是2^(k+1)大小，而定位桶的方式是:12// sizeMask = newTable.length - 1，即 sizeMask = 11...1，共k+1个1。int idx = e.hash &amp; sizeMask; 因此这样得到的idx实际上就是key的hash值的低k+1位的值，而原table的sizeMask也全是1的二进制，不过总共是k位，那么原table的idx就是key的hash值的低k位的值。所以，如果元素的hashcode的第k+1位是0，那么元素在新桶的序号就是和原桶的序号是相等的；如果第k+1位的值是1，那么元素在新桶的序号就是原桶的序号加上2^k。因此，JDK直接将子链lastRun放到newTable[lastIdx]桶中就没问题了，因为newTable中新序号处此时肯定是空的。 读取操作与put操作类似，当我们从ConcurrentHashMap中查询一个指定Key的键值对时，首先会定位其应该存在的段，然后查询请求委托给这个段进行处理，源码如下：1234public V get(Object key) { int hash = hash(key.hashCode()); return segmentFor(hash).get(key, hash);} 我们紧接着研读Segment中get操作的源码：12345678910111213141516V get(Object key, int hash) { if (count != 0) { // read-volatile，首先读 count 变量 HashEntry&lt;K,V&gt; e = getFirst(hash); // 获取桶中链表头结点 while (e != null) { if (e.hash == hash &amp;&amp; key.equals(e.key)) { // 查找链中是否存在指定Key的键值对 V v = e.value; if (v != null) // 如果读到value域不为 null，直接返回 return v; // 如果读到value域为null，说明发生了重排序，加锁后重新读取 return readValueUnderLock(e); // recheck } e = e.next; } } return null; // 如果不存在，直接返回null} 了解了ConcurrentHashMap的put操作后，上述源码就很好理解了。但是有一个情况需要特别注意，就是链中存在指定Key的键值对并且其对应的Value值为null的情况。在剖析ConcurrentHashMap的put操作时，我们就知道ConcurrentHashMap不同于HashMap，它既不允许key值为null，也不允许value值为null。但是，此处怎么会存在键值对存在且的Value值为null的情形呢？JDK官方给出的解释是，这种情形发生的场景是：初始化HashEntry时发生的指令重排序导致的，也就是在HashEntry初始化完成之前便返回了它的引用。这时，JDK给出的解决之道就是加锁重读，源码如下：12345678V readValueUnderLock(HashEntry&lt;K,V&gt; e) { lock(); try { return e.value; } finally { unlock(); } } 在ConcurrentHashMap进行存取时，首先会定位到具体的段，然后通过对具体段的存取来完成对整个ConcurrentHashMap的存取。特别地，无论是ConcurrentHashMap的读操作还是写操作都具有很高的性能：在进行读操作时不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。","link":"/2018/11/23/javase/ConcurrentHashMap源码剖析/"}],"tags":[{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"Oracle","slug":"Oracle","link":"/tags/Oracle/"},{"name":"memcached","slug":"memcached","link":"/tags/memcached/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"计划任务","slug":"计划任务","link":"/tags/计划任务/"},{"name":"存储过程","slug":"存储过程","link":"/tags/存储过程/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"Server","slug":"Server","link":"/tags/Server/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"防火墙","slug":"防火墙","link":"/tags/防火墙/"},{"name":"源码分析","slug":"源码分析","link":"/tags/源码分析/"},{"name":"JavaSE","slug":"JavaSE","link":"/tags/JavaSE/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","link":"/tags/ConcurrentHashMap/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Integer","slug":"Integer","link":"/tags/Integer/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"tomcat","slug":"tomcat","link":"/tags/tomcat/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"md","slug":"md","link":"/tags/md/"},{"name":"ssl","slug":"ssl","link":"/tags/ssl/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"HashMap","slug":"HashMap","link":"/tags/HashMap/"},{"name":"Hashtable","slug":"Hashtable","link":"/tags/Hashtable/"}],"categories":[{"name":"programming","slug":"programming","link":"/categories/programming/"},{"name":"JavaSE源码分析","slug":"JavaSE源码分析","link":"/categories/JavaSE源码分析/"},{"name":"Spring基础","slug":"Spring基础","link":"/categories/Spring基础/"},{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"}]}