{"pages":[{"title":"黄敏","text":"生于89年，男，湖南人，毕业于吉首大学，本科学历。现就职于芒果TV，主要负责芒果多元化的开发管理工作。 主要从事Java开发，了解Python、PHP、NodeJS，写过shell和python脚本。独立开发过微信、安卓应用，擅长后端开发，小至SSH，大到分布式，技术涉略比较杂，也算一个小小全栈吧！在管理方面只能说是马马虎虎吧！负责开发过几个项目，成功上线6、7个地方。 技术前端：HTML、CSS、JavaScript、Ajax、JQuery、EasyUI、bootstrap后端：Java、Struts2、Spring MVC、Spring、Mybatis、FreeMarker、Quartz、Dubbo、HSF(DEAS)、ActiveMQ、RabbitMQ、Netty、WebService、WebSocket、Spring boot、Spring Cloud数据库：Oracle、MySQL和Mongodb缓存：Ehcache、Memcached、Redis应用服务器：Jetty、Tomcat、JBoss、Weblogic、Apache、Ngnix等。构建：ANT、Maven、Gradle、SVN、Git运维：Linux、shell，VMware开发工具：Eclipse、IDEA、PLSQL Developer、PowerDesigner、Rose其他：Android、Python、PHP、公众号、小程序、第三方支付 经历2012.07 – 2013.11 科盟软件有限公司2013.11 – 2017.05 北京智慧眼科技股份有限公司2017.05 – 2017.11 长沙东东科技有限公司2017.11 – 至今 湖南快乐阳光互动娱乐传媒有限公司(芒果TV) 箴言业精于勤荒于嬉","link":"/about/index.html"}],"posts":[{"title":"Oracle在Linux下的安装","text":"安装环境：系统是 centos6.5","link":"/2017/03/18/Database/Oracle在Linux下的安装/"},{"title":"memcached在Linux下的安装","text":"安装环境：系统是 centos6.5 libevent安装打开memcached官网的下载界面http://memcached.org/downloads，看到有如下提示： Debian/Ubuntu: apt-get install libevent-dev Redhat/Centos: yum install libevent-devel 说明memcached依赖于libevent 。 yum安装1yum install libevent-devel 源码安装下载网址：http://libevent.org/123456#解压 tar -zxvf libevent-2.0.22-stable.tar.gz #配置 ./configure –prefix=/usr/local/libevent #安装 make &amp;&amp; make install 下载memcachedmemcached下载网址： http://memcached.org/downloads 或者 wget http://memcached.org/latest 解压该版本的memcached没有使用gzip压缩，所以不能加-g1tar -xvf memcached-1.4.25.tar.gz 编译通过 ./configure –help可以参考编译参数，可以看出，我们可以通过 –with-libevent来指定libevent安装目录。 编译命令如下：1./configure --prefix=/usr/local/memcached --with-libevent=/usr/local/libevent/ 安装执行如下命令1make &amp;&amp; make install 安装完成后，可以看到/usr/local目录下多了一个memcached目录。 启动1./memcached -vv -u nobody","link":"/2017/03/18/Database/memcached在Linux下的安装/"},{"title":"Oracle查看表空间常用语句","text":"查看表空间使用率12345678910111213select a.tablespace_name, a.bytes / 1024 / 1024 \"Sum MB\", (a.bytes - b.bytes) / 1024 / 1024 \"used MB\", b.bytes / 1024 / 1024 \"free MB\", round(((a.bytes - b.bytes) / a.bytes) * 100, 2) \"percent_used\" from (select tablespace_name, sum(bytes) bytes from dba_data_files group by tablespace_name) a, (select tablespace_name, sum(bytes) bytes, max(bytes) largest from dba_free_space group by tablespace_name) b where a.tablespace_name = b.tablespace_name order by ((a.bytes - b.bytes) / a.bytes) desc 查看用户、表空间使用率123456789101112select a.username,b.* from dba_users a left join (select a.tablespace_name, a.bytes / 1024 / 1024 \"Sum MB\", (a.bytes - b.bytes) / 1024 / 1024 \"used MB\", b.bytes / 1024 / 1024 \"free MB\", round(((a.bytes - b.bytes) / a.bytes) * 100, 2) \"percent_used\" from (select tablespace_name, sum(bytes) bytes from dba_data_files group by tablespace_name) a, (select tablespace_name, sum(bytes) bytes, max(bytes) largest from dba_free_space group by tablespace_name) b where a.tablespace_name = b.tablespace_name) b on a.default_tablespace = b.tablespace_name; 删除用户和数据文件12345--步骤一： 删除userdrop user ×× cascade--说明： 删除了user，只是删除了该user下的schema objects，是不会删除相应的tablespace的。--步骤二： 删除tablespaceDROP TABLESPACE tablespace_name INCLUDING CONTENTS AND DATAFILES; 改变数据文件大小1alter database datafile '/u01/oracle/oradata/aeye/sysaux01.dbf' resize 2G;","link":"/2017/05/30/Database/Oracle查看表空间常用语句/"},{"title":"mysql事件（计划任务）","text":"例子：123456789101112131415161718192021222324252627282930313233343536373839DELIMITER $$-- 1、设置 全局变量event_scheduler开启，使用event必须开启-- SET GLOBAL event_scheduler = ON$$ -- required for event to execute but not create CREATE /*[DEFINER = { user | CURRENT_USER }]*/ EVENT `start_distribution`.`event_account_checking` ON SCHEDULE EVERY 1 DAY STARTS '2017-09-04 06:00:00' ON COMPLETION NOT PRESERVE ENABLE COMMENT '每天自动统计对账数据' /* uncomment the example below you want to use */ -- scheduleexample 1: run once 执行一次 -- AT 'YYYY-MM-DD HH:MM.SS'/CURRENT_TIMESTAMP { + INTERVAL 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] } -- scheduleexample 2: run at intervals forever after creation 创建后每多久执行一次 -- EVERY 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] -- scheduleexample 3: specified start time, end time and interval for execution 指定开始开始、结束时间和间隔时间执行任务。 /*EVERY 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] STARTS CURRENT_TIMESTAMP/'YYYY-MM-DD HH:MM.SS' { + INTERVAL 1[HOUR|MONTH|WEEK|DAY|MINUTE|...] } ENDS CURRENT_TIMESTAMP/'YYYY-MM-DD HH:MM.SS' { + INTERVAL 1 [HOUR|MONTH|WEEK|DAY|MINUTE|...] } */ /*[ON COMPLETION [NOT] PRESERVE] -- 执行后删除(NOT)还是保留 [ENABLE | DISABLE] -- 创建时，event的章台 [COMMENT 'comment']*/ -- 注释DO BEGIN call pro_account_checking(); END$$DELIMITER ;","link":"/2017/09/04/Database/mysql事件（计划任务）/"},{"title":"Redis在Linux下的安装","text":"基本知识1、 Redis的数据类型：字符串、列表（lists）、集合（sets）、有序集合（sorts sets）、哈希表（hashs） 2、 Redis和memcache相比的独特之处：（1）redis可以用来做存储（storge）、而memcache是来做缓存（cache）。这个特点主要是因为其有“持久化”功能（2）存储的数据有“结构”，对于memcache来说，存储的数据，只有一种类型——“字符串”，而redis则可以存储字符串、链表、集合、有序集合、哈序结构 3、 持久化的两种方式：Redis将数据存储于内存中，或被配置为使用虚拟内存。实现数据持久化的两种方式：（1）使用截图的方式，将内存中的数据不断写入磁盘（性能高，但可能会引起一定程度的数据丢失）（2）使用类似mysql的方式，记录每次更新的日志 4、 Redis的主从同步：对提高读取性能非常有益 5、Redis服务端的默认端口是6379 下载安装环境：系统是 centos6.5 下载地址：http://download.redis.io/releases/redis-3.2.1.tar.gz上传到Linux。 解压1tar -zxvf redis-3.2.1.tar.gz 编译源程序123456cd redis-3.2.1make#编译完成之后cd srcmake install PREFIX=/usr/local/redis 配置文件1cp redis.conf /usr/local/redis/bin/ 服务启动和停止12cd /usr/local/redis/bin/./redis-server redis.conf 默认情况，Redis不是在后台运行，我们需要把redis放在后台运行123vim redis.conf#将daemonize的值改为yesdaemonize yes 客户端连接1./redis-cli 停止服务1./redis-cli shutdown redis开机自启123vim /etc/rc.local#加入redis启动脚本/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis.conf 其他redis-benchmark：redis性能测试工具 redis-check-aof：检查aof日志的工具 redis-check-dump：检查rdb日志的工具 redis-cli：连接用的客户端 redis-server：redis服务进程 Redis的配置daemonize：如需要在后台运行，把该项的值改为yes pdifile：把pid文件放在/var/run/redis.pid，可以配置到其他地址 bind：指定redis只接收来自该IP的请求，如果不设置，那么将处理所有请求，在生产环节中最好设置该项 port：监听端口，默认为6379 timeout：设置客户端连接时的超时时间，单位为秒 loglevel：等级分为4级，debug，revbose，notice和warning。生产环境下一般开启notice logfile：配置log文件地址，默认使用标准输出，即打印在命令行终端的端口上 database：设置数据库的个数，默认使用的数据库是0 save：设置redis进行数据库镜像的频率 rdbcompression：在进行镜像备份时，是否进行压缩 dbfilename：镜像备份文件的文件名 dir：数据库镜像备份的文件放置的路径 slaveof：设置该数据库为其他数据库的从数据库 masterauth：当主数据库连接需要密码验证时，在这里设定 requirepass：设置客户端连接后进行任何其他指定前需要使用的密码 maxclients：限制同时连接的客户端数量 maxmemory：设置redis能够使用的最大内存 appendonly：开启appendonly模式后，redis会把每一次所接收到的写操作都追加到appendonly.aof文件中，当redis重新启动时，会从该文件恢复出之前的状态 appendfsync：设置appendonly.aof文件进行同步的频率 vm_enabled：是否开启虚拟内存支持 vm_swap_file：设置虚拟内存的交换文件的路径 vm_max_momery：设置开启虚拟内存后，redis将使用的最大物理内存的大小，默认为0 vm_page_size：设置虚拟内存页的大小 vm_pages：设置交换文件的总的page数量 vm_max_thrrads：设置vm IO同时使用的线程数量","link":"/2017/03/18/Database/Redis在Linux下的安装/"},{"title":"mysql在Linux下的安装","text":"安装环境：系统是 centos6.5 下载下载地址：http://dev.mysql.com/downloads/mysql/5.6.html#downloads下载版本：我这里选择的5.6.33，通用版，linux下64位 也可以直接复制64位的下载地址，通过命令下载：wget http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz 解压1234#解压tar -zxvf mysql-5.6.33-linux-glibc2.5-x86_64.tar.gz#复制解压后的mysql目录cp -r mysql-5.6.33-linux-glibc2.5-x86_64 /usr/local/mysql 添加用户组和用户1234#添加用户组groupadd mysql#添加用户mysql 到用户组mysqluseradd -g mysql mysql 安装1234567891011121314151617181920212223242526272829303132cd /usr/local/mysql/mkdir ./data/mysqlchown -R mysql:mysql ././scripts/mysql_install_db --user=mysql --datadir=/usr/local/mysql/data/mysqlcp support-files/mysql.server /etc/init.d/mysqldchmod 755 /etc/init.d/mysqldcp support-files/my-default.cnf /etc/my.cnf#修改启动脚本vi /etc/init.d/mysqld #修改项：basedir=/usr/local/mysql/datadir=/usr/local/mysql/data/mysql #启动服务service mysqld start #测试连接./mysql/bin/mysql -uroot #加入环境变量，编辑 /etc/profile，这样可以在任何地方用mysql命令了export PATH=$PATH:/usr/local/mysql//binsource /etc/profile #启动mysqlservice mysqld start#关闭mysqlservice mysqld stop#查看运行状态service mysqld status 错误sqlyog连接时，报1130错误.是由于没有给远程连接的用户权限问题解决1:更改 ‘mysql’数据库‘user’表‘host’项，从‘localhost’改成‘%’。1234use mysql;select 'host' from user where user='root'; update user set host = '%' where user ='root';flush privileges; 解决2：直接授权1GRANT ALL PRIVILEGES ON *.* TO ‘root’@'%’ IDENTIFIED BY ‘youpassword’ WITH GRANT OPTION; 安装时的一些错误-bash: ./scripts/mysql_install_db: /usr/bin/perl: bad interpreter: 没有那个文件或目录1解决： yum -y install perl perl-devel Installing MySQL system tables…./bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directory1解决：yum -y install libaio-devel #其他 配置环境变量12vi + /etc/profileexport PATH=....:/usr/local/mysql/bin Linux表名大小写问题编辑/etc/my.cnf,在[mysqld]下面添加如下：12[mysqld]lower_case_table_names=1 乱码问题编辑/etc/my.cnf,在[mysqld]下面添加如下：123[mysqld]...character-set-server=utf8","link":"/2017/03/18/Database/mysql在Linux下的安装/"},{"title":"FreeMarker简单介绍","text":"","link":"/2017/05/30/Java/FreeMarker简单介绍/"},{"title":"mysql存储过程","text":"简介存储过程是可编程的函数，在数据库中创建并保存，可以由SQL语句和控制结构组成。当想要在不同的应用程序或平台上执行相同的函数，或者封装特定功能时，存储过程是非常有用的。数据库中的存储过程可以看做是对编程中面向对象方法的模拟，它允许控制数据的访问方式。 存储过程是数据库的一个重要的功能，MySQL 5.0以前并不支持存储过程，这使得MySQL在应用上大打折扣。好在MySQL 5.0开始支持存储过程，这样即可以大大提高数据库的处理速度，同时也可以提高数据库编程的灵活性。 存储过程的优点(1).增强SQL语言的功能和灵活性：存储过程可以用控制语句编写，有很强的灵活性，可以完成复杂的判断和较复杂的运算。 (2).标准组件式编程：存储过程被创建后，可以在程序中被多次调用，而不必重新编写该存储过程的SQL语句。而且数据库专业人员可以随时对存储过程进行修改，对应用程序源代码毫无影响。 (3).较快的执行速度：如果某一操作包含大量的Transaction-SQL代码或分别被多次执行，那么存储过程要比批处理的执行速度快很多。因为存储过程是预编译的。在首次运行一个存储过程时查询，优化器对其进行分析优化，并且给出最终被存储在系统表中的执行计划。而批处理的Transaction-SQL语句在每次运行时都要进行编译和优化，速度相对要慢一些。 (4).减少网络流量：针对同一个数据库对象的操作（如查询、修改），如果这一操作所涉及的Transaction-SQL语句被组织进存储过程，那么当在客户计算机上调用该存储过程时，网络中传送的只是该调用语句，从而大大减少网络流量并降低了网络负载。 (5).作为一种安全机制来充分利用：通过对执行某一存储过程的权限进行限制，能够实现对相应的数据的访问权限的限制，避免了非授权用户对数据的访问，保证了数据的安全。 例子1234567891011121314151617181920212223242526272829303132333435363738394041424344DELIMITER $$ USE `start_user`$$ DROP PROCEDURE IF EXISTS `rob_test`$$ CREATE /*[DEFINER = { user | CURRENT_USER }]*/ -- 0、定义存储过程名称和参数、返回值 -- in 为输入参数；out 为返回值，inout 即是参数也是返回值 PROCEDURE `start_user`.`rob_test`( out msg VARCHAR(50) ) /* LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT 'string' */ BEGIN -- 1、声明变量 DECLARE userId int; DECLARE i int; DECLARE phoneId BIGint; -- 2、赋值，也可以用DEFAULT，如：DECLARE i int DEFAULT 0; SET i=0; SET phoneId= 11010000002; -- 3、业务逻辑，此处用循环插入数据 while i&lt;5 do INSERT INTO `start_user`.`user` (`nickName`, `gender`, `mobile`, `signatureText`, `avatarUrl`, `userType`, `isUserGroup`, `status`, `channel`, `inviteGuestPermission`, `reported`, `flag`, `appId`, `loginBundleId`) VALUES(CONCAT('机器人',i),'MALE',CONCAT(phoneId,''),NULL,NULL,'NORMAL',NULL,'1',NULL,NULL,NULL,NULL,NULL,NULL); SELECT LAST_INSERT_ID() INTO userId ; insert into `start_distribution`.`dis_org_startuser` ( `orgId`, `agentId`, `cooperatorId`, `opcenterId`, `startUserId`, `startUserType`, `inviteUserId`) values('999992348','999992347','999992346','999992345',userId,'ANCHOR',NULL); set i=i+1; set phoneId= phoneId+1; end while; -- 4、结束循环，设定返回值 set msg ='执行成功'; END$$DELIMITER ; 语法CREATE PROCEDURE 过程名([[IN|OUT|INOUT] 参数名 数据类型[,[IN|OUT|INOUT] 参数名 数据类型…]]) [特性 …] 过程体 结束符DELIMITER 定义语句结束符。首先将结束符变为“$$”,在完成存储过程之后再将结束符改为默认的“;”。 参数存储过程根据需要可能会有输入、输出、输入输出参数，如果有多个参数用”,”分割开。MySQL存储过程的参数用在存储过程的定义，共有三种参数类型,IN,OUT,INOUT: IN：参数的值必须在调用存储过程时指定，在存储过程中修改该参数的值不能被返回，为默认值OUT:该值可在存储过程内部被改变，并可返回INOUT:调用时指定，并且可被改变和返回 过程体过程体的开始与结束使用BEGIN与END进行标识。 变量赋值语法：SET 变量名 = 变量值 [,变量名= 变量值 …] 用户变量用户变量一般以@开头注意：滥用用户变量会导致程序难以理解及管理 注释MySQL存储过程可使用两种风格的注释：双杠：–，该风格一般用于单行注释C风格： 一般用于多行注释 调用用call和你过程名以及一个括号，括号里面根据需要，加入参数，参数包括输入参数、输出参数、输入输出参数。 控制语句条件语句IF-THEN-ELSE语句1234567891011121314151617DROP PROCEDURE IF EXISTS proc3;DELIMITER //CREATE PROCEDURE proc3(IN parameter int) BEGIN DECLARE var int; SET var=parameter+1; IF var=0 THEN INSERT INTO t VALUES (17); END IF ; IF parameter=0 THEN UPDATE t SET s1=s1+1; ELSE UPDATE t SET s1=s1+2; END IF ; END ; //DELIMITER ; CASE-WHEN-THEN-ELSE语句12345678910111213141516DELIMITER // CREATE PROCEDURE proc4 (IN parameter INT) BEGIN DECLARE var INT; SET var=parameter+1; CASE var WHEN 0 THEN INSERT INTO t VALUES (17); WHEN 1 THEN INSERT INTO t VALUES (18); ELSE INSERT INTO t VALUES (19); END CASE ; END ; //DELIMITER ; 循环语句WHILE-DO…END-WHILE123456789101112DELIMITER // CREATE PROCEDURE proc5() BEGIN DECLARE var INT; SET var=0; WHILE var&lt;6 DO INSERT INTO t VALUES (var); SET var=var+1; END WHILE ; END; //DELIMITER ; REPEAT…END REPEAT此语句的特点是执行操作后检查结果12345678910111213DELIMITER // CREATE PROCEDURE proc6 () BEGIN DECLARE v INT; SET v=0; REPEAT INSERT INTO t VALUES(v); SET v=v+1; UNTIL v&gt;=5 END REPEAT; END; //DELIMITER ; LOOP…END LOOP12345678910111213141516171819202122232425DELIMITER // CREATE PROCEDURE proc7 () BEGIN DECLARE v INT; SET v=0; LOOP_LABLE:LOOP INSERT INTO t VALUES(v); SET v=v+1; IF v &gt;=5 THEN LEAVE LOOP_LABLE; END IF; END LOOP; END; //DELIMITER ;``` #### LABLES标号标号可以用在begin repeat while 或者loop 语句前，语句标号只能在合法的语句前面使用。可以跳出循环，使运行指令达到复合语句的最后一步。#### ITERATE迭代通过引用复合语句的标号,来从新开始复合语句### ITERATE DELIMITER // CREATE PROCEDURE proc8() BEGIN DECLARE v INT; SET v=0; LOOP_LABLE:LOOP IF v=3 THEN SET v=v+1; ITERATE LOOP_LABLE; END IF; INSERT INTO t VALUES(v); SET v=v+1; IF v&gt;=5 THEN LEAVE LOOP_LABLE; END IF; END LOOP; END; //DELIMITER ;```","link":"/2017/08/18/Database/mysql存储过程/"},{"title":"Android常用部署属性","text":"第一类：通用属性1234567891011android:id 设置IDandroid:layout_width 该组件的宽度,wrap_content|fill_parent|match_parentandroid:layout_height 该组件的高度，wrap_content|fill_parent|match_parentandroid:layout_gravity 该组件在父组件的对其方式，bottom|left|top|rightandroid:orientation 布局组件的内部子组件的排列方式，horizontal|verticalandroid:gravity 布局组件的内部子组件的对其方式,bottom|left|top|rightandroid:text 设置显示的文本内容，通过、string.xml文件引用android:textColor 设置字体颜色，通过colors.xml资源来引用android:textStyle 设置字体风格，normal(无效果)|bold(加粗)|italic(斜体)android:textSize 字体大小，单位一般是用sp！android:background 设置主键的背景图片 第二类:属性值为true或false12345678android:layout_centerHrizontal 水平居中android:layout_centerVertical 垂直居中android:layout_centerInparent 相对于父元素完全居中。android:layout_alignParentBottom 贴紧父元素的下边缘android:layout_alignParentLeft 贴紧父元素的左边缘android:layout_alignParentRight 贴紧父元素的右边缘android:layout_alignParentTop 贴紧父元素的上边缘android:layout_alignWithParentIfMissing 如果对应的兄弟元素找不到的话就以父元素做参照物 第三类：属性值必须为id的引用名“@id/id-name”12345678android:layout_below 在某元素的下方android:layout_above 在某元素的的上方android:layout_toLeftOf 在某元素的左边android:layout_toRightOf 在某元素的右边android:layout_alignTop 本元素的上边缘和某元素的的上边缘对齐android:layout_alignLeft 本元素的左边缘和某元素的的左边缘对齐android:layout_alignBottom 本元素的下边缘和某元素的的下边缘对齐android:layout_alignRight 本元素的右边缘和某元素的的右边缘对齐 第四类：属性值为具体的像素值，如30dip，40px12345678910android:layout_margin 离某元素上下左右的的距离android:layout_marginTop 离某元素上边缘的距离android:layout_marginRight 离某元素右边缘的距离android:layout_marginBottom 离某元素底边缘的距离android:layout_marginLeft 离某元素左边缘的距离android:padding 指该控件内部内容距离该控件上下左右边缘的边距android:paddingTop 指该控件内部内容距离该控件上边缘的边距android:paddingRight 指该控件内部内容距离该控件左边缘的边距android:paddingBottom 指该控件内部内容距离该控件下边缘的边距android:paddingLeft 指该控件内部内容距离该控件右边缘的边距","link":"/2017/03/18/Android/Android常用部署属性/"},{"title":"Velocity简单介绍","text":"","link":"/2017/05/30/Java/Velocity简单介绍/"},{"title":"Logger4j详解","text":"一、介绍Log4j是Apache的一个开源项目，通过使用Log4j，我们可以控制日志信息输送的目的地是控制台、文件、GUI组件、甚至是套接口服务 器、NT的事件记录器、UNIX Syslog守护进程等；我们也可以控制每一条日志的输出格式；通过定义每一条日志信息的级别，我们能够更加细致地控制日志的生成过程。 Log4j由三个重要的组件构成：日志信息的优先级（Loggers），日志信息的输出目的地（Appenders），日志信息的输出格式（Layouts）。日志信息的优先级从高到低有ERROR、WARN、 INFO、DEBUG，分别用来指定这条日志信息的重要程度；日志信息的输出目的地指定了日志将打印到控制台还是文件中；而输出格式则控制了日志信息的显示内容。 二、配置文件其实您也可以完全不使用配置文件，而是在代码中配置Log4j环境。但是，使用配置文件将使您的应用程序更加灵活。Log4j支持两种配置文件格式，一种是XML格式的文件，一种是properties格式的文件。下面我们介绍使用properties格式做为配置文件的方法：示例：log4j.rootLogger=INFO, A1log4j.appender.A1=org.apache.log4j.ConsoleAppenderlog4j.appender.A1.layout=org.apache.log4j.PatternLayoutlog4j.appender.A1.layout.ConversionPattern=%-4r %-5p [%t] %37c %3x - %m%n 1. 配置根Logger，其语法为：log4j.rootLogger = [ level ] , appenderName, appenderName, …其中，level 是日志记录的优先级，分为OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL或者您定义的级别。Log4j建议只使用四个级别，优先级从高到低分别是ERROR、WARN、INFO、DEBUG。通过在这里定义的级别，您可以控制到应用程序中相应级别的日志信息的开关。比如在这里定义了INFO级别，则应用程序中所有DEBUG级别的日志信息将不被打印出来。 appenderName就是指定日志信息输出到哪个地方。您可以同时指定多个输出目的地。 2. 配置日志信息输出目的地Appender，其语法为：log4j.appender.appenderName = package+appender_class_namelog4j.appender.appenderName.option1 = value1…log4j.appender.appenderName.option = valueN其中，Log4j提供的appender有以下几种：org.apache.log4j.ConsoleAppender（控制台），org.apache.log4j.FileAppender（文件），org.apache.log4j.DailyRollingFileAppender（每天产生一个日志文件），org.apache.log4j.RollingFileAppender（文件大小到达指定尺寸的时候产生一个新的文件），org.apache.log4j.WriterAppender（将日志信息以流格式发送到任意指定的地方) org.apache.log4j.jdbc.JDBCAppender(将日志写入数据库) (1).ConsoleAppender选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。Target=System.err：默认情况下是：System.out,指定输出控制台(2).FileAppender 选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。(3).DailyRollingFileAppender 选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。DatePattern=’.’yyyy-ww:每周滚动一次文件，即每周产生一个新的文件。当然也可以指定按月、周、天、时和分。即对应的格式如下：1)’.’yyyy-MM: 每月2)’.’yyyy-ww: 每周3)’.’yyyy-MM-dd: 每天4)’.’yyyy-MM-dd-a: 每天两次5)’.’yyyy-MM-dd-HH: 每小时6)’.’yyyy-MM-dd-HH-mm: 每分钟(4).RollingFileAppender 选项Threshold=WARN:指定日志消息的输出最低层次。ImmediateFlush=true:默认值是true,意谓着所有的消息都会被立即输出。File=mylog.txt:指定消息输出到mylog.txt文件。Append=false:默认值是true,即将消息增加到指定文件中，false指将消息覆盖指定的文件内容。MaxFileSize=100KB: 后缀可以是KB, MB 或者是 GB. 在日志文件到达该大小时，将会自动滚动，即将原来的内容移到mylog.log.1文件。MaxBackupIndex=2:指定可以产生的滚动文件的最大数。 (5). JDBCApperder选项 URL=jdbc:mysql://localhost:3306/test：指定日志写入的数据库链接driver=com.mysql.jdbc.Driver：指定数据库驱动user=root：指定数据库的用户名password=123：指定数据库的登录密码sql=insert into tb_log (message) values(‘=[%-5p] %d(%r) –&gt; [%t] %l: %m %x %n’)：指定写入数据库的执行语句 3. 配置日志信息的布局，其语法为：log4j.appender.appenderName.layout = package+layout_class_namelog4j.appender.appenderName.layout.option1 = value1…log4j.appender.appenderName.layout.option = valueN其中，Log4j提供的layout有以下几种：org.apache.log4j.HTMLLayout（以HTML表格形式布局），org.apache.log4j.PatternLayout（可以灵活地指定布局模式），org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串），org.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等等信息） 4、输出格式设置在配置文件中可以通过log4j.appender.A1.layout.ConversionPattern设置日志输出格式。参数：%p: 输出日志信息优先级，即DEBUG，INFO，WARN，ERROR，FATAL,%d: 输出日志时间点的日期或时间，默认格式为ISO8601，也可以在其后指定格式，比如：%d{yyy MMM dd HH:mm:ss,SSS}，输出类似：2002年10月18日 22：10：28，921 %r: 输出自应用启动到输出该log信息耗费的毫秒数%c: 输出日志信息所属的类目，通常就是所在类的全名%t: 输出产生该日志事件的线程名%l: 输出日志事件的发生位置，相当于%C.%M(%F:%L)的组合,包括类目名、发生的线程，以及在代码中的行数。举例：Testlog4.main(TestLog4.java:10) %x: 输出和当前线程相关联的NDC(嵌套诊断环境),尤其用到像java servlets这样的多客户多线程的应用中。%%: 输出一个”%”字符%F: 输出日志消息产生时所在的文件名称%L: 输出代码中的行号%m: 输出代码中指定的消息,产生的日志具体信息%n: 输出一个回车换行符，Windows平台为”\\r\\n”，Unix平台为”\\n”输出日志信息换行可以在%与模式字符之间加上修饰符来控制其最小宽度、最大宽度、和文本的对齐方式。如：1)%20c：指定输出category的名称，最小的宽度是20，如果category的名称小于20的话，默认的情况下右对齐。2)%-20c:指定输出category的名称，最小的宽度是20，如果category的名称小于20的话，”-”号指定左对齐。3)%.30c:指定输出category的名称，最大的宽度是30，如果category的名称大于30的话，就会将左边多出的字符截掉，但小于30的话也不会有空格。4)%20.30c:如果category的名称小于20就补空格，并且右对齐，如果其名称长于30字符，就从左边交远销出的字符截掉。 三、如何在不同的模块中输出不同的日志用户基础信息模块路径为：com.test.user它下面有个类：com.test.user.service.impl.UserInfoprivate Log log = LogFactory.getLog(UserInfo.class); 方法1：在log4j.properties中加入:log4j.logger.com.test.user=info,userLog,stdoutlog4j.appender.userLog=org.apache.log4j.FileAppenderlog4j.appender.userLog.File=../logs/userinfo.loglog4j.appender.userLog.Append=truelog4j.appender.userLog.Threshold=infolog4j.appender.userLog.layout=org.apache.log4j.PatternLayoutlog4j.appender.userLog.layout.ConversionPattern==%d %p [%c] - %m%n 注：也就是让com.test.user模块下所有的logger使用log4j.appender.userLog所做的配置。 方法2：自定义“别名”private Log log = LogFactory.getLog(“userInfoLog”);然后在log4j.properties中加入:log4j.logger.userInfoLog=info,userLog,stdoutlog4j.appender.userLog=org.apache.log4j.FileAppenderlog4j.appender.userLog.File=../logs/userinfo.loglog4j.appender.userLog.Append=truelog4j.appender.userLog.Threshold=infolog4j.appender.userLog.layout=org.apache.log4j.PatternLayoutlog4j.appender.userLog.layout.ConversionPattern==%d %p [%c] - %m%n 注：也就是在用logger时给它一个自定义的名字(如这里的”userInfoLog”)，然后在log4j.properties中做出相应配置即可。，在这种模式下，即使在同一个类中也能定义多个不同输出的log. 在类中调用代码如下：private Log loggerError = LogFactory.getLog(“userErrorLog”);private Log loggerInfo = LogFactory.getLog(“userInfoLog”); 自定义的日志默认是同时输出到log4j.rootLogger所配置的日志中的，如何能只让它们输出到自己指定的日志中呢？log4j.additivity.userInfoLog = false它用来设置是否同时输出到log4j.rootLogger所配置的日志中，设为false就不会输出到其它地方啦！注意这里的”userInfoLog”是你在程序中给logger起的那个自定义的名字！如果你说，我只是不想同时输出这个日志到log4j.rootLogger所配置的logfile中，stdout里我还想同时输出呢！如：log4j.logger.userInfoLog=DEBUG, userLog, stdout 三、加载log4j.properties文件1、spring方式加载，配置与web.xml中：Spring加载log4j.properties，它提供了一个Log4jConfigListener，本身就能通过web.xml配置从指定位置加载log4j配置文件和log4j的输出路径，要注意的是 Log4jConfigListener必须要在Spring的Listener之前。 web.xml12345678910111213141516171819202122232425&lt;!-- 设置由Spring载入的Log4j配置文件位置 --&gt;&lt;context-param&gt;&lt;param-name&gt;log4jConfigLocation&lt;/param-name&gt;&lt;param-value&gt;WEB-INF/classes/log4j.properties&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- Spring刷新Log4j配置文件变动的间隔,单位为毫秒 --&gt;&lt;context-param&gt;&lt;param-name&gt;log4jRefreshInterval&lt;/param-name&gt;&lt;param-value&gt;10000&lt;/param-value&gt;&lt;/context-param&gt;&lt;listener&gt;&lt;listener-class&gt;org.springframework.web.util.Log4jConfigListener&lt;/listener-class&gt;&lt;/listener&gt; 2、可以通过资源类对资源文件进行加载，与使用为一体123456789public class Logger4JTest { public static void main(String[] args) { PropertyConfigurator.configure(\" D:/log/log4j.properties \"); Logger logger = Logger.getLogger(Logger4JTest.class); logger.debug(\" debug \"); logger.error(\" error \"); } } 四、在程序中的使用在程序中使用Log4j之前，首先要将commons-logging.jar和logging-log4j-1.2.9.jar导入到classpath中，并将log4j.properties放于src根目录中。接下来就可以使用了。 1.得到记录器使用Log4j，第一步就是获取日志记录器，这个记录器将负责控制日志信息。其语法为：public static Logger getLogger( String name)，通过指定的名字获得记录器，如果必要的话，则为这个名字创建一个新的记录器。Name一般取本类的名字，比如：static Logger logger = Logger.getLogger ( ServerWithLog4j.class.getName () ) ; 2.插入记录信息（格式化日志信息）当上两个必要步骤执行完毕，您就可以轻松地使用不同优先级别的日志记录语句插入到您想记录日志的任何地方，其语法如下：logger.debug ( Object message ) ;logger.info ( Object message ) ;logger.warn ( Object message ) ;logger.error ( Object message ) ;","link":"/2017/12/27/Java/Logger4j详解/"},{"title":"lambda与函数式","text":"前言Lambda表达式是Java SE 8才引进的新特性。对于只申明一个函数的接口，它提供了一个简单和简洁的编写方式。 语法1(参数...) -&gt; { 代码块 } 有三种格式：123(params) -&gt; expression(params) -&gt; statement(params) -&gt; { statements } 函数式接口像Comparator这样的只有一个抽象方法的接口，叫做函数式接口（Functional Interface）。与Comparator类似，其他函数式接口的唯一的抽象方法也可以用lambda来表示。 我们看一下Comparator的源码，发现其多了一个@FunctionalInterface的注解，用来表明它是一个函数式接口。标记了该注解的接口有且仅有一个抽象方法，否则会报编译错误。 再看一下其他的仅有一个抽象方法的接口，比如Runnable和Callable，发现也都在Java 8之后加了@FunctionalInterface注解。对于Runnable来说，接口定义如下：1234@FunctionalInterfacepublic interface Runnable { public abstract void run();} 不难推测，其lambda的写法应该是 () -&gt; { body }，它不接收任何参数，方法体中也无return返回值，用起来像这样：1new Thread(() -&gt; {doSomething();}); 此外，随lambda一同增加的还有一个java.util.function包，其中定义了一些常见的函数式接口的。比如： Function，接受一个输入参数，返回一个结果。参数与返回值的类型可以不同，我们之前的map方法内的lambda就是表示这个函数式接口的； Consumer，接受一个输入参数并且无返回的操作。比如我们针对数据流的每一个元素进行打印，就可以用基于Consumer的lambda； Supplier，无需输入参数，只返回结果。看接口名就知道是发挥了对象工厂的作用； Predicate，接受一个输入参数，返回一个布尔值结果。比如我们在对数据流中的元素进行筛选的时候，就可以用基于Predicate的lambda； … 例子Runnable Lambda我们可以使用Lambda表达式写一个Runnable测试程序：123456789101112131415161718public class RunnableTest { public static void main(String[] args) { //匿名内部类 Runnable r1 = new Runnable() { @Override public void run() { System.out.println(\"hello Runnable 1!\"); } }; //Lambda Runnable r2 = ()-&gt; System.out.println(\"hello Runnable 2\"); r1.run(); r2.run(); }} Comparator Lambda下面是java.util.Comparator的例子： 1234567891011enum Gender { MALE, FEMALE }public class Person { private String givenName; private String surName; private int age; private Gender gender; private String eMail; private String phone; private String address;} 123456789101112131415161718192021222324252627282930313233import java.util.Collections;import java.util.Comparator;import java.util.List;public class ComparatorTest { public static void main(String[] args) { List&lt;Person&gt; personList = Person.createShortList(); //匿名内部类 Collections.sort(personList, new Comparator&lt;Person&gt;() { @Override public int compare(Person o1, Person o2) { return o1.getSurName().compareTo(o2.getSurName()); } }); for(Person p: personList){ p.printName(); } //lambda 1 Collections.sort(personList, (Person o1, Person o2)-&gt;o1.getSurName().compareTo(o2.getSurName())); for(Person p: personList){ p.printName(); } //lambda 2 Collections.sort(personList, (o1, o2)-&gt;o1.getSurName().compareTo(o2.getSurName())); for(Person p: personList){ p.printName(); } }} 从上面的lambda 1和lambda 2中的参数可以看到，我们传入的o1, o2可以不用指定它的类型，编译器能够自动判断。（为什么？java.lang.Comparator接口只有一个方法） Listener Lambda最后，我们再看一下ActionListenter的例子： 123456789101112131415161718192021222324import javax.swing.*;import java.awt.*;import java.awt.event.ActionEvent;import java.awt.event.ActionListener;public class ListenerTest { public static void main(String[] args) { JButton testButton = new JButton(\"button\"); testButton.addActionListener(new ActionListener() { @Override public void actionPerformed(ActionEvent e) { System.out.println(\"click button, 匿名内部类\"); } }); testButton.addActionListener(event -&gt; System.out.println(\"click button, lambda\")); JFrame frame = new JFrame(\"test\"); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.add(testButton, BorderLayout.CENTER); frame.pack(); frame.setVisible(true); }} 通过上面的程序我们看到，lambda表达式作为一个参数传进方法中。","link":"/2018/11/13/Java/lambda与函数式/"},{"title":"Java配置中心","text":"阿里：Diamondhttps://github.com/hengyunabc/xdiamondhttps://yq.aliyun.com/articles/6058 携程：Apollohttps://github.com/ctripcorp/apollo/wiki/Apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83%E4%BB%8B%E7%BB%8Dhttps://github.com/ctripcorp/apollo/wiki/Quick-Start Spring: Spring Cload Confighttps://github.com/spring-cloud/spring-cloud-config","link":"/2017/08/08/Java/Java配置中心/"},{"title":"Centos7.3防火墙配置","text":"firewall防火墙1、查看firewall服务状态123456789101112131415查看状态： systemctl status firewalld 启动： systemctl start firewalld停止： systemctl stop firewalld禁用： systemctl enable firewalld启用： systemctl disable firewalld启动一个服务：systemctl start firewalld.service关闭一个服务：systemctl stop firewalld.service重启一个服务：systemctl restart firewalld.service显示一个服务的状态：systemctl status firewalld.service在开机时启用一个服务：systemctl enable firewalld.service在开机时禁用一个服务：systemctl disable firewalld.service查看服务是否开机启动：systemctl is-enabled firewalld.service查看已启动的服务列表：systemctl list-unit-files|grep enabled查看启动失败的服务列表：systemctl --failed 2、查看firewall的状态1firewall-cmd --state 3、开启、重启、关闭、firewalld.service服务123456# 开启service firewalld start# 重启service firewalld restart# 关闭service firewalld stop 4、查看防火墙规则123456789101112firewall-cmd --list-all -zone=public查看版本： firewall-cmd --version查看帮助： firewall-cmd --help显示状态： firewall-cmd --state查看所有打开的端口： firewall-cmd --zone=public --list-ports更新防火墙规则： firewall-cmd --reload查看区域信息: firewall-cmd --get-active-zones查看指定接口所属区域： firewall-cmd --get-zone-of-interface=eth0拒绝所有包：firewall-cmd --panic-on取消拒绝状态： firewall-cmd --panic-off查看是否拒绝： firewall-cmd --query-panic 5、查询、开放、关闭端口 12345678# 查询端口是否开放firewall-cmd --query-port=8080/tcp # 开放80端口firewall-cmd --permanent --add-port=8080/tcp --zone=public# 移除端口firewall-cmd --permanent --remove-port=8080/tcp --zone=public#重启防火墙(修改配置后要重启防火墙)firewall-cmd --reload 参数解释1、firwall-cmd：是Linux提供的操作firewall的一个工具；2、–permanent：表示设置为持久；3、–add-port：标识添加的端口； iptables防火墙1、安装123sudo yum install iptables-servicessudo systemctl enable iptables &amp;&amp; sudo systemctl enable ip6tablessudo systemctl start iptables &amp;&amp; sudo systemctl start ip6tables 2、启用、禁用1234 #最后重启防火墙使配置生效systemctl restart iptables.service# 设置防火墙开机启动systemctl enable iptables.service 3、编辑配置文件1vi /etc/sysconfig/iptables 12345678910111213141516# sampleconfiguration for iptables service# you can edit thismanually or use system-config-firewall# please do not askus to add additional ports/services to this default configuration*filter:INPUT ACCEPT [0:0]:FORWARD ACCEPT[0:0]:OUTPUT ACCEPT[0:0]-A INPUT -m state--state RELATED,ESTABLISHED -j ACCEPT-A INPUT -p icmp -jACCEPT-A INPUT -i lo -jACCEPT-A INPUT -p tcp -mstate --state NEW -m tcp --dport 22 -j ACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -jACCEPT-A INPUT -p tcp -m state --state NEW -m tcp --dport 8080-j ACCEPT-A INPUT -j REJECT--reject-with icmp-host-prohibited-A FORWARD -jREJECT --reject-with icmp-host-prohibitedCOMMIT 4、开启、停止123456# 查看防火墙状态： service iptables status# 开启防火墙：service iptables start# 关闭防火墙：service iptables stop","link":"/2018/01/02/Linux/Centos7.3防火墙配置/"},{"title":"Linux时间和时区","text":"如果你的 Linux 系统时区配置不正确，必需要手动调整到正确的当地时区。NTP 对时间的同步处理只计算当地时间与 UTC 时间的偏移量，因此配置一个 NTP 对时间进行同步并不能解决时区不正确的问题。所以大家在用了国外云计算服务商如 Microsoft Azure 或其它 VPS、虚拟机时，需要注意是否与中国大陆的时区一致。 时间123456# 查询时间date# 修改时间date -s \"2018-01-03 15:36:25\"# 查看时区时间ls -l /etc/localtime 时区/etc/localtime是用来描述本机时间，而 /etc/timezone是用来描述本机所属的时区.1234# 修改时区tzselect# 查看时区timedatectl Linux 用户一个多用户系统，每个用户都可以配置自己所需的时区，你可以为自己新增一个 TZ 环境变量：1export TZ='Asia/Shanghai' 执行完成之后需要重新登录系统或刷新 ~/.bashrc 生效。1source ~/.bashrc 更改Linux系统时区要更改 Linux 系统整个系统范围的时区可以使用如下命令：12sudo rm -f /etc/localtimesudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 注意：/usr/share/zoneinfo/Asia/Shanghai 中的具体时区请用自己获取到的 TZ 值进行替换。","link":"/2018/01/03/Linux/Linux时间和时区/"},{"title":"NIO框架的一点想法","text":"不管是什么NIO框架。本身其实都是对Java底层的一种在封装。封装一套更简便，更易于扩展的一套东西以方便开发者使用。所以性能上也许会有所差异，但是绝对没有java和C++之间这么多。(代码写的太烂的除外，不过想要使用java写出很烂的代码也比较困难。)这些框架在性能方面差别不会超过1%。 Mina和Netty开始。因为这两个NIO框架的创作者是同一个人Trustin Lee （韩国人）。GitHub主页地址 ：https://github.com/trustin。尽管创作者现在已经不专注与开发了。但是框架的后续开发和继承，可以说都是符合最开始的设定的。两个框架的架构设计思路基本一致。 Netty从某种程度上讲是Mina的延伸和扩展。解决了一些Mina上的设计缺陷，也优化了一下Mina上面的设计理念。 另一方面Netty相比较Mina更容易学习。API更简单。详细的范例源码和API文档。更活跃的论坛和社区。更高的代码更新维护速度。 我想不出什么理由来不选择Netty。 xSocket：是一个轻量级的基于nio的服务器框架用于开发高性能、可扩展、多线程的服务器。该框架封装了线程处理、异步读/写等方面。（只是对Java的NIO做了最简单的封装，以便于开发使用。） Grizzly ： 是一种应用程序框架，专门解决编写成千上万用户访问服务器时候产生的各种问题。使用JAVA NIO作为基础，并隐藏其编程的复杂性。容易使用的高性能的API。带来非阻塞socketd到协议处理层。利用高性能的缓冲和缓冲管理使用高性能的线程池。","link":"/2017/12/27/Java/NIO框架的一点想法/"},{"title":"Linux查看物理CPU个数、核数、逻辑CPU个数","text":"CPU总核数 = 物理CPU个数 每颗物理CPU的核数总逻辑CPU数 = 物理CPU个数 每颗物理CPU的核数 * 超线程数 查看CPU信息（型号）1234567891011121314[root@AAA ~]# cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 24 Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz# 查看物理CPU个数[root@AAA ~]# cat /proc/cpuinfo| grep \"physical id\"| sort| uniq| wc -l2# 查看每个物理CPU中core的个数(即核数)[root@AAA ~]# cat /proc/cpuinfo| grep \"cpu cores\"| uniqcpu cores : 6# 查看逻辑CPU的个数[root@AAA ~]# cat /proc/cpuinfo| grep \"processor\"| wc -l24","link":"/2018/02/07/Linux/Linux查看物理CPU个数、核数、逻辑CPU个数/"},{"title":"Spring-IOC源码剖析","text":"虽然现在已经是2018年，我们都用springboot，都用注解了，但spring的核心代码还是 spring 之父 Rod Johnson 在 2001 年写的。所以不影响我们学习spring 的核心。 我们打开spring-framework 源码。从哪里开始学习呢？回忆一下，我们最初学spring的时候，我们是这样开始的：123456789101112131415package test;import org.springframework.beans.tests.Person;import org.springframework.context.ApplicationContext;import org.springframework.context.support.FileSystemXmlApplicationContext;public class Test { public static void main(String[] args) throws ClassNotFoundException { ApplicationContext ctx = new FileSystemXmlApplicationContext (\"spring-beans/src/test/resources/beans.xml\"); System.out.println(\"number : \" + ctx.getBeanDefinitionCount()); ((Person) ctx.getBean(\"person\")).work(); }}","link":"/2018/11/23/Spring/Spring-IOC源码剖析/"},{"title":"Java线程-Thread源码剖析","text":"概述 线程是一个操作系统级别的概念。JAVA语言（包括其他编程语言）本身不创建线程；而是调用操作系统层提供的接口创建、控制、销毁线程实例。 根据操作系统的不同（Windows/Unix/Linux/其他），他们所支持的线程底层实现和操作效果也是不尽相同的。不过一个操作系统支持的线程至少会有四种状态：就绪、执行、阻塞和终结。线程在四种状态下进行切换，都是要消耗不少的CPU计算能力的。 并且根据操作系统使用线程的进程的不一样，线程还分为用户线程和操作系统线程。操作系统线程（内核线程），是指操作系统内核为了完成硬件接口层操作，由操作系统内核创建的线程：例如I/O操作的内核线程，这些线程应用程序是不能干预的；用户线程，是指用户安装/管理的应用程序，为执行某一种操作，而由这个应用程序创建的线程。后文我们讨论的JAVA线程，都是用户级线程。 线程在创建时，操作系统不会为这个线程分配独立的资源（除了必要的数据支撑）。一个应用程序（进程）下的所有线程，都是共享这个应用程序（进程）中的资源，例如这个应用程序的CPU资源、I/O资源、内存资源。 现在基本上主流操作系统都支持多线程实现。即一个应用程序中（一个进程中），可以创建多个线程。一个应用程序下，各个线程间都可以进行通讯、可以进行状态互操作。且一个进程中，至少有一个线程存在。 类定义1public class Thread implements Runnable Thread实现Runnable接口，实现run方法 静态初始化块12345private static native void registerNatives();static { registerNatives();} 从上面的代码中看到定义了一个静态初始化块，我们知道当创建Java对象时，系统总是先调用静态初始化块 在上面的静态初始化块中调用了registerNatives()方法，并且使用了private来修饰，表面这个方法是私有的并不被外部调用。 在Java中使用native关键字修饰的方法，说明此方法并不是由Java中完成的，而是通过C/C++来完成的，并被编译成.dll，之后才由Java调用。方法的具体实现是在dll文件中，当然对于不同平台实现的细节也有所不同，以上registerNatives()方法主要作用就是将C/C++中的方法映射到Java中的native方法，实现方法命名的解耦 属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104 private char name[]; //线程名字 private int priority; //优先级 private Thread threadQ; //未知 private long eetop; //未知 /* Whether or not to single_step this thread. */ //是否是单步执行 private boolean single_step; /* Whether or not the thread is a daemon thread. */ //是否是守护线程 private boolean daemon = false; /* JVM state */ // 虚拟机状态 private boolean stillborn = false; /* What will be run. */ //将会被执行的Runnable. private Runnable target; /* The group of this thread */ //这个线程的组 private ThreadGroup group; /* The context ClassLoader for this thread */ //这个线程的上下文 private ClassLoader contextClassLoader; /* The inherited AccessControlContext of this thread */ //继承的请求控制 private AccessControlContext inheritedAccessControlContext; //默认线程的自动编号 private static int threadInitNumber; private static synchronized int nextThreadNum() { return threadInitNumber++; } //该线程请求的堆栈大小 默认一般都是忽略 private long stackSize; //未知 private long nativeParkEventPointer; // 每个线程都有专属ID，但名字可能重复 private long tid; //用来生成thread ID private static long threadSeqNumber; //得到下个thread ID private static synchronized long nextThreadID() { return ++threadSeqNumber; }//标识线程状态，默认是线程未启动 private int threadStatus = 0; /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ //当前线程附属的ThreadLocal，而ThreadLocalMap会被ThreadLocal维护） ThreadLocal.ThreadLocalMap threadLocals = null; /* * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */ // 主要作用：为子线程提供从父线程那里继承的值 // 在创建子线程时，子线程会接收所有可继承的线程局部变量的初始值，以获得父线程所具有的值 // 创建一个线程时如果保存了所有 InheritableThreadLocal 对象的值，那么这些值也将自动传递给子线程 // 如果一个子线程调用 InheritableThreadLocal 的 get() ，那么它将与它的父线程看到同一个对象 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; // 中断阻塞器：当线程发生IO中断时，需要在线程被设置为中断状态后调用该对象的interrupt方法 volatile Object parkBlocker; //阻塞器锁，主要用于处理阻塞情况 private volatile Interruptible blocker; //阻断锁 private Object blockerLock = new Object(); void blockedOn(Interruptible b) { synchronized (blockerLock) { blocker = b; } } //线程的优先级中最小的 public final static int MIN_PRIORITY = 1; //线程的优先级中第二的同时也是默认的优先级 public final static int NORM_PRIORITY = 5; //最高的优先级 public final static int MAX_PRIORITY = 10; /* If stop was called before start */ //判断stop是否在Start前 private boolean stopBeforeStart; /* Remembered Throwable from stop before start */ //未知 private Throwable throwableFromStop; 构造方法group是线程组，target被调用RUN方法的目标对象，name新线程的名字，stackSize用于新线程分配所需堆栈大小123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public Thread() { init(null, null, \"Thread-\" + nextThreadNum(), 0);}public Thread(Runnable target) { init(null, target, \"Thread-\" + nextThreadNum(), 0);}public Thread(ThreadGroup group, Runnable target) { init(group, target, \"Thread-\" + nextThreadNum(), 0);}public Thread(String name) { init(null, null, name, 0);}public Thread(ThreadGroup group, String name) { init(group, null, name, 0);}public Thread(Runnable target, String name) { init(null, target, name, 0);}public Thread(ThreadGroup group, Runnable target, String name) { init(group, target, name, 0);}public Thread(ThreadGroup group, Runnable target, String name, long stackSize) { init(group, target, name, stackSize);}private void init(ThreadGroup g, Runnable target, String name, long stackSize) { init(g, target, name, stackSize, null, true);}private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) { // name不能为空 if (name == null) { throw new NullPointerException(\"name cannot be null\"); } this.name = name; // 将当前线程设置问父线程 Thread parent = currentThread(); //获得系统的安全管理器 SecurityManager security = System.getSecurityManager(); if (g == null) { /* Determine if it's an applet or not */ //安全检查 if (security != null) { g = security.getThreadGroup(); } /* 如果没有指定线程组，默认使用父线程组. */ if (g == null) { g = parent.getThreadGroup(); } } /* 检测当前运行线是否有权限修改线程组，没有会抛异常 */ g.checkAccess(); /* * 检测我们是否拥有所需的权限--子类实现权限 */ if (security != null) { if (isCCLOverridden(getClass())) { security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); } } //往线程组添加线程但未启动 g.addUnstarted(); //设置当前线程线程组 this.group = g; //设置当前线程守护线程 this.daemon = parent.isDaemon(); //设置当前线程优先级 this.priority = parent.getPriority(); // 每个线程都有一个优先级，高优先级线程的执行优先于低优先级线程。每个线程都可以或不可以标记为一个守护程序。 // 当某个线程中运行的代码创建一个新 Thread 对象时，该新线程的初始优先级被设定为创建线程的优先级， // 并且当且仅当创建线程是守护线程时，新线程才是守护程序。 if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); this.target = target; setPriority(priority); if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* 设置线程ID */ tid = nextThreadID();} 普通方法start使线程进入可执行（runnable状态）的状态1234567891011121314151617public synchronized void start() { //验证线程的状态，如果线程已启动则抛异常 if (threadStatus != 0) throw new IllegalThreadStateException(); //向线程组里添加此线程 group.add(this); //使线程进入可执行（runnable状态）的状态 start0(); //如果验证停止前于开始 if (stopBeforeStart) { //停止线程 stop0(throwableFromStop); }//开始线程private native void start0(); run使线程从可运行状态转化为运行状态123456public void run() { if (target != null) { // 执行Runnable 的run方法 target.run(); }} sleep使线程进入睡眠（sleep状态）的状态12345678910111213141516171819202122// 静态方法强制当前正在执行的线程休眠（暂停执行），以“减慢线程”。 当线程睡眠时，它睡在某个地方，在苏醒之前不会返回到可运行状态。 当睡眠时间到期，则返回到可运行状态。public static native void sleep(long millis) throws InterruptedException;// millis为毫秒，nanos为纳秒，1000纳秒=1毫秒，其他跟sleep方法一样public static void sleep(long millis, int nanos) throws InterruptedException { if (millis &lt; 0) { throw new IllegalArgumentException(\"timeout value is negative\"); } if (nanos &lt; 0 || nanos &gt; 999999) { throw new IllegalArgumentException( \"nanosecond timeout value out of range\"); } if (nanos &gt;= 500000 || (nanos != 0 &amp;&amp; millis == 0)) { millis++; } // 调用了native方法 sleep(millis);} exit这个方法是在Run方法执行结束后用于结束线程的。通过单步调试一个线程发现执行完run方法之后会进入exit方法。1234567891011121314private void exit() { if (group != null) { group.remove(this); group = null; } target = null; threadLocals = null; inheritableThreadLocals = null; inheritedAccessControlContext = null; blocker = null; uncaughtExceptionHandler = null;} stop停止线程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public final void stop() { // If the thread is already dead, return. // A zero status value corresponds to \"NEW\". //如果线程已启动或者已经销毁 if ((threadStatus != 0) &amp;&amp; !isAlive()) { return; } //执行stop1 stop1(new ThreadDeath());}@Deprecatedpublic final synchronized void stop(Throwable obj) { stop1(obj);}private final synchronized void stop1(Throwable th) { //获得系统的安全管理器 SecurityManager security = System.getSecurityManager(); //如果安全管理器为空 if (security != null) { //检查是否允许调用线程修改线程组参数 checkAccess(); //如果这个线程不是当前线程或者这个线程以销毁 if ((this != Thread.currentThread()) || (!(th instanceof ThreadDeath))) { //如果所请求的访问，通过给定的权限，指定的安全策略不允许根据当前有效的方法将抛出一个SecurityException。 security.checkPermission(SecurityConstants.STOP_THREAD_PERMISSION); } } // A zero status value corresponds to \"NEW\" //如果线程已启动 if (threadStatus != 0) { //如果线程被挂起则唤醒 resume(); // Wake up thread if it was suspended; no-op otherwise //停止线程 stop0(th); } else { // Must do the null arg check that the VM would do with stop0 //如果线程为空抛空指针 if (th == null) { throw new NullPointerException(); } // Remember this stop attempt for if/when start is used stopBeforeStart = true; throwableFromStop = th; }}//停止线程private native void stop0(Object o); suspend线程挂起1234567public final void suspend() { checkAccess(); suspend0();}// 线程挂起(暂停)private native void suspend0(); resume线程从挂起到唤醒1234567public final void resume() { checkAccess(); resume0();}//将一个挂起线程复活继续执行private native void resume0(); yield使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。cpu会从众多的可执行态里选择，也就是说，当前也就是刚刚的那个线程还是有可能会被再次执行到的，并不是说一定会执行其他线程而该线程在下一次中不会执行到了1public static native void yield(); join1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public final void join() throws InterruptedException { join(0);}public final synchronized void join(long millis) throws InterruptedException { long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) { throw new IllegalArgumentException(\"timeout value is negative\"); } if (millis == 0) { while (isAlive()) { wait(0); } } else { while (isAlive()) { long delay = millis - now; if (delay &lt;= 0) { break; } wait(delay); now = System.currentTimeMillis() - base; } }}public final synchronized void join(long millis, int nanos) throws InterruptedException { if (millis &lt; 0) { throw new IllegalArgumentException(\"timeout value is negative\"); } if (nanos &lt; 0 || nanos &gt; 999999) { throw new IllegalArgumentException( \"nanosecond timeout value out of range\"); } if (nanos &gt;= 500000 || (nanos != 0 &amp;&amp; millis == 0)) { millis++; } join(millis);} 实例Thread1234567891011121314151617181920212223public class SimpleThreadLearning extends Thread{ private String name; public SimpleThreadLearning(String name){ this.name = name; super.setName(\"SimpleThreadLearning\"); } @Override public void run() { int i = 1; while(i&lt;100){ System.out.println(name + \": \"+i); i++; } } public static void main(String[] args) { new SimpleThreadLearning(\"线程1\").start(); new SimpleThreadLearning(\"线程2\").start(); }} Runnable123456789101112131415161718192021public class SimpleRunableLearning implements Runnable{ private String name; public SimpleRunableLearning(String name) { this.name = name; } public void run() { for (int i = 0; i &lt; 5; i++) { for (long k = 0; k &lt; 10000000; k++) ; System.out.println(name + \": \" + i); } } public static void main(String[] args) { new Thread(new SimpleRunableLearning(\"线程1\")).start(); new Thread(new SimpleRunableLearning(\"线程2\")).start(); }}","link":"/2018/12/06/Java线程/Thread源码剖析/"},{"title":"Hexo博客（一）在GitHub搭建博客","text":"1 Git准备1.1 申请GitHub1、进入https://github.com/ 2、点击”New repository”，新建一个仓库。 注意：输入Repository name:yourname.github.io(yourname与你的注册用户名一致,这个就是你博客的域名了) 3、启用GitHub Page 点击右边的“Setting”菜单进入设置,点击”Launch automatic page generator”，然后再点击点击底部的”Continue to layouts”。最后点击”Publish page”,发布github默认生成的一个静态站点。 4、验证邮箱 点击右上角个人设置的“Setting”菜单进入设置，再点击Emails,点击”Send verification Email”发送验证邮件。进入你的邮箱，查收验证邮件进行验证。 1.2 安装Git1.2.1下载https://git-for-windows.github.io/ 1.2.2安装 安装过程中，询问是否修改环境变量，选择“Use Git Bash Only”. 即只在msysGit提供的Shell (NOTE: 这个步骤最好选择第二项“Use Git from the Windows Command Prompt”， 这样在Windows的命令行cmd中也可以运行git命令了。这样会对以后的一些操作带来方便，比如Win7下安装配置gVim) 配置行结束标记，保持默认“Checkout Windows-style, commit Unix-style line endings”. 1.2.3中文乱码问题解决方法ls 不能显示中文目录 解决办法：在git/git-completion.bash中增加一行【4】： 1alias ls='ls --show-control-chars --color=auto' 另外，Git Shell 不支持 ls -l的缩写形式ll，也为其添加一个alias 1alias ll='ls -l' 1.2.4 运行 Git 前的配置 配置你个人的用户名称和电子邮件地址,打开git bash。 12$git config --global user.name \"xxx\"$git config --global user.email xxx@example.com 配置GitHub SSH （1）首先使用 ssh-keygen 生成 SSH 密钥123456cd ~/.ssh/Administrator@THINKPAD ~/.ssh ls known_hostsssh-keygen -t rsa -C \"youremail@example.com\" 有提示，直接回车即可。生成key以后检查下”~”目录下的.ssh目录下是否多了2个文件 。 把 id_rsa.pub 中的全部内容复制，包括最后的一个换行。或者用命令：1clip &lt; ~/.ssh/id_rsa.pub （2）配置Github SSH。 登陆GitHub-&gt;Settings-&gt;“SSH Keys”，然后，点“Add SSH Key”，起个Title，在Key文本框里粘贴id_rsa.pub文件的内容，点“Add Key”。 （3）测试是否可以连接到github1234ssh git@github.comHi imsofter! You've successfully authenticated, but GitHub does not provide shell access.Connection to github.com closed. 现在可以将代码推上github上了。 2 安装node.js下载：http://nodejs.org/download/ 可以下载 node-v4.2.1-x64.msi 安装时直接保持默认配置即可。 3 安装Hexo关于Hexo的安装配置过程，请以官方Hexo给出的步骤为准,大致如下： 在电脑上E盘新建一个blog文件夹,该文件夹用于存放你的博客文件,然后右键单击选择“Git Bash” 3.1 Installation打开Git命令行，执行如下命令1$ npm install hexo-cli g 3.2 Quick Start1. Setup your blog 在电脑中建立一个名字叫「Hexo」的文件夹（比如我建在了E:\\Hexo），然后在此文件夹中右键打开Git Bash。执行下面的命令12345$ cd /e/blog$ hexo init[info] Copying data[info] You are almost done! Don't forget to run `npm install` before you start blogging with Hexo! Hexo随后会自动在目标文件夹建立网站所需要的文件。 然后按照提示，运行 npm install（在 /e/blog下）1npm install 会在e:\\blog目录中生成 node_modules。 安装其它插件12345678npm install hexo-server --savenpm install hexo-admin --savenpm install hexo-generator-archive --savenpm install hexo-generator-feed --savenpm install hexo-generator-search --savenpm install hexo-generator-tag --savenpm install hexo-deployer-git --savenpm install hexo-generator-sitemap --save 2. Start the server 运行下面的命令（在 /E/Hexo下）12$ hexo server[info] Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 表明Hexo Server已经启动了，在浏览器中打开 http://localhost:4000/，这时可以看到Hexo已为你生成了一篇blog。 你可以按Ctrl+C 停止Server。 3. Create a new post 新打开一个git bash命令行窗口，cd到/e/blog下，执行下面的命令12$ hexo new \"My New Post\"[info] File created at e:\\blog\\source\\_posts\\My-New-Post.md 刷新http://localhost:4000/，可以发现已生成了一篇新文章 “My New Post”。 NOTE： 有一个问题，发现 “My New Post” 被发了2遍，在Hexo server所在的git bash窗口也能看到create了2次。1234$ hexo server[info] Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.[create] e:\\blog\\source\\_posts\\My-New-Post.md[create] e:\\blog\\source\\_posts\\My-New-Post.md 经验证，在hexo new “My New Post” 时，如果按Ctrl+C将hexo server停掉，就不会出现发2次的问题了。 所以，在hexo new文章时，需要stop server。 4. Generate static files 执行下面的命令，将markdown文件生成静态网页。 该命令执行完后，会在 E:\\blog\\public\\ 目录下生成一系列html，css等文件。 5. 编辑文章 hexo new “My New Post”会在E:\\blog\\source_posts目录下生成一个markdown文件：My-New-Post.md 可以使用一个支持markdown语法的编辑器（比如 Sublime Text 2）来编辑该文件。 6. 部署到Github 运行 npm install hexo-deployer-git --save 安装git支持 部署到Github前需要配置_config.yml文件，首先找到下面的内容1234# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: 然后将它们修改为123456# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: github repository: git@github.com:hmiter/hmiter.github.io.git branch: master NOTE1: Repository：必须是SSH形式的url（git@github.com:hmiter/hmiter.github.io.git），而不能是HTTPS形式的url（https://github.com/hmiter/hmiter.github.io.git），否则会出现错误：123$ hexo deploy[info] Start deploying: github[error] https://github.com/hmiter/hmiter.github.io is not a valid repositor URL! 使用SSH url，如果电脑没有开放SSH 端口，会致部署失败。12fatal: Could not read from remote repository.Please make sure you have the correct access rights and the repository exists. NOTE2： 如果你是为一个项目制作网站，那么需要把branch设置为gh-pages。 NOTE3： hexo3.0以上的版本type为git 7. 测试 当部署完成后，在浏览器中打开http://hmiter.github.io/（https://hmiter.github.io/） ，正常显示网页，表明部署成功。 8. 总结：部署步骤 每次部署的步骤，可按以下三步来进行。123hexo cleanhexo generatehexo deploy 9. 总结：本地调试 在执行下面的命令后，12$ hexo g #生成$ hexo s #启动本地服务，进行文章预览调试 浏览器输入http://localhost:4000，查看搭建效果。此后的每次变更_config.yml 文件或者新建文件都可以先用此命令调试，尤其是当你想调试新添加的主题时。 可以用简化的一条命令1hexo s -g 10.hexo命令缩写hexo支持命令缩写，如下所示。hexo g等价于hexo generate1234hexo g：hexo generatehexo c：hexo cleanhexo s：hexo serverhexo d：hexo deploy","link":"/2017/01/04/hexo/Hexo博客（一）在GitHub搭建博客/"},{"title":"Spring常用注解","text":"Java原生注解@Retention：注解说明,这种类型的注解会被保留到那个阶段1.RetentionPolicy.SOURCE —— 这种类型的Annotations只在源代码级别保留,编译时就会被忽略2.RetentionPolicy.CLASS —— 这种类型的Annotations编译时被保留,在class文件中存在,但JVM将会忽略3.RetentionPolicy.RUNTIME —— 这种类型的Annotations将被JVM保留,所以他们能在运行时被JVM或其他使用反射机制的代码所读取和使用. @Documented：作用是在生成javadoc文档的时候将该注解也写入到文档中。javadoc默认是不会将注解写到文档中的。 @Target:注解的使用的目标 @Target(ElementType.TYPE) //接口、类、枚举、注解 @Target(ElementType.FIELD) //字段、枚举的常量 @Target(ElementType.METHOD) //方法 @Target(ElementType.PARAMETER) //方法参数 @Target(ElementType.CONSTRUCTOR) //构造函数 @Target(ElementType.LOCAL_VARIABLE)//局部变量 @Target(ElementType.ANNOTATION_TYPE)//注解 @Target(ElementType.PACKAGE) ///包 @Inherited：它指明被注解的类的所有属性将会自动继承到它的子类中. #java EE规范@Resource：为目标bean指定协作者Bean@Inject：为目标bean指定协作者Bean@Named：为目标bean指定协作者Bean@Qualifier：@PostConstruct：在构造函数执行完成之后执行。@PreDestory：bean销毁之前的执行方法。 Spring注解@Component :标注一个普通的spring Bean类@Service :标注一个业务逻辑组件类。@Repository :标注一个DAO组件类。@Controller:标注一个控制器组件类。 @EnableWebMvc：开启MVC 方法级别@ResetController：组合注解，组合了@Controller和@RequestBody@RequestMapping：配置URI和方法之间的映射 @GetMapping @PostMapping @PutMapping@RequestHeader 注解，可以把Request请求header部分的值绑定到方法的参数上。如：@RequestHeader(“Accept-Encoding”) String encoding@CookieValue 可以把Request header中关于cookie的值绑定到方法的参数上。如：@CookieValue(“JSESSIONID”) String cookie @CrossOrigin:实现跨域访问。 参数@PathVariable：接受路径参数，如someUrl/{paramId}, 这时的paramId可通过 @Pathvariable注解绑定它传过来的值到方法的参数上。@RequestParam：A） 常用来处理简单类型的绑定，通过Request.getParameter() 获取的String可直接转换为简单类型的情况（ String–&gt; 简单类型的转换操作由ConversionService配置的转换器来完成）；因为使用request.getParameter()方式获取参数，所以可以处理get 方式中queryString的值，也可以处理post方式中 body data的值；B）用来处理Content-Type: 为 application/x-www-form-urlencoded编码的内容，提交方式GET、POST；C) 该注解有两个属性： value、required； value用来指定要传入值的id名称，required用来指示参数是否必须绑定；@RequestBody：该注解常用来处理Content-Type: 不是application/x-www-form-urlencoded编码的内容，例如application/json, application/xml等；它是通过使用HandlerAdapter 配置的HttpMessageConverters来解析post data body，然后绑定到相应的bean上的。因为配置有FormHttpMessageConverter，所以也可以用来处理 application/x-www-form-urlencoded的内容，处理完的结果放在一个MultiValueMap里，这种情况在某些特殊需求下使用 @InitBinder：用来设置WebDataBinder，WebDataBinder用来自动绑定前台请求的参数到Model中。@ExceptionHandler：用于全局处理控制器里的异常。@ModelAttribute该注解有两个用法，一个是用于方法上，一个是用于参数上；用于方法上时： 通常用来在处理@RequestMapping之前，为请求绑定需要从后台查询的model；用于参数上时： 用来通过名称对应，把相应名称的值绑定到注解的参数bean上；要绑定的值来源于： @SessionAttribute:该注解用来绑定HttpSession中的attribute对象的值，便于在方法中的参数里使用。@RequestAttribute：可以被用于访问由过滤器或拦截器创建的、预先存在的请求属性 @ResponseBody：支持将返回值放在response的body体内 @Scope：注解也可以指定Bean实例的作用域。取值：Singleton(默认)|Prototype|Request|Session|GlobalSession@Autowired：为目标bean指定协作者Bean@Value：为属性注入值 @Aspect：声明一个切面@PointCut：定义拦截规则@After：标注一个之后建言@Before：标注一个之前建言@Around：标注一个运行时建言 @Transcational：事物@Cacheable：数据缓存 @EnableScheduling：开启计划任务@Scheduled：声明这是一个计划任务 @Import：since4.2。导入普通的java类,并将其声明成一个bean@Configuration：声明一个配置类@ComponentScan：制定Spring扫描包路径。@Bean 注解在方法上，声明当前方法的返回@Profile：可以注解在类或方法上。 在不同环境下使用不同配置提供支持。@Conditional：根据满足某一个特定条件创建一个特定的Bean。 Spring boot@SpringBootApplication ：会根据类路径中jar包自动进行相关配置。@EnableAutoConfiguration@ConfigurationProperties：加载一个properties文件。 @ConditionalOnBean:当容器里有指定的Bean的条件下@ConditionalOnClass:当类路径下有指定的类的条件下。@ConditionalOnExpression：基于SPEl表达式作为判断条件。@ConditionalOnJava：基于JVM版本作为判断条件。@ConditionalOnJndi:在JNDI存在的条件下查找指定的位置。@ConditionalOnMissingBean：当容器里没有指定Bean的条件下。@ConditionalOnMissingClass:当类路径下没有指定的类的条件下。@ConditionalOnWebApplication:当前项目是web项目的条件下。@ConditionalOnNotWebApplication:当前项目部是WEb项目的条件下。@ConditionalOnProperty:指定的属性是否有指定的值。@ConditionalOnResource：类路径是否有指定的值。@ConditionalOnSingleCandidate:当指定首选的Bean。","link":"/2017/04/20/Spring/Spring常用注解/"},{"title":"spring声明式事务配置","text":"上周要一个同事开发一个模块，他说事物死活不起作用。我看了一下，大致主要配置如下： 12345678910111213141516171819202122232425262728&lt;!-- 配置事务管理 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"businessDataSource\" /&gt;&lt;/bean&gt;&lt;!-- 事务相关控制配置：例如配置事务的传播机制 --&gt;&lt;tx:advice id=\"fortressAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"get*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"query*\" propagation=\"SUPPORTS\" read-only=\"false\" /&gt; &lt;tx:method name=\"count*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"save*\" propagation=\"REQUIRED\" read-only=\"false\" /&gt; &lt;tx:method name=\"remove*\" propagation=\"REQUIRED\" read-only=\"false\" /&gt; &lt;tx:method name=\"modify*\" propagation=\"REQUIRED\" read-only=\"false\" /&gt; &lt;tx:method name=\"*\" propagation=\"SUPPORTS\" read-only=\"true\" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 事务控制切入点在service层|第一个 * —— 通配 任意返回值类型||第二个 * —— 通配包com.mindasoft.fortress.service下的任意class||第三个 * —— 通配包com.mindasoft.fortress.service下的任意class的任意方法||第四个 .. —— 通配 办法可以有0个或多个参数--&gt;&lt;aop:config&gt; &lt;aop:pointcut id=\"allFortressMethod\" expression=\"execution(* com.mindasoft.fortress.service.*.*(..))\" /&gt; &lt;aop:advisor advice-ref=\"fortressAdvice\" pointcut-ref=\"allFortressMethod\" /&gt;&lt;/aop:config&gt; 配置是这样没错，他的配置也问题，但是为什么会不起作用呢？Debug进行，发现根本就没有进入事物处理。为什么会这样？看配置web.xml1234567891011121314151617181920212223&lt;!-- Spring MVC 子Context配置 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:/config/servlet.web.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;DispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;*.html&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- Spring root Context监听配置 --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath*:/config/**/*.bean.xml&lt;/param-value&gt;&lt;/context-param&gt; Spring和SpringMVC一起使用时，ContextLoaderListener会初始化一个Context，这个Context是RootContext，而DispatcherServlet也会初始化一个Context，简称WebContext。WebContext是RootContext的子集。当mvc有自己的bean时便不再去向父context要bean。 所以，在servlet.web.xml 和*.bean.xml 当中，各自的component-scan配置要指定相应位置，否则会导致bean混乱，从导致声明事务无效。如下：servlet.web.xml1&lt;context:component-scan base-package=\"com.mindasoft.*.controllers\" /&gt; *.bean.xml12&lt;context:component-scan base-package=\"com.mindasoft.*.dao\" /&gt;&lt;context:component-scan base-package=\"com.mindasoft.*.service\" /&gt; 当然你也可以使用DispatcherServlet加载全部的配置文件或者将AOP配置复制到servlet.web.xml中。 这个修改了之后还是不行？？看了下他的代码，他是自己写了一个Exception。然后断点跟踪源代码TransactionAspectSupport：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * Handle a throwable, completing the transaction. * We may commit or roll back, depending on the configuration. * @param txInfo information about the current transaction * @param ex throwable encountered */protected void completeTransactionAfterThrowing(TransactionInfo txInfo, Throwable ex) { if (txInfo != null &amp;&amp; txInfo.hasTransaction()) { if (logger.isTraceEnabled()) { logger.trace(\"Completing transaction for [\" + txInfo.getJoinpointIdentification() + \"] after exception: \" + ex); } if (txInfo.transactionAttribute.rollbackOn(ex)) { try { txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus()); } catch (TransactionSystemException ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); ex2.initApplicationException(ex); throw ex2; } catch (RuntimeException ex2) { logger.error(\"Application exception overridden by rollback exception\", ex); throw ex2; } catch (Error err) { logger.error(\"Application exception overridden by rollback error\", ex); throw err; } } else { // We don't roll back on this exception. // Will still roll back if TransactionStatus.isRollbackOnly() is true. try { txInfo.getTransactionManager().commit(txInfo.getTransactionStatus()); } catch (TransactionSystemException ex2) { logger.error(\"Application exception overridden by commit exception\", ex); ex2.initApplicationException(ex); throw ex2; } catch (RuntimeException ex2) { logger.error(\"Application exception overridden by commit exception\", ex); throw ex2; } catch (Error err) { logger.error(\"Application exception overridden by commit error\", ex); throw err; } } }} 上面的方法中有这么一段txInfo.transactionAttribute.rollbackOn(ex)，这里是判断是否需要执行回滚操作的，跟踪rollbackOn方法最后会执行到DefaultTransactionAttribute中的rollbackOn方法。12345678/** * The default behavior is as with EJB: rollback on unchecked exception. * Additionally attempt to rollback on Error. * &lt;p&gt;This is consistent with TransactionTemplate's default behavior. */public boolean rollbackOn(Throwable ex) { return (ex instanceof RuntimeException || ex instanceof Error);} 到这里，应该都清楚了。。。自己主动抛异常Exception是不对的。这里只捕获运行时异常RuntimeException 及Error，所以我们测试时不可以直接抛Exception，而应该换成RuntimeException 。当然。也可在xml中指定rollback-for。123456789101112&lt;!-- 事务相关控制配置：例如配置事务的传播机制 --&gt;&lt;tx:advice id=\"fortressAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;tx:method name=\"get*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"query*\" propagation=\"SUPPORTS\" read-only=\"false\" /&gt; &lt;tx:method name=\"count*\" propagation=\"SUPPORTS\" read-only=\"false\"/&gt; &lt;tx:method name=\"save*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"Exception\"/&gt; &lt;tx:method name=\"remove*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"Exception\"/&gt; &lt;tx:method name=\"modify*\" propagation=\"REQUIRED\" read-only=\"false\" rollback-for=\"Exception\"/&gt; &lt;tx:method name=\"*\" propagation=\"SUPPORTS\" read-only=\"true\" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;","link":"/2017/03/19/Spring/spring声明式事务配置/"},{"title":"Linux服务器时间同步","text":"一、连网的情况：同步互联网的时间(可自行找一个时间服务器)1ntpdate ntp1.aliyun.com 如果没有该命令，可以执行一下安装1yum -y install ntupdate ntp 二、离线情况：以其中一台最接近当前网络时间的服务器作为时间服务器，然后其他机器将时间同步到与该机器一致。1、作为时间服务器的那台机器需要开启ntpd服务，其他机器不用开启，命令如下1service ntpd start 2、其它机器依次执行同步命令1ntpdate 时间服务器的ip 执行完上述步骤便完成时间同步了。 三、配置定时任务同步时间建议可以通过配置定时任务定时去同步时间，配置如下10 3 * * * /usr/sbin/ntpdate -u ntp1.aliyun.com ,凌晨3点 或者10 */1 * * * /usr/sbin/ntpdate -u ntp1.aliyun.com，每隔1小时同步一次时间。 四、系统时间同步到硬件，防止系统重启后时间被还原hwclock –systohc","link":"/2018/01/03/Linux/Linux 服务器时间同步/"},{"title":"Hexo博客（二）更换主题和相关设置","text":"更换主题icarus1、下载icarus主题地址：https://github.com/ppoffice/hexo-theme-icarus2、更换主题icarus，修改Hexo的_config.yml里面的theme如下：1theme: icarus 搜索插件主题已经集成了，主题的_config.yml1234search: insight: true # you need to install `hexo-generator-json-content` before using Insight Search swiftype: # enter swiftype install key here baidu: false # you need to disable other search engines to use Baidu search, options: true, false 默认配置不用改。但是需要安装hexo-generator-json-content。命令如下：1$ npm install -S hexo-generator-json-content 打赏在国外比较流行打赏，在中国嘛…有总比没有好，万一有人打赏呢？是吧。在主题配置文件_config.yml的comment上面添加:123#donate 打赏donate: true # options: true , falsedonate_message: 如果您觉得文章不错,可以请我喝一杯咖啡！ 在layout添加文件jdonate.ejs，内容如下：123456789101112131415161718192021222324252627&lt;! -- 添加捐赠图标 --&gt;&lt;div class =\"post-donate\"&gt; &lt;div id=\"donate_board\" class=\"donate_bar center\"&gt; &lt;a id=\"btn_donate\" class=\"btn_donate\" href=\"javascript:;\" title=\"打赏\"&gt;&lt;/a&gt; &lt;span class=\"donate_txt\"&gt; &lt;%=theme.donate_message%&gt; &lt;/span&gt; &lt;br&gt; &lt;/div&gt; &lt;div id=\"donate_guide\" class=\"donate_bar center hidden\" &gt; &lt;!-- 支付宝打赏图案 --&gt; &lt;img src=\"/img/zhifubao.png\" alt=\"支付宝打赏\" &gt; &lt;!-- 微信打赏图案 --&gt; &lt;img src=\"/img/weixin.png\" alt=\"微信打赏\" &gt; &lt;/div&gt; &lt;script type=\"text/javascript\"&gt; document.getElementById('btn_donate').onclick = function(){ if($('#donate_guide').hasClass('hidden')){ $('#donate_guide').removeClass('hidden'); }else{ $('#donate_guide').addClass('hidden'); } } &lt;/script&gt;&lt;/div&gt;&lt;! -- 添加捐赠图标 --&gt; 在主题source\\css_partial目录新增donate.styl：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647.donate_bar { text-align: center; background: #fff --margin-top: 5%}.donate_bar a.btn_donate { display: inline-block; width: 82px; height: 82px; margin-left: auto; margin-right: auto; background: url(../img/btn_reward.gif)no-repeat; -webkit-transition: background 0s; -moz-transition: background 0s; -o-transition: background 0s; -ms-transition: background 0s; transition: background 0s}.donate_bar a.btn_donate:hover { background-position: 0 -82px}.donate_bar .donate_txt { display: block; color: #9d9d9d; font: 14px/2 \"Microsoft Yahei\"}.donate_bar.hidden{ display: none}.post-donate{ --margin-top: 80px;}#donate_guide{ height: 320px; width: 100%; margin: 0 auto;}#donate_guide img{ height: 300px; height: 300px;} btn_reward.gif直接拿我的，然后放在img目录吧。最后修改layout\\common\\article.ejs，添加如下：1234567&lt;footer class=\"article-footer\"&gt; &lt;% if (!index &amp;&amp; theme.donate){ %&gt; &lt;%- partial('donate') %&gt; &lt;% } %&gt; &lt;%- partial('share/index', { post: post }) %&gt; &lt;%- partial('comment/counter', { post: post }) %&gt;&lt;/footer&gt; 需要在加入icarus/css/style.styl计入@import ‘_partial/donate’ 添加评论这个主题的评论已经集成了，并不需要我们进行手工导入，要求你有多说或者disqus的账号。1234comment: disqus: # enter disqus shortname here duoshuo: # enter duoshuo shortname here youyan: # enter youyan uid here disqus是国外的，不适用。我们选择duoshuo。在多说注册账号：http://duoshuo.com/创建站点后在主题目录的_config.yml中填写shortname： duoshuo: 多说设置的名称 分享主题已经集成，只需要配置即可。启用jiathis，如下12# Share 分享share: jiathis # options: jiathis, bdshare, addtoany, default 百度/谷歌验证站点为什么要验证站点了，因为要搜索引擎进行收录，说白了就是让别人更容易搜索到你的网站，仅此而已。首先需要到百度/谷歌站长统计中注册，以及验证：百度站长工具Google网站管理员工具地址注册完后，进行输入相应的网站地址，然后选择html验证，将代码加入以下路径layout/_partial/head.ejs：12345&lt;head&gt; &lt;meta name=\"baidu-site-verification\" content=\"xxxx\" /&gt; &lt;meta name=\"google-site-verification\" content=\"xxxx\" /&gt; &lt;meta charset=\"utf-8\"&gt; .... 然后发布到github中，再进行验证即可。 添加站长统计我们通过站长统计来及时查看我们个人网站的浏览情况。我寻找cnzz，需要先注册：站长统计1、在theme的_config.yml中的末尾添加以下：12# CNZZ web_idcnzz: CNZZ的web_id 2、在主题目录的layout/common添加文件为cnzz.ejs，内容如下：123&lt;% if (theme.cnzz){ %&gt;Analyse with &lt;script src=\"http://s23.cnzz.com/z_stat.php?id=&lt;%= theme.cnzz %&gt;&amp;web_id=&lt;%= theme.cnzz %&gt;\" language=\"JavaScript\"&gt;&lt;/script&gt;&lt;% } %&gt; 如需其他形式的，请参数CNZZ上的代码。3、最后进行显示，在路径layout/common/footer.ejs里面添加1...PPOffice&lt;/a&gt;.&lt;%- partial('cnzz') %&gt; 提醒：注意在_config.xml中添加web_id， 自定义widget在layout/widget 自定义模板即可，例如：1234567891011121314151617&lt;% if (site.tags.length) { %&gt;&lt;div class=\"card widget\"&gt; &lt;div class=\"card-content\"&gt; &lt;h3 class=\"menu-label\"&gt; &lt;%= __('widget.announcement') %&gt; &lt;/h3&gt; &lt;p class=\"board\"&gt; 欢迎来访问！&lt;br&gt; QQ：150045153&lt;br&gt; 微信：150045153&lt;br&gt; 邮箱：hmiter@sina.com&lt;br&gt;&lt;br&gt; 欢迎交流与分享经验! &lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;% } %&gt; categories和tags页面不显示解决办法默认是没有 categories 和 tags 的需要12hexo new page \"tags\" hexo new page \"categories\" 编辑 /tags/index.md /categories/index.md12345type: \"tags\"layout: \"tags\"type: \"categories\"layout: \"categories\"","link":"/2017/02/06/hexo/Hexo博客（二）更换主题和相关设置/"},{"title":"JavaSE源码分析-ArrayList源码剖析","text":"ArrayList概述ArrayList是实现List接口的动态数组，所谓动态就是它的大小是可变的。实现了所有可选列表操作，并允许包括 null 在内的所有元素。除了实现 List 接口外，此类还提供一些方法来操作内部用来存储列表的数组的大小。 每个ArrayList实例都有一个容量，该容量是指用来存储列表元素的数组的大小。默认初始容量为10。随着ArrayList中元素的增加，它的容量也会不断的自动增长。在每次添加新的元素时，ArrayList都会检查是否需要进行扩容操作，扩容操作带来数据向新数组的重新拷贝，所以如果我们知道具体业务数据量，在构造ArrayList时可以给ArrayList指定一个初始容量，这样就会减少扩容时数据的拷贝问题。当然在添加大量元素前，应用程序也可以使用ensureCapacity操作来增加ArrayList实例的容量，这可以减少递增式再分配的数量。 注意，ArrayList实现不是同步的。如果多个线程同时访问一个ArrayList实例，而其中至少一个线程从结构上修改了列表，那么它必须保持外部同步。所以为了保证同步，最好的办法是在创建时完成，以防止意外对列表进行不同步的访问：1List list = Collections.synchronizedList(new ArrayList(...)); 类定义ArrayList我们使用的实在是太多了，非常熟悉，所以在这里将不介绍它的使用方法。ArrayList是实现List接口的，底层采用数组实现，所以它的操作基本上都是基于对数组的操作。12345public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable,Serializable { } 变量12345678// 序列版本号private static final long serialVersionUID = 8683452581122892189L;// ArrayList基于该数组实现，用该数组保存数据private transient Object[] elementData;// ArrayList中实际数据的数量private int size; transient 为java关键字，为变量修饰符，如果用transient声明一个实例变量，当对象存储时，它的值不需要维持。Java的serialization提供了一种持久化对象实例的机制。当持久化对象时，可能有一个特殊的对象数据成员，我们不想用serialization机制来保存它。为了在一个特定对象的一个域上关闭serialization，可以在这个域前加上关键字transient。当一个对象被序列化的时候，transient型变量的值不包括在序列化的表示中，然而非transient型的变量是被包括进去的。这里Object[] elementData，就是我们的ArrayList容器，下面介绍的基本操作都是基于该elementData变量来进行操作的。 函数构造函数ArrayList提供了三个构造函数：ArrayList()：默认构造函数，提供初始容量为10的空列表。ArrayList(int initialCapacity)：构造一个具有指定初始容量的空列表。ArrayList(Collection&lt;? extends E&gt; c)：构造一个包含指定 collection 的元素的列表，这些元素是按照该 collection 的迭代器返回它们的顺序排列的。 12345678910111213141516171819202122232425262728/** * 构造一个初始容量为 10 的空列表 */public ArrayList() { this(10);}/** * 构造一个具有指定初始容量的空列表。 */public ArrayList(int initialCapacity) { super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal Capacity: \" + initialCapacity); this.elementData = new Object[initialCapacity];}/** * 构造一个包含指定 collection 的元素的列表，这些元素是按照该 collection 的迭代器返回它们的顺序排列的。 */public ArrayList(Collection&lt;? extends E&gt; c) { elementData = c.toArray(); size = elementData.length; // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class);} 新增ArrayList提供了add(E e)、add(int index, E element)、addAll(Collection&lt;? extends E&gt; c)、addAll(int index, Collection&lt;? extends E&gt; c)、set(int index, E element)这个五个方法来实现ArrayList增加。 add(E e)：将指定的元素添加到此列表的尾部。12345public boolean add(E e) { ensureCapacity(size + 1); // Increments modCount!! elementData[size++] = e; return true;} 这里ensureCapacity()方法是对ArrayList集合进行扩容操作，elementData(size++) = e，将列表末尾元素指向e。 add(int index, E element)：将指定的元素插入此列表中的指定位置。123456789101112131415161718public void add(int index, E element) { //判断索引位置是否正确 if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException( \"Index: \"+index+\", Size: \"+size); //扩容检测 ensureCapacity(size+1); /* * 对源数组进行复制处理（位移），从index + 1到size-index。 * 主要目的就是空出index位置供数据插入， * 即向右移动当前位于该位置的元素以及所有后续元素。 */ System.arraycopy(elementData, index, elementData, index + 1, size - index); //在指定位置赋值 elementData[index] = element; size++;} 在这个方法中最根本的方法就是System.arraycopy()方法，该方法的根本目的就是将index位置空出来以供新数据插入，这里需要进行数组数据的右移，这是非常麻烦和耗时的，所以如果指定的数据集合需要进行大量插入（中间插入）操作，推荐使用LinkedList。 addAll(Collection&lt;? extends E&gt; c)：按照指定 collection 的迭代器所返回的元素顺序，将该 collection 中的所有元素添加到此列表的尾部。 12345678910public boolean addAll(Collection&lt;? extends E&gt; c) { // 将集合C转换成数组 Object[] a = c.toArray(); int numNew = a.length; // 扩容处理，大小为size + numNew ensureCapacity(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0;} 这个方法无非就是使用System.arraycopy()方法将C集合(先准换为数组)里面的数据复制到elementData数组中。这里就稍微介绍下System.arraycopy()，因为下面还将大量用到该方法。该方法的原型为：public static void arraycopy(Object src, int srcPos,Object dest, int destPos, int length)。它的根本目的就是进行数组元素的复制。即从指定源数组中复制一个数组，复制从指定的位置开始，到目标数组的指定位置结束。将源数组src从srcPos位置开始复制到dest数组中，复制长度为length，数据从dest的destPos位置开始粘贴。 addAll(int index, Collection&lt;? extends E&gt; c)：从指定的位置开始，将指定 collection 中的所有元素插入到此列表中。12345678910111213141516171819202122public boolean addAll(int index, Collection&lt;? extends E&gt; c) { //判断位置是否正确 if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(\"Index: \" + index + \", Size: \" + size); //转换成数组 Object[] a = c.toArray(); int numNew = a.length; //ArrayList容器扩容处理 ensureCapacity(size + numNew); // Increments modCount //ArrayList容器数组向右移动的位置 int numMoved = size - index; //如果移动位置大于0，则将ArrayList容器的数据向右移动numMoved个位置，确保增加的数据能够增加 if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); //添加数组 System.arraycopy(a, 0, elementData, index, numNew); //容器容量变大 size += numNew; return numNew != 0;} set(int index, E element)：用指定的元素替代此列表中指定位置上的元素。123456789public E set(int index, E element) { //检测插入的位置是否越界 RangeCheck(index); E oldValue = (E) elementData[index]; //替代 elementData[index] = element; return oldValue; } 删除ArrayList提供了remove(int index)、remove(Object o)、removeRange(int fromIndex, int toIndex)、removeAll()四个方法进行元素的删除。 remove(int index)：移除此列表中指定位置上的元素。123456789101112131415161718public E remove(int index) { //位置验证 RangeCheck(index); modCount++; //需要删除的元素 E oldValue = (E) elementData[index]; //向左移的位数 int numMoved = size - index - 1; //若需要移动，则想左移动numMoved位 if (numMoved &gt; 0) System.arraycopy(elementData, index + 1, elementData, index, numMoved); //置空最后一个元素 elementData[--size] = null; // Let gc do its work return oldValue; } remove(Object o)：移除此列表中首次出现的指定元素（如果存在）。123456789101112131415161718public boolean remove(Object o) { //因为ArrayList中允许存在null，所以需要进行null判断 if (o == null) { for (int index = 0; index &lt; size; index++) if (elementData[index] == null) { //移除这个位置的元素 fastRemove(index); return true; } } else { for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } 其中fastRemove()方法用于移除指定位置的元素。如下12345678private void fastRemove(int index) { modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // Let gc do its work } removeRange(int fromIndex, int toIndex)：移除列表中索引在 fromIndex（包括）和 toIndex（不包括）之间的所有元素。123456789101112protected void removeRange(int fromIndex, int toIndex) { modCount++; int numMoved = size - toIndex; System .arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // Let gc do its work int newSize = size - (toIndex - fromIndex); while (size != newSize) elementData[--size] = null; } removeAll()：是继承自AbstractCollection的方法，ArrayList本身并没有提供实现。1234567891011public boolean removeAll(Collection&lt;?&gt; c) { boolean modified = false; Iterator&lt;?&gt; e = iterator(); while (e.hasNext()) { if (c.contains(e.next())) { e.remove(); modified = true; } } return modified; } 查找ArrayList提供了get(int index)用读取ArrayList中的元素。由于ArrayList是动态数组，所以我们完全可以根据下标来获取ArrayList中的元素，而且速度还比较快，故ArrayList长于随机访问。12345public E get(int index) { RangeCheck(index); return (E) elementData[index];} 扩容在上面的新增方法的源码中我们发现每个方法中都存在这个方法：ensureCapacity()，该方法就是ArrayList的扩容方法。在前面就提过ArrayList每次新增元素时都会需要进行容量检测判断，若新增元素后元素的个数会超过ArrayList的容量，就会进行扩容操作来满足新增元素的需求。所以当我们清楚知道业务数据量或者需要插入大量元素前，我可以使用ensureCapacity来手动增加ArrayList实例的容量，以减少递增式再分配的数量。123456789101112131415161718public void ensureCapacity(int minCapacity) { //修改计时器 modCount++; //ArrayList容量大小 int oldCapacity = elementData.length; /* * 若当前需要的长度大于当前数组的长度时，进行扩容操作 */ if (minCapacity &gt; oldCapacity) { Object oldData[] = elementData; //计算新的容量大小，为当前容量的1.5倍 int newCapacity = (oldCapacity * 3) / 2 + 1; if (newCapacity &lt; minCapacity) newCapacity = minCapacity; //数组拷贝，生成新的数组 elementData = Arrays.copyOf(elementData, newCapacity); }} 在这里有一个疑问，为什么每次扩容处理会是1.5倍，而不是2.5、3、4倍呢？通过google查找，发现1.5倍的扩容是最好的倍数。因为一次性扩容太大(例如2.5倍)可能会浪费更多的内存(1.5倍最多浪费33%，而2.5被最多会浪费60%，3.5倍则会浪费71%……)。但是一次性扩容太小，需要多次对数组重新分配内存，对性能消耗比较严重。所以1.5倍刚刚好，既能满足性能需求，也不会造成很大的内存消耗。 处理这个ensureCapacity()这个扩容数组外，ArrayList还给我们提供了将底层数组的容量调整为当前列表保存的实际元素的大小的功能。它可以通过trimToSize()方法来实现。该方法可以最小化ArrayList实例的存储量。 1234567public void trimToSize() { modCount++; int oldCapacity = elementData.length; if (size &lt; oldCapacity) { elementData = Arrays.copyOf(elementData, size); }} subList1234public List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex);} subListRangeCheck方式是判断fromIndex、toIndex是否合法，如果合法就直接返回一个subList对象，注意在产生该new该对象的时候传递了一个参数 this ，该参数非常重要，因为他代表着原始list。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * 继承AbstractList类，实现RandomAccess接口 */private class SubList extends AbstractList&lt;E&gt; implements RandomAccess { private final AbstractList&lt;E&gt; parent; //列表 private final int parentOffset; private final int offset; int size; //构造函数 SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) { this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; this.modCount = ArrayList.this.modCount; } //set方法 public E set(int index, E e) { rangeCheck(index); checkForComodification(); E oldValue = ArrayList.this.elementData(offset + index); ArrayList.this.elementData[offset + index] = e; return oldValue; } //get方法 public E get(int index) { rangeCheck(index); checkForComodification(); return ArrayList.this.elementData(offset + index); } //add方法 public void add(int index, E e) { rangeCheckForAdd(index); checkForComodification(); parent.add(parentOffset + index, e); this.modCount = parent.modCount; this.size++; } //remove方法 public E remove(int index) { rangeCheck(index); checkForComodification(); E result = parent.remove(parentOffset + index); this.modCount = parent.modCount; this.size--; return result; }} 该SubLsit是ArrayList的内部类，它与ArrayList一样，都是继承AbstractList和实现RandomAccess接口。同时也提供了get、set、add、remove等list常用的方法。但是它的构造函数有点特殊，在该构造函数中有两个地方需要注意： 1、this.parent = parent;而parent就是在前面传递过来的list，也就是说this.parent就是原始list的引用。 2、this.offset = offset + fromIndex;this.parentOffset = fromIndex;。同时在构造函数中它甚至将modCount（fail-fast机制）传递过来了。 我们再看get方法，在get方法中return ArrayList.this.elementData(offset + index);这段代码可以清晰表明get所返回就是原列表offset + index位置的元素。同样的道理还有add方法里面的：12parent.add(parentOffset + index, e);this.modCount = parent.modCount; remove方法里面的12E result = parent.remove(parentOffset + index);this.modCount = parent.modCount; 诚然，到了这里我们可以判断subList返回的SubList同样也是AbstractList的子类，同时它的方法如get、set、add、remove等都是在原列表上面做操作，它并没有像subString一样生成一个新的对象。所以subList返回的只是原列表的一个视图，它所有的操作最终都会作用在原列表上。字 因此，当我们使用子集合进行元素的修改操作时，会影响原有的list集合。所以在使用subList方法时，一定要想清楚，是否需要对子集合进行修改元素而不影响原有的list集合。如果需要对子集合的元素进行修改操作而不需要影响原集合时，我们可以使用以下方法进行处理：1List&lt;Object&gt; tempList = new ArrayList&lt;Object&gt;(lists.subList(2, lists.size())); Fail-Fast机制动机： 在 Java Collection 中，为了防止在某个线程在对 Collection 进行迭代时，其他线程对该 Collection 进行结构上的修改。换句话说，迭代器的快速失败行为仅用于检测代码的 bug。 本质： Fail-Fast 是 Java 集合的一种错误检测机制。 作用场景： 在使用迭代器时，Collection 的结构发生变化，抛出 ConcurrentModificationException 。当然，这并不能说明 Collection对象 已经被不同线程并发修改，因为如果单线程违反了规则，同样也有会抛出该异常。 当多个线程对集合进行结构上的改变的操作时，有可能会产生fail-fast机制。例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会触发fail-fast机制，抛出 ConcurrentModificationException 异常。在面对并发的修改时，迭代器很快就会完全失败，而不是冒着在将来某个不确定时间发生任意不确定行为的风险。 我们知道 fail-fast 产生的原因就在于：程序在对 collection 进行迭代时，某个线程对该 collection 在结构上对其做了修改。要想进一步了解 fail-fast 机制，我们首先要对 ConcurrentModificationException 异常有所了解。当方法检测到对象的并发修改，但不允许这种修改时就抛出该异常。同时需要注意的是，该异常不会始终指出对象已经由不同线程并发修改，如果单线程违反了规则，同样也有可能会抛出改异常。诚然，迭代器的快速失败行为无法得到保证，它不能保证一定会出现该错误，但是快速失败操作会尽最大努力抛出 ConcurrentModificationException 异常，所以，为提高此类操作的正确性而编写一个依赖于此异常的程序是错误的做法，正确做法是：ConcurrentModificationException 应该仅用于检测 bug。 下面我们以 ArrayList 为例进一步分析 fail-fast 产生的原因:123456789101112131415161718192021222324252627private class Itr implements Iterator&lt;E&gt; { int cursor; int lastRet = -1; int expectedModCount = ArrayList.this.modCount; public boolean hasNext() { return (this.cursor != ArrayList.this.size); } public E next() { checkForComodification(); /** 省略此处代码 */ } public void remove() { if (this.lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); /** 省略此处代码 */ } final void checkForComodification() { if (ArrayList.this.modCount == this.expectedModCount) return; throw new ConcurrentModificationException(); } } 从上面的源代码我们可以看出，迭代器在调用 next() 、 remove() 方法时都是调用 checkForComodification() 方法，该方法用于判断 “modCount == expectedModCount”：若不等，触发 fail-fast 机制，抛出 ConcurrentModificationException 异常。所以，要弄清楚为什么会产生 fail-fast 机制，我们就必须要弄明白 “modCount != expectedModCount” 什么时候发生，换句话说，他们的值在什么时候发生改变的。 expectedModCount 是在 Itr 中定义的：“int expectedModCount = ArrayList.this.modCount;”，所以它的值是不可能会修改的，所以会变的就是 modCount。modCount 是在 AbstractList 中定义的，为全局变量：1protected transient int modCount = 0; 从 ArrayList 源码中我们可以看出，我们直接或间接的通过 RemoveRange 、 trimToSize 和 ensureCapcity（add，remove，clear） 三个方法完成对 ArrayList 结构上的修改,所以 ArrayList 实例每当调用一次上面的方法，modCount 的值就递增一次。所以,我们这里可以判断:由于expectedModCount 的值与 modCount 的改变不同步，导致两者之间不等，从而触发fail-fast机制。我们可以考虑如下场景： 有两个线程（线程A，线程B），其中线程A负责遍历list、线程B修改list。线程A在遍历list过程的某个时候（此时expectedModCount = modCount=N），线程启动，同时线程B增加一个元素，这是modCount的值发生改变（modCount + 1 = N + 1）。线程A继续遍历执行next方法时，通告checkForComodification方法发现expectedModCount = N ，而modCount = N + 1，两者不等，这时触发 fail-fast机制。","link":"/2018/11/27/javase/ArrayList源码剖析/"},{"title":"JavaSE源码分析-HashMap源码剖析","text":"HashMap简介HashMap是基于哈希表实现的，每一个元素是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。HashMap 实现了Serializable接口，因此它支持序列化，实现了Cloneable接口，能被克隆。 哈希的相关概念Hash 就是把任意长度的输入(又叫做预映射， pre-image)，通过哈希算法，变换成固定长度的输出(通常是整型)，该输出就是哈希值。这种转换是一种压缩映射，也就是说，散列值的空间通常远小于输入的空间。不同的输入可能会散列成相同的输出，从而不可能从散列值来唯一的确定输入值。简单的说，就是一种将任意长度的消息压缩到某一固定长度的息摘要函数。 哈希的应用：数据结构我们知道，数组的特点是：寻址容易，插入和删除困难；而链表的特点是：寻址困难，插入和删除容易。那么我们能不能综合两者的特性，做出一种寻址容易，插入和删除也容易的数据结构呢？答案是肯定的，这就是我们要提起的哈希表。事实上，哈希表有多种不同的实现方法，我们接下来解释的是最经典的一种方法 ——拉链法，我们可以将其理解为链表的数组，如下图所示：我们可以从上图看到，左边很明显是个数组，数组的每个成员是一个链表。该数据结构所容纳的所有元素均包含一个指针，用于元素间的链接。我们根据元素的自身特征把元素分配到不同的链表中去，反过来我们也正是通过这些特征找到正确的链表，再从链表中找出正确的元素。其中，根据元素特征计算元素数组下标的方法就是 哈希算法。 总的来说，哈希表适合用作快速查找、删除的基本数据结构，通常需要总数据量可以放入内存。在使用哈希表时，有以下几个关键点： hash 函数（哈希算法）的选择：针对不同的对象(字符串、整数等)具体的哈希方法； 碰撞处理：常用的有两种方式，一种是open hashing，即 &gt;拉链法；另一种就是 closed hashing，即开地址法(opened addressing)。 存储结构1、首先要清楚HashMap的存储结构，如下图所示：我们知道，在Java中最常用的两种结构是 数组 和 链表，几乎所有的数据结构都可以利用这两种来组合实现，HashMap 就是这种应用的一个典型。 图中，紫色部分即为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 类定义HashMap实现了Map接口，并继承 AbstractMap 抽象类，其中 Map 接口定义了键值映射规则。和 AbstractCollection抽象类在 Collection 族的作用类似， AbstractMap 抽象类提供了 Map 接口的骨干实现，以最大限度地减少实现Map接口所需的工作。1234public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable{} 属性123456789101112131415161718192021222324// 默认的初始容量（容量为HashMap中槽的数目）是16，且实际容量必须是2的整数次幂。 static final int DEFAULT_INITIAL_CAPACITY = 16; // 最大容量（必须是2的幂且小于2的30次方，传入容量过大将被这个值替换） static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 存储数据的Entry数组，长度是2的幂。 // HashMap采用链表法解决冲突，每一个Entry本质上是一个单向链表 transient Entry[] table; // HashMap的底层数组中已用槽的数量 transient int size; // HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*加载因子） int threshold; // 加载因子实际大小 final float loadFactor; // HashMap被改变的次数 transient volatile int modCount; 方法构造方法HashMap 一共提供了四个构造函数，其中 默认无参的构造函数 和 参数为Map的构造函数 为 Java Collection Framework 规范的推荐实现，其余两个构造函数则是 HashMap 专门提供的。1234567891011121314151617/** * Constructs an empty HashMap with the default initial capacity * (16) and the default load factor (0.75). */ public HashMap() { //负载因子:用于衡量的是一个散列表的空间的使用程度 this.loadFactor = DEFAULT_LOAD_FACTOR; //HashMap进行扩容的阈值，它的值等于 HashMap 的容量乘以负载因子 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // HashMap的底层实现仍是数组，只是数组的每一项都是一条链 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); } 该构造函数意在构造一个具有&gt; 默认初始容量 (16) 和 默认负载因子(0.75) 的空 HashMap。 从上述源码中我们可以看出，每次新建一个HashMap时，都会初始化一个Entry类型的table数组。12345678910111213141516171819static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final K key; // 键值对的键 V value; // 键值对的值 Entry&lt;K,V&gt; next; // 下一个节点 final int hash; // hash(key.hashCode())方法的返回值 /** * Creates new entry. */ Entry(int h, K k, V v, Entry&lt;K,V&gt; n) { // Entry 的构造函数 value = v; next = n; key = k; hash = h; } ......} 其中，Entry为HashMap的内部类，实现了 Map.Entry 接口，其包含了键key、值value、下一个节点next，以及hash值四个属性。事实上，Entry 是构成哈希表的基石，是哈希表所存储的元素的具体形式。、 1234567891011121314151617181920212223242526272829 public HashMap(int initialCapacity, float loadFactor) { //初始容量不能小于 0 if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal initial capacity: \" + initialCapacity); //初始容量不能超过 2^30 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //负载因子不能小于 0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal load factor: \" + loadFactor); // HashMap 的容量必须是2的幂次方，超过 initialCapacity 的最小 2^n int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; //负载因子 this.loadFactor = loadFactor; //设置HashMap的容量极限，当HashMap的容量达到该极限时就会进行自动扩容操作 threshold = (int)(capacity * loadFactor); // HashMap的底层实现仍是数组，只是数组的每一项都是一条链 table = new Entry[capacity]; init();} 该构造函数意在构造一个 指定初始容量 和 指定负载因子的空 HashMap 123public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); // 直接调用上述构造函数} 该构造函数意在构造一个指定初始容量和默认负载因子 (0.75)的空 HashMap 1234567public HashMap(Map&lt;? extends K, ? extends V&gt; m) { // 初始容量不小于 16 this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); putAllForCreate(m); } 该构造函数意在构造一个与指定 Map 具有相同映射的 HashMap，其 初始容量不小于 16 (具体依赖于指定Map的大小)，负载因子是 0.75 在这里，我们提到了两个非常重要的参数：初始容量 和 负载因子，这两个参数是影响HashMap性能的重要参数。其中，容量表示哈希表中桶的数量 (table 数组的大小)，初始容量是创建哈希表时桶的数量；负载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，它衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小。 对于使用 拉链法的哈希表来说，查找一个元素的平均时间是 O(1+a)，a 指的是链的长度，是一个常数。特别地，若负载因子越大，那么对空间的利用更充分，但查找效率的也就越低；若负载因子越小，那么哈希表的数据将越稀疏，对空间造成的浪费也就越严重。系统默认负载因子为 0.75，这是时间和空间成本上一种折衷，一般情况下我们是无需修改的。 构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16），加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。下面说下加载因子，如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。另外，无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方 快速存储在HashMap中，我们最常用的两个操作就是：put(Key,Value) 和 get(Key)。我们都知道，HashMap中的Key是唯一的，那它是如何保证唯一性的呢？我们首先想到的是用equals比较，没错，这样可以实现，但随着元素的增多，put 和 get 的效率将越来越低，这里的时间复杂度是O(n)。也就是说，假如 HashMap 有1000个元素，那么 put时就需要比较 1000 次，这是相当耗时的，远达不到HashMap快速存取的目的。实际上，HashMap 很少会用到equals方法，因为其内通过一个哈希表管理所有元素，利用哈希算法可以快速的存取元素。当我们调用put方法存值时，HashMap首先会调用Key的hashCode方法，然后基于此获取Key哈希码，通过哈希码快速找到某个桶，这个位置可以被称之为 bucketIndex。可以知道，如果两个对象的hashCode不同，那么equals一定为 false；否则，如果其hashCode相同，equals也不一定为 true。所以，理论上，hashCode 可能存在碰撞的情况，当碰撞发生时，这时会取出bucketIndex桶内已存储的元素，并通过hashCode() 和 equals() 来逐个比较以判断Key是否已存在。如果已存在，则使用新Value值替换旧Value值，并返回旧Value值；如果不存在，则存放新的键值对到桶中。因此，在 HashMap中，equals() 方法只有在哈希码碰撞时才会被用到。 在 HashMap 中，键值对的存储是通过 put(key,vlaue) 方法来实现的，其源码如下：1234567891011121314151617181920212223242526272829public V put(K key, V value) { //当key为null时，调用putForNullKey方法，并将该键值对保存到table的第一个位置 if (key == null) return putForNullKey(value); //根据key的hashCode计算hash值 int hash = hash(key.hashCode()); // ------- (1) //计算该键值对在数组中的存储位置（哪个桶） int i = indexFor(hash, table.length); // ------- (2) //在table的第i个桶上进行迭代，寻找 key 保存的位置 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) { // ------- (3) Object k; //判断该条链上是否存在hash值相同且key值相等的映射，若存在，则直接覆盖 value，并返回旧value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; // 返回旧值 } } modCount++; //修改次数增加1，快速失败机制 //原HashMap中无该映射，将该添加至该链的链头 addEntry(hash, key, value, i); return null;} 通过上述源码我们可以清楚了解到HashMap保存数据的过程。首先，判断key是否为null，若为null，则直接调用putForNullKey方法；1234567891011121314private V putForNullKey(V value) { // 若key==null，则将其放入table的第一个桶，即 table[0] for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) { // 若已经存在key为null的键，则替换其值，并返回旧值 V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } } modCount++; // 快速失败 addEntry(0, null, value, 0); // 否则，将其添加到 table[0] 的桶中 return null;} 通过上述源码我们可以清楚知到，HashMap 中可以保存键为NULL的键值对，且该键值对是唯一的。若再次向其中添加键为NULL的键值对，将覆盖其原值。此外，如果HashMap中存在键为NULL的键值对，那么一定在第一个桶中。 如果key不为null，则同样先求出key的hash值，根据hash值得出在table中的索引，而后遍历对应的单链表，如果单链表中存在与目标key相等的键值对，则将新的value覆盖旧的value，比将旧的value返回，如果找不到与目标key相等的键值对，或者该单链表为空，则将该键值对插入到改单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有addEntry方法实现的，它的源码如下：1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) { // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); } 注意这里倒数第三行的构造方法，将key-value键值对赋给table[bucketIndex]，并将其next指向元素e，这便将key-value放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次put键值对的时候，总是将新的该键值对放在table[bucketIndex]处（即头结点处）。两外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。 再看源码中的 (3) 处，此处迭代原因就是为了防止存在相同的key值。如果发现两个hash值（key）相同时，HashMap的处理方式是用新value替换旧value，这里并没有处理key，这正好解释了 HashMap 中没有两个相同的 key。 读取实现相对于HashMap的存储而言，读取就显得比较简单了。因为，HashMap只需通过key的hash值定位到table数组的某个特定的桶，然后查找并返回该key对应的value即可，源码如下：12345678910111213141516171819202122232425262728// 获取key对应的value public V get(Object key) { if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) { Object k; //判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; } //没找到则返回null return null; } // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() { for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) { if (e.key == null) return e.value; } return null; } 首先，如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中，当然不一定是存放在头结点table[0]中。如果key不为null，则先求的key的hash值，根据hash值找到在table中的索引，在该索引对应的单链表中查找是否有键值对的key与目标key相等，有就返回对应的value，没有则返回null。 关于扩容上面我们看到了扩容的方法，resize方法，它的源码如下：12345678910111213141516// 重新调整HashMap的大小，newCapacity是调整后的单位 void resize(int newCapacity) { Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return; } // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); } 很明显，是新建了一个HashMap的底层数组，而后调用transfer方法，将就HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。transfer方法的源码如下： 123456789101112131415161718// 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) { src[j] = null; do { Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; } while (e != null); } } } 很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap的时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。 hash算法我们重点来分析下求hash值和索引值的方法，这两个方法便是HashMap设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。计算哈希值的方法如下：1234static int hash(int h) { h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); } 它只是一个数学公式，IDK这样设计对hash值的计算，自然有它的好处，至于为什么这样设计，我们这里不去追究，只要明白一点，用的位的操作使hash值的计算效率很高。 由hash值找到对应索引的方法如下：123static int indexFor(int h, int length) { return h &amp; (length-1); } 这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法），Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&amp;(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&amp;(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&amp;(length-1)的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&amp;(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。 contains方法注意containsKey方法和containsValue方法。前者直接可以通过key的哈希值将搜索范围定位到指定索引对应的链表，而后者要对哈希数组的每个链表进行搜索。","link":"/2018/11/23/javase/HashMap源码剖析/"},{"title":"JavaSE源码分析-ConcurrentHashMap源码剖析","text":"ConcurrentHashMap简介因为多线程环境下，使用Hashmap进行put操作会引起死循环，导致CPU利用率接近100%，所以在并发情况下不能使用HashMap。 HashTable容器使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。 ConcurrentHashMap本质上是一个Segment数组，而一个Segment实例又包含若干个桶，每个桶中都包含一条由若干个 HashEntry 对象链接起来的链表。总的来说，ConcurrentHashMap的高效并发机制是通过以下三方面来保证的(具体细节见后文阐述)： 通过锁分段技术保证并发环境下的写操作； 通过 HashEntry的不变性、Volatile变量的内存可见性和加锁重读机制保证高效、安全的读操作； 通过不加锁和加锁两种方案控制跨段操作的的安全性。 ConcurrentHashMap类中包含两个静态内部类 HashEntry 和 Segment，其中 HashEntry 用来封装具体的K/V对，是个典型的四元组；Segment 用来充当锁的角色，每个 Segment 对象守护整个ConcurrentHashMap的若干个桶 (可以把Segment看作是一个小型的哈希表)，其中每个桶是由若干个 HashEntry 对象链接起来的链表。总的来说，一个ConcurrentHashMap实例中包含由若干个Segment实例组成的数组，而一个Segment实例又包含由若干个桶，每个桶中都包含一条由若干个 HashEntry 对象链接起来的链表。特别地，ConcurrentHashMap 在默认并发级别下会创建16个Segment对象的数组，如果键能均匀散列，每个 Segment 大约守护整个散列表中桶总数的 1/16。 类定义ConcurrentHashMap 继承了AbstractMap并实现了ConcurrentMap接口，12345public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable { ...} 成员变量定义与HashMap相比，ConcurrentHashMap 增加了两个属性用于定位段，分别是 segmentMask 和 segmentShift。此外，不同于HashMap的是，ConcurrentHashMap底层结构是一个Segment数组，而不是Object数组，具体源码如下：123456789101112131415/** * Mask value for indexing into segments. The upper bits of a * key's hash code are used to choose the segment. */ final int segmentMask; // 用于定位段，大小等于segments数组的大小减 1，是不可变的 /** * Shift value for indexing within segments. */ final int segmentShift; // 用于定位段，大小等于32(hash值的位数)减去对segments的大小取以2为底的对数值，是不可变的 /** * The segments, each of which is a specialized hash table */ final Segment&lt;K,V&gt;[] segments; // ConcurrentHashMap的底层结构是一个Segment数组 ConcurrentHashMap 的数据结构本质上，ConcurrentHashMap就是一个Segment数组，而一个Segment实例则是一个小的哈希表。由于Segment类继承于ReentrantLock类，从而使得Segment对象能充当锁的角色，这样，每个 Segment对象就可以守护整个ConcurrentHashMap的若干个桶，其中每个桶是由若干个HashEntry 对象链接起来的链表。通过使用段(Segment)将ConcurrentHashMap划分为不同的部分，ConcurrentHashMap就可以使用不同的锁来控制对哈希表的不同部分的修改，从而允许多个修改操作并发进行, 这正是ConcurrentHashMap锁分段技术的核心内涵。进一步地，如果把整个ConcurrentHashMap看作是一个父哈希表的话，那么每个Segment就可以看作是一个子哈希表，如下图所示：注意，假设ConcurrentHashMap一共分为2^n个段，每个段中有2^m个桶，那么段的定位方式是将key的hash值的高n位与(2^n-1)相与。在定位到某个段后，再将key的hash值的低m位与(2^m-1)相与，定位到具体的桶位。 段的定义：SegmentSegment 类继承于 ReentrantLock 类，从而使得 Segment 对象能充当锁的角色。每个 Segment 对象用来守护它的成员对象 table 中包含的若干个桶。table 是一个由 HashEntry 对象组成的链表数组，table 数组的每一个数组成员就是一个桶。在Segment类中，count 变量是一个计数器，它表示每个 Segment 对象管理的 table 数组包含的 HashEntry 对象的个数，也就是 Segment 中包含的 HashEntry 对象的总数。特别需要注意的是，之所以在每个 Segment 对象中包含一个计数器，而不是在 ConcurrentHashMap 中使用全局的计数器，是对 ConcurrentHashMap 并发性的考虑：因为这样当需要更新计数器时，不用锁定整个ConcurrentHashMap。事实上，每次对段进行结构上的改变，如在段中进行增加/删除节点(修改节点的值不算结构上的改变)，都要更新count的值，此外，在JDK的实现中每次读取操作开始都要先读取count的值。特别需要注意的是，count是volatile的，这使得对count的任何更新对其它线程都是立即可见的。modCount用于统计段结构改变的次数，主要是为了检测对多个段进行遍历过程中某个段是否发生改变，这一点具体在谈到跨段操作时会详述。threashold用来表示段需要进行重哈希的阈值。loadFactor表示段的负载因子，其值等同于ConcurrentHashMap的负载因子的值。table是一个典型的链表数组，而且也是volatile的，这使得对table的任何更新对其它线程也都是立即可见的。段(Segment)的定义如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * Segments are specialized versions of hash tables. This * subclasses from ReentrantLock opportunistically, just to * simplify some locking and avoid separate construction. */ static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable { /** * The number of elements in this segment's region. */ transient volatile int count; // Segment中元素的数量，可见的 /** * Number of updates that alter the size of the table. This is * used during bulk-read methods to make sure they see a * consistent snapshot: If modCounts change during a traversal * of segments computing size or checking containsValue, then * we might have an inconsistent view of state so (usually) * must retry. */ transient int modCount; //对count的大小造成影响的操作的次数（比如put或者remove操作） /** * The table is rehashed when its size exceeds this threshold. * (The value of this field is always &lt;tt&gt;(int)(capacity * * loadFactor)&lt;/tt&gt;.) */ transient int threshold; // 阈值，段中元素的数量超过这个值就会对Segment进行扩容 /** * The per-segment table. */ transient volatile HashEntry&lt;K,V&gt;[] table; // 链表数组 /** * The load factor for the hash table. Even though this value * is same for all segments, it is replicated to avoid needing * links to outer object. * @serial */ final float loadFactor; // 段的负载因子，其值等同于ConcurrentHashMap的负载因子 ... }我们知道，ConcurrentHashMap允许多个修改(写)操作并发进行，其关键在于使用了锁分段技术，它使用了不同的锁来控制对哈希表的不同部分进行的修改(写)，而 ConcurrentHashMap 内部使用段(Segment)来表示这些不同的部分。实际上，每个段实质上就是一个小的哈希表，每个段都有自己的锁(Segment 类继承了 ReentrantLock 类)。这样，只要多个修改(写)操作发生在不同的段上，它们就可以并发进行。下图是依次插入 ABC 三个 HashEntry节点后，Segment 的结构示意图：## 基本元素：HashEntryHashEntry用来封装具体的键值对，是个典型的四元组。与HashMap中的Entry类似，HashEntry也包括同样的四个域，分别是key、hash、value和next。不同的是，在HashEntry类中，key，hash和next域都被声明为final的，value域被volatile所修饰，因此HashEntry对象几乎是不可变的，这是ConcurrentHashmap读操作并不需要加锁的一个重要原因。next域被声明为final本身就意味着我们不能从hash链的中间或尾部添加或删除节点，因为这需要修改next引用值，因此所有的节点的修改只能从头部开始。对于put操作，可以一律添加到Hash链的头部。但是对于remove操作，可能需要从中间删除一个节点，这就需要将要删除节点的前面所有节点整个复制(重新new)一遍，最后一个节点指向要删除结点的下一个结点(这在谈到ConcurrentHashMap的删除操作时还会详述)。特别地，由于value域被volatile修饰，所以其可以确保被读线程读到最新的值，这是ConcurrentHashmap读操作并不需要加锁的另一个重要原因。实际上，ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁。HashEntry代表hash链中的一个节点，其结构如下所示：1234567891011121314151617181920212223242526272829303132/** * ConcurrentHashMap 中的 HashEntry 类 * * ConcurrentHashMap list entry. Note that this is never exported * out as a user-visible Map.Entry. * * Because the value field is volatile, not final, it is legal wrt * the Java Memory Model for an unsynchronized reader to see null * instead of initial value when read via a data race. Although a * reordering leading to this is not likely to ever actually * occur, the Segment.readValueUnderLock method is used as a * backup in case a null (pre-initialized) value is ever seen in * an unsynchronized access method. */ static final class HashEntry&lt;K,V&gt; { final K key; // 声明 key 为 final 的 final int hash; // 声明 hash 值为 final 的 volatile V value; // 声明 value 被volatile所修饰 final HashEntry&lt;K,V&gt; next; // 声明 next 为 final 的 HashEntry(K key, int hash, HashEntry&lt;K,V&gt; next, V value) { this.key = key; this.hash = hash; this.next = next; this.value = value; } @SuppressWarnings(\"unchecked\") static final &lt;K,V&gt; HashEntry&lt;K,V&gt;[] newArray(int i) { return new HashEntry[i]; } }与HashMap类似，在ConcurrentHashMap中，如果在散列时发生碰撞，也会将碰撞的 HashEntry 对象链成一个链表。由于HashEntry的next域是final的，所以新节点只能在链表的表头处插入。下图是在一个空桶中依次插入 A，B，C 三个 HashEntry 对象后的结构图(由于只能在表头插入，所以链表中节点的顺序和插入的顺序相反)：# 方法## 构造方法ConcurrentHashMap 一共提供了五个构造函数，其中默认无参的构造函数和参数为Map的构造函数 为 Java Collection Framework 规范的推荐实现，其余三个构造函数则是 ConcurrentHashMap 专门提供的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 构造一个具有默认初始容量(16)、默认负载因子(0.75)和默认并发级别(16)的空ConcurrentHashMappublic ConcurrentHashMap() { this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);}public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) { this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL); putAll(m);}public ConcurrentHashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);}public ConcurrentHashMap(int initialCapacity, float loadFactor) { this(initialCapacity, loadFactor, DEFAULT_CONCURRENCY_LEVEL); // 默认并发级别为16}public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) { if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; // 大小为 lg(ssize) int ssize = 1; // 段的数目，segments数组的大小(2的幂次方) while (ssize &lt; concurrencyLevel) { ++sshift; ssize &lt;&lt;= 1; } segmentShift = 32 - sshift; // 用于定位段 segmentMask = ssize - 1; // 用于定位段 this.segments = Segment.newArray(ssize); // 创建segments数组 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; // 总的桶数/总的段数 if (c * ssize &lt; initialCapacity) ++c; int cap = 1; // 每个段所拥有的桶的数目(2的幂次方) while (cap &lt; c) cap &lt;&lt;= 1; for (int i = 0; i &lt; this.segments.length; ++i) // 初始化segments数组 this.segments[i] = new Segment&lt;K,V&gt;(cap, loadFactor);}在这里，我们提到了三个非常重要的参数：初始容量、负载因子 和 并发级别，这三个参数是影响ConcurrentHashMap性能的重要参数。从上述源码我们可以看出，ConcurrentHashMap 也正是通过initialCapacity、loadFactor和concurrencyLevel这三个参数进行构造并初始化segments数组、段偏移量segmentShift、段掩码segmentMask和每个segment的。## 并发写操作在ConcurrentHashMap中，线程对映射表做读操作时，一般情况下不需要加锁就可以完成，对容器做结构性修改的操作(比如，put操作、remove操作等)才需要加锁。典型结构性修改操作包括put、remove和clear，下面我们首先以put操作为例说明对ConcurrentHashMap做结构性修改的过程。ConcurrentHashMap的put操作对应的源码如下：123456public V put(K key, V value) { if (value == null) throw new NullPointerException(); int hash = hash(key.hashCode()); return segmentFor(hash).put(key, hash, value, false);}从上面的源码我们看到，ConcurrentHashMap不同于HashMap，它既不允许key值为null，也不允许value值为null。此外，我们还可以看到，实际上我们对ConcurrentHashMap的put操作被ConcurrentHashMap委托给特定的段来实现。也就是说，当我们向ConcurrentHashMap中put一个Key/Value对时，首先会获得Key的哈希值并对其再次哈希，然后根据最终的hash值定位到这条记录所应该插入的段，定位段的segmentFor()方法源码如下：123final Segment&lt;K,V&gt; segmentFor(int hash) { return segments[(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask];}segmentFor()方法根据传入的hash值向右无符号右移segmentShift位，然后和segmentMask进行与操作就可以定位到特定的段。在这里，假设Segment的数量(segments数组的长度)是2的n次方(Segment的数量总是2的倍数，具体见构造函数的实现)，那么segmentShift的值就是32-n(hash值的位数是32)，而segmentMask的值就是2^n-1（写成二进制的形式就是n个1）。进一步地，我们就可以得出以下结论：根据key的hash值的高n位就可以确定元素到底在哪一个Segment中。紧接着，调用这个段的put()方法来将目标Key/Value对插到段中，段的put()方法的源码如下所示：12345678910111213141516171819202122232425262728293031V put(K key, int hash, V value, boolean onlyIfAbsent) { lock(); // 上锁 try { int c = count; if (c++ &gt; threshold) // ensure capacity rehash(); HashEntry&lt;K,V&gt;[] tab = table; // table是Volatile的 int index = hash &amp; (tab.length - 1); // 定位到段中特定的桶 HashEntry&lt;K,V&gt; first = tab[index]; // first指向桶中链表的表头 HashEntry&lt;K,V&gt; e = first; // 检查该桶中是否存在相同key的结点 while (e != null &amp;&amp; (e.hash != hash || !key.equals(e.key))) e = e.next; V oldValue; if (e != null) { // 该桶中存在相同key的结点 oldValue = e.value; if (!onlyIfAbsent) e.value = value; // 更新value值 }else { // 该桶中不存在相同key的结点 oldValue = null; ++modCount; // 结构性修改，modCount加1 tab[index] = new HashEntry&lt;K,V&gt;(key, hash, first, value); // 创建HashEntry并将其链到表头 count = c; //write-volatile，count值的更新一定要放在最后一步(volatile变量) } return oldValue; // 返回旧值(该桶中不存在相同key的结点，则返回null) } finally { unlock(); // 在finally子句中解锁 }}从源码中首先可以知道，ConcurrentHashMap对Segment的put操作是加锁完成的。我们已经知道Segment是ReentrantLock的子类，因此Segment本身就是一种可重入的Lock，所以我们可以直接调用其继承而来的lock()方法和unlock()方法对代码进行上锁/解锁。需要注意的是，这里的加锁操作是针对某个具体的Segment，锁定的也是该Segment而不是整个ConcurrentHashMap。因为插入键/值对操作只是在这个Segment包含的某个桶中完成，不需要锁定整个ConcurrentHashMap。因此，其他写线程对另外15个Segment的加锁并不会因为当前线程对这个Segment的加锁而阻塞。故而 相比较于 HashTable 和由同步包装器包装的HashMap每次只能有一个线程执行读或写操作，ConcurrentHashMap 在并发访问性能上有了质的提高。在理想状态下，ConcurrentHashMap 可以支持 16 个线程执行并发写操作（如果并发级别设置为 16），及任意数量线程的读操作。 在将Key/Value对插入到Segment之前，首先会检查本次插入会不会导致Segment中元素的数量超过阈值threshold，如果会，那么就先对Segment进行扩容和重哈希操作，然后再进行插入。重哈希操作暂且不表，稍后详述。第8和第9行的操作就是定位到段中特定的桶并确定链表头部的位置。第12行的while循环用于检查该桶中是否存在相同key的结点，如果存在，就直接更新value值；如果没有找到，则进入21行生成一个新的HashEntry并且把它链到该桶中链表的表头，然后再更新count的值(由于count是volatile变量，所以count值的更新一定要放在最后一步)。 到此为止，除了重哈希操作，ConcurrentHashMap的put操作已经介绍完了。此外，在ConcurrentHashMap中，修改操作还包括putAll()和replace()。其中，putAll()操作就是多次调用put方法，而replace()操作实现要比put()操作简单得多，此不赘述。 重哈希操作上面叙述到，在ConcurrentHashMap中使用put操作插入Key/Value对之前，首先会检查本次插入会不会导致Segment中节点数量超过阈值threshold，如果会，那么就先对Segment进行扩容和重哈希操作。特别需要注意的是，ConcurrentHashMap的重哈希实际上是对ConcurrentHashMap的某个段的重哈希，因此ConcurrentHashMap的每个段所包含的桶位自然也就不尽相同。针对段进行rehash()操作的源码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364void rehash() { HashEntry&lt;K,V&gt;[] oldTable = table; // 扩容前的table int oldCapacity = oldTable.length; if (oldCapacity &gt;= MAXIMUM_CAPACITY) // 已经扩到最大容量，直接返回 return; /* * Reclassify nodes in each list to new Map. Because we are * using power-of-two expansion, the elements from each bin * must either stay at same index, or move with a power of two * offset. We eliminate unnecessary node creation by catching * cases where old nodes can be reused because their next * fields won't change. Statistically, at the default * threshold, only about one-sixth of them need cloning when * a table doubles. The nodes they replace will be garbage * collectable as soon as they are no longer referenced by any * reader thread that may be in the midst of traversing table * right now. */ // 新创建一个table，其容量是原来的2倍 HashEntry&lt;K,V&gt;[] newTable = HashEntry.newArray(oldCapacity&lt;&lt;1); threshold = (int)(newTable.length * loadFactor); // 新的阈值 int sizeMask = newTable.length - 1; // 用于定位桶 for (int i = 0; i &lt; oldCapacity ; i++) { // We need to guarantee that any existing reads of old Map can // proceed. So we cannot yet null out each bin. HashEntry&lt;K,V&gt; e = oldTable[i]; // 依次指向旧table中的每个桶的链表表头 if (e != null) { // 旧table的该桶中链表不为空 HashEntry&lt;K,V&gt; next = e.next; int idx = e.hash &amp; sizeMask; // 重哈希已定位到新桶 if (next == null) // 旧table的该桶中只有一个节点 newTable[idx] = e; else { // Reuse trailing consecutive sequence at same slot HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; // 寻找k值相同的子链，该子链尾节点与父链的尾节点必须是同一个 if (k != lastIdx) { lastIdx = k; lastRun = last; } } // JDK直接将子链lastRun放到newTable[lastIdx]桶中 newTable[lastIdx] = lastRun; // 对该子链之前的结点，JDK会挨个遍历并把它们复制到新桶中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) { int k = p.hash &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(p.key, p.hash, n, p.value); } } } } table = newTable; // 扩容完成} 其实JDK官方的注释已经解释的很清楚了。由于扩容是按照2的幂次方进行的，所以扩展前在同一个桶中的元素，现在要么还是在原来的序号的桶里，或者就是原来的序号再加上一个2的幂次方，就这两种选择。根据本文前面对HashEntry的介绍，我们知道链接指针next是final的，因此看起来我们好像只能把该桶的HashEntry链中的每个节点复制到新的桶中(这意味着我们要重新创建每个节点)，但事实上JDK对其做了一定的优化。因为在理论上原桶里的HashEntry链可能存在一条子链，这条子链上的节点都会被重哈希到同一个新的桶中，这样我们只要拿到该子链的头结点就可以直接把该子链放到新的桶中，从而避免了一些节点不必要的创建，提升了一定的效率。因此，JDK为了提高效率，它会首先去查找这样的一个子链，而且这个子链的尾节点必须与原hash链的尾节点是同一个，那么就只需要把这个子链的头结点放到新的桶中，其后面跟的一串子节点自然也就连接上了。对于这个子链头结点之前的结点，JDK会挨个遍历并把它们复制到新桶的链头(只能在表头插入元素)中。特别地，我们注意这段代码：12345678910for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) { int k = last.hash &amp; sizeMask; if (k != lastIdx) { lastIdx = k; lastRun = last; }}newTable[lastIdx] = lastRun; 在该代码段中，JDK直接将子链lastRun放到newTable[lastIdx]桶中，难道这个操作不会覆盖掉newTable[lastIdx]桶中原有的元素么？事实上，这种情形时不可能出现的，因为桶newTable[lastIdx]在子链添加进去之前压根就不会有节点存在，这还是因为table的大小是按照2的幂次方的方式去扩展的。假设原来table的大小是2^k大小，那么现在新table的大小是2^(k+1)大小，而定位桶的方式是:12// sizeMask = newTable.length - 1，即 sizeMask = 11...1，共k+1个1。int idx = e.hash &amp; sizeMask; 因此这样得到的idx实际上就是key的hash值的低k+1位的值，而原table的sizeMask也全是1的二进制，不过总共是k位，那么原table的idx就是key的hash值的低k位的值。所以，如果元素的hashcode的第k+1位是0，那么元素在新桶的序号就是和原桶的序号是相等的；如果第k+1位的值是1，那么元素在新桶的序号就是原桶的序号加上2^k。因此，JDK直接将子链lastRun放到newTable[lastIdx]桶中就没问题了，因为newTable中新序号处此时肯定是空的。 读取操作与put操作类似，当我们从ConcurrentHashMap中查询一个指定Key的键值对时，首先会定位其应该存在的段，然后查询请求委托给这个段进行处理，源码如下：1234public V get(Object key) { int hash = hash(key.hashCode()); return segmentFor(hash).get(key, hash);} 我们紧接着研读Segment中get操作的源码：12345678910111213141516V get(Object key, int hash) { if (count != 0) { // read-volatile，首先读 count 变量 HashEntry&lt;K,V&gt; e = getFirst(hash); // 获取桶中链表头结点 while (e != null) { if (e.hash == hash &amp;&amp; key.equals(e.key)) { // 查找链中是否存在指定Key的键值对 V v = e.value; if (v != null) // 如果读到value域不为 null，直接返回 return v; // 如果读到value域为null，说明发生了重排序，加锁后重新读取 return readValueUnderLock(e); // recheck } e = e.next; } } return null; // 如果不存在，直接返回null} 了解了ConcurrentHashMap的put操作后，上述源码就很好理解了。但是有一个情况需要特别注意，就是链中存在指定Key的键值对并且其对应的Value值为null的情况。在剖析ConcurrentHashMap的put操作时，我们就知道ConcurrentHashMap不同于HashMap，它既不允许key值为null，也不允许value值为null。但是，此处怎么会存在键值对存在且的Value值为null的情形呢？JDK官方给出的解释是，这种情形发生的场景是：初始化HashEntry时发生的指令重排序导致的，也就是在HashEntry初始化完成之前便返回了它的引用。这时，JDK给出的解决之道就是加锁重读，源码如下：12345678V readValueUnderLock(HashEntry&lt;K,V&gt; e) { lock(); try { return e.value; } finally { unlock(); } } 在ConcurrentHashMap进行存取时，首先会定位到具体的段，然后通过对具体段的存取来完成对整个ConcurrentHashMap的存取。特别地，无论是ConcurrentHashMap的读操作还是写操作都具有很高的性能：在进行读操作时不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。","link":"/2018/11/23/javase/ConcurrentHashMap源码剖析/"},{"title":"JavaSE源码分析-HashSet源码剖析","text":"HashSet简介HashSet实现Set接口，由哈希表（实际上是一个HashMap实例）支持。主要具有以下的特点： 不保证set的迭代顺序，特别是它不保证该顺序恒久不变 有且只允许一个null元素 不允许有重复元素，这是因为HashSet是基于HashMap实现的，HashSet中的元素都存放在HashMap的key上面，而value中的值都是统一的一个private static final Object PRESENT = new Object(); 非同步的。如果多 个线程同时访问一个哈希set，而其中至少一个线程修改了该 set，那么它必须保持外部同步。这通常是通过对自然封装该set的对象执行同步操作来完成的。如果不存在这样的对象，则应该使用 Collections.synchronizedSet 方法来“包装” set。最好在创建时完成这一操作，以防止对该set进行意外的不同步访问： 1Set s = Collections.synchronizedSet(new HashSet(...)); HashSet通过iterator()返回的迭代器是fail-fast的 类定义通过HashSet实现的接口可知，其支持所有集合操作，能被克隆，支持序列化12345678910111213public class HashSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements Set&lt;E&gt;, Cloneable, java.io.Serializable{ static final long serialVersionUID = -5024744406713321676L; private transient HashMap&lt;E,Object&gt; map; //定义一个\"虚拟\"的static final Object对象作为HashMap的value private static final Object PRESENT = new Object(); ......} HashSet包含了两个重要的成员变量：map, PRESENT。 map是一个HashMap对象，HashSet是由一个HashMap实例支持的。 PRESENT是一个static final Object对象，用来作为HashMap中的value值。 构造函数HashSet提供了四种方式的构造器，可以构造一个新的空 set，其底层 HashMap实例的默认初始容量是16，加载因子是 0.75，构造一个包含指定collection中的元素的新set，构造一个新的空set，其底层HashMap实例具有指定的初始容量和默认的加载因子（0.75），以及构造一个新的空set，其底层HashMap实例具有指定的初始容量和指定的加载因子。12345678910111213141516171819202122232425262728293031323334//默认的无参构造器，构造一个空的HashSet,实际底层会初始化一个空的HashMap，并使用默认初始容量为16和加载因子0.75。public HashSet() { map = new HashMap&lt;E, Object&gt;();}//构造一个包含指定collection中的元素的新set。//实际底层使用默认的加载因子0.75和足以包含指定collection中所有元素的初始容量来创建一个HashMap。public HashSet(Collection&lt;? extends E&gt; c) { map = new HashMap&lt;E, Object&gt;(Math.max((int) (c.size()/.75f) + 1, 16)); addAll(c); //AbstractCollection.addAll(Collection&lt;? extends E&gt; c)}//以指定的初始容量和加载因子构造一个空的HashSetpublic HashSet(int initialCapacity, float loadFactor) { map = new HashMap&lt;E, Object&gt;(initialCapacity, loadFactor);}//以指定的initialCapacity和默认加载因子0.75构造一个空的HashSetpublic HashSet(int initialCapacity) { map = new HashMap&lt;E, Object&gt;(initialCapacity);}/** * 以指定的initialCapacity和loadFactor构造一个新的空链接哈希集合 * 此构造函数为包访问权限，不对外公开，实际只是是对LinkedHashSet的支持 * * @param initialCapacity 初始容量 * @param loadFactor 加载因子 * @param dummy 标记，用于与其他的构造函数区分（可忽略） * @throws IllegalArgumentException 如果初始容量小于零或加载因子为非正数 */HashSet(int initialCapacity, float loadFactor, boolean dummy) { map = new LinkedHashMap&lt;E, Object&gt;(initialCapacity, loadFactor);} 添加HashSet提供了add(E e)添加元素的方法，其调用的是底层HashMap中的put(K key, V value)方法，首先判断元素（也就是key）是否存在，如果不存在则插入，如果存在则不插入，这样HashSet中就不存在重复值。1234//如果此set中尚未包含指定元素，则添加指定元素public boolean add(E e) { return map.put(e, PRESENT)==null;} 底层：当向HashSet集合中存入一个元素时，HashSet会调用该对象的hashCode()方法来得到该对象的hashCode值，然后根据该值确定对象在HashSet中的存储位置。在Hash集合中，不能同时存放两个相等的元素，而判断两个元素相等的标准是两个对象通过equals方法比较相等并且两个对象的HashCode方法返回值也相等。 注意：对于HashSet中保存的对象，请注意正确重写其equals和hashCode方法，以保证放入的对象的唯一性。 清空与删除HashSet提供了remove(Object o)删除元素、clear()清除所有元素的方法。123456789//如果指定元素存在于此set中，则将其移除public boolean remove(Object o) { return map.remove(o)==PRESENT;}//从此set中移除所有元素public void clear() { map.clear();} 查找HashSet提供了contains(Object o)查看是否包含指定元素的方法，其底层调用的是HashMap.containsKey(Object key)判断是否包含指定key。1234//如果此set包含指定元素，则返回 truepublic boolean contains(Object o) { return map.containsKey(o);} 其他公开的方法size()、isEmpty()、clone()123456789101112131415161718192021//返回此set中的元素的数量public int size() { return map.size();}//如果此set不包含任何元素，则返回 truepublic boolean isEmpty() { return map.isEmpty();}//返回此 HashSet实例的浅表副本@SuppressWarnings(\"unchecked\")public Object clone() { try { HashSet&lt;E&gt; newSet = (HashSet&lt;E&gt;) super.clone(); newSet.map = (HashMap&lt;E, Object&gt;) map.clone(); return newSet; } catch (CloneNotSupportedException e) { throw new InternalError(e); }} 支持序列化的写入函数writeObject(java.io.ObjectOutputStream s)和读取函数readObject(java.io.ObjectInputStream s)：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//java.io.Serializable的写入函数，将HashSet的“总的容量，加载因子，实际容量，所有的元素”都写入到输出流中private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException { // Write out any hidden serialization magic s.defaultWriteObject(); // Write out HashMap capacity and load factor s.writeInt(map.capacity()); s.writeFloat(map.loadFactor()); // Write out size s.writeInt(map.size()); // Write out all elements in the proper order. for (E e : map.keySet()) s.writeObject(e);}// java.io.Serializable的读取函数，将HashSet的“总的容量，加载因子，实际容量，所有的元素”依次读出private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException { // Read in any hidden serialization magic s.defaultReadObject(); // Read capacity and verify non-negative. int capacity = s.readInt(); if (capacity &lt; 0) { throw new InvalidObjectException(\"Illegal capacity: \" + capacity); } // Read load factor and verify positive and non NaN. float loadFactor = s.readFloat(); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) { throw new InvalidObjectException(\"Illegal load factor: \" + loadFactor); } // Read size and verify non-negative. int size = s.readInt(); if (size &lt; 0) { throw new InvalidObjectException(\"Illegal size: \" + size); } // Set the capacity according to the size and load factor ensuring that // the HashMap is at least 25% full but clamping to maximum capacity. capacity = (int) Math.min(size * Math.min(1 / loadFactor, 4.0f), HashMap.MAXIMUM_CAPACITY); // Create backing HashMap map = (((HashSet&lt;?&gt;)this) instanceof LinkedHashSet ? new LinkedHashMap&lt;E,Object&gt;(capacity, loadFactor) : new HashMap&lt;E,Object&gt;(capacity, loadFactor)); // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) { @SuppressWarnings(\"unchecked\") E e = (E) s.readObject(); map.put(e, PRESENT); }} HashSet的迭代 HashSet通过调用HashMap.keySet()返回对中的key集以此得到集合的迭代器。1234//返回对此 set中元素进行迭代的迭代器public Iterator&lt;E&gt; iterator() { return map.keySet().iterator(); //HashMap.keySet()返回&lt;key, value&gt;对中的key集}","link":"/2018/12/05/javase/HashSet源码剖析/"},{"title":"JavaSE源码分析-Integer源码分析","text":"Integer 类在对象中包装了一个基本类型 int 的值。Integer 类型的对象包含一个 int 类型的字段。 类定义1public final class Integer extends Number implements Comparable&lt;Integer&gt; 从类定义中我们可以知道以下几点： Integer类不能被继承 Integer类实现了Comparable接口，所以可以用compareTo进行比较并且Integer对象只能和Integer类型的对象进行比较，不能和其他类型比较（至少调用compareTo方法无法比较）。 Integer继承了Number类，所以该类可以调用longValue、floatValue、doubleValue等系列方法返回对应的类型的值。 Numbers实现Serializable接口，所以Integer也支持序列化和反序列化。 属性私有属性Integer类中定义了以下几个私有属性：12private final int value;private static final long serialVersionUID = 1360826667806852920L; value属性就是Integer对象中真正保存int值的。 serialVersionUID和序列化有关。Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常(InvalidCastException)。 公共属性12345678910//值为 （－（2的31次方）） 的常量，它表示 int 类型能够表示的最小值。@Native public static final int MIN_VALUE = 0x80000000;//值为 （（2的31次方）－1） 的常量，它表示 int 类型能够表示的最大值。@Native public static final int MAX_VALUE = 0x7fffffff; //表示基本类型 int 的 Class 实例。public static final Class&lt;Integer&gt; TYPE = (Class&lt;Integer&gt;) Class.getPrimitiveClass(\"int\");//用来以二进制补码形式表示 int 值的比特位数。@Native public static final int SIZE = 32;//用来以二进制补码形式表示 int 值的字节数。1.8以后才有public static final int BYTES = SIZE / Byte.SIZE; 以上属性可直接使用，因为他们已经定义成publis static fianl能用的时候尽量使用他们，这样不仅能使代码有很好的可读性，也能提高性能节省资源。 装箱拆箱12345678public class IntegerLearning { public static void main(String[] args) { Integer i = 100; int i2 = i; System.out.println(i); }} 上面代码，编译后，我们通过jdk自带的javap命令工具对IntegerLearning.class 进行分析javap -v IntegerLearning.class123456789101112131415public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=3, args_size=1 0: bipush 100 2: invokestatic #2 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 5: astore_1 6: aload_1 7: invokevirtual #3 // Method java/lang/Integer.intValue:()I 10: istore_2 11: getstatic #4 // Field java/lang/System.out:Ljava/io/PrintStream; 14: aload_1 15: invokevirtual #5 // Method java/io/PrintStream.println:(Ljava/lang/Object;)V 18: return 从上面可以看出Integer i = 100; 编译器会转成 Integer i = Integer.valueOf(100);int i2 = i 自动拆箱用的是 Integer.intValue 内部类123456789101112131415161718192021222324252627282930313233private static class IntegerCache { static final int low = -128; static final int high; static final Integer cache[]; static { // high value may be configured by property int h = 127; String integerCacheHighPropValue = sun.misc.VM.getSavedProperty(\"java.lang.Integer.IntegerCache.high\"); if (integerCacheHighPropValue != null) { try { int i = parseInt(integerCacheHighPropValue); i = Math.max(i, 127); // Maximum array size is Integer.MAX_VALUE h = Math.min(i, Integer.MAX_VALUE - (-low) -1); } catch( NumberFormatException nfe) { // If the property cannot be parsed into an int, ignore it. } } high = h; cache = new Integer[(high - low) + 1]; int j = low; for(int k = 0; k &lt; cache.length; k++) cache[k] = new Integer(j++); // range [-128, 127] must be interned (JLS7 5.1.7) assert IntegerCache.high &gt;= 127; } private IntegerCache() {}} Integer内部有一个内部类IntegerCache，用来缓存缓存以支持-128和127（包括）之间的值的自动装箱的对象标识语义。可以看出，Integer实例化时，IntegerCache中的 Integer cache[] 缓存了 -128 和127（包括）之间的值，下面会介绍如何使用了缓存的内部类。 方法构造方法Integer提供了两个构造方法： 123456789//构造一个新分配的 Integer 对象，它表示指定的 int 值。public Integer(int value) { this.value = value;}//构造一个新分配的 Integer 对象，它表示 String 参数所指示的 int 值。public Integer(String s) throws NumberFormatException { this.value = parseInt(s, 10);} 从构造方法中我们可以知道，初始化一个Integer对象的时候只能创建一个十进制的整数。 String转Integerparse方法在String的构造方法中，使用了parseInt，我们来看下全部的parse方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970// 使用进制基数radix（2,8,10,16,27,36）解析字符串Spublic static int parseInt(String s, int radix) throws NumberFormatException{ if (s == null) { throw new NumberFormatException(\"null\"); } if (radix &lt; Character.MIN_RADIX) { throw new NumberFormatException(\"radix \" + radix + \" less than Character.MIN_RADIX\"); } if (radix &gt; Character.MAX_RADIX) { throw new NumberFormatException(\"radix \" + radix + \" greater than Character.MAX_RADIX\"); } int result = 0; boolean negative = false; int i = 0, len = s.length(); int limit = -Integer.MAX_VALUE; int multmin; int digit; if (len &gt; 0) { // 判断字符串开通是否以 + - 开头 char firstChar = s.charAt(0); if (firstChar &lt; '0') { // Possible leading \"+\" or \"-\" if (firstChar == '-') { negative = true; limit = Integer.MIN_VALUE; } else if (firstChar != '+') throw NumberFormatException.forInputString(s); if (len == 1) // Cannot have lone \"+\" or \"-\" throw NumberFormatException.forInputString(s); i++; } // 循环遍历字符串每个字符转成相应进制数字 multmin = limit / radix; while (i &lt; len) { // Accumulating negatively avoids surprises near MAX_VALUE digit = Character.digit(s.charAt(i++),radix); if (digit &lt; 0) { throw NumberFormatException.forInputString(s); } if (result &lt; multmin) { throw NumberFormatException.forInputString(s); } result *= radix; if (result &lt; limit + digit) { throw NumberFormatException.forInputString(s); } result -= digit; } } else { throw NumberFormatException.forInputString(s); } return negative ? result : -result;}public static int parseInt(String s) throws NumberFormatException { return parseInt(s,10);}public static int parseUnsignedInt(String s)public static int parseUnsignedInt(String s, int radix) parseUnsignedInt 解析无符号的字符串，在内部原理调用的是parseInt 或者Long.parseLong。这里就不在说明。 valueOf方法12345678910111213public static Integer valueOf(int i) { if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);}public static Integer valueOf(String s, int radix) throws NumberFormatException { return Integer.valueOf(parseInt(s,radix));}public static Integer valueOf(String s) throws NumberFormatException { return Integer.valueOf(parseInt(s, 10));} valueOf(int i)使用缓存内部类IntegerCache,如果是-128和127之间的数，不是重新new的类，而直接返回缓存的Integer类。 getInteger方法1234567891011121314151617181920212223public static Integer getInteger(String nm) { return getInteger(nm, null);}public static Integer getInteger(String nm, int val) { Integer result = getInteger(nm, null); return (result == null) ? Integer.valueOf(val) : result;}public static Integer getInteger(String nm, Integer val) { String v = null; try { v = System.getProperty(nm); } catch (IllegalArgumentException | NullPointerException e) { } if (v != null) { try { return Integer.decode(v); } catch (NumberFormatException e) { } } return val;} 从上面可以看出getInteger 实际上调用的是Integer.decode方法。那我们来看看decode1234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 通过字符串前面几位字符得出 是否为 十进制，十六进制和八进制数，而以其规则解析成Integer类public static Integer decode(String nm) throws NumberFormatException { int radix = 10; int index = 0; boolean negative = false; Integer result; if (nm.length() == 0) throw new NumberFormatException(\"Zero length string\"); char firstChar = nm.charAt(0); // Handle sign, if present if (firstChar == '-') { negative = true; index++; } else if (firstChar == '+') index++; // Handle radix specifier, if present if (nm.startsWith(\"0x\", index) || nm.startsWith(\"0X\", index)) { index += 2; radix = 16; } else if (nm.startsWith(\"#\", index)) { index ++; radix = 16; } else if (nm.startsWith(\"0\", index) &amp;&amp; nm.length() &gt; 1 + index) { index ++; radix = 8; } if (nm.startsWith(\"-\", index) || nm.startsWith(\"+\", index)) throw new NumberFormatException(\"Sign character in wrong position\"); try { result = Integer.valueOf(nm.substring(index), radix); result = negative ? Integer.valueOf(-result.intValue()) : result; } catch (NumberFormatException e) { // If number is Integer.MIN_VALUE, we'll end up here. The next line // handles this case, and causes any genuine format error to be // rethrown. String constant = negative ? (\"-\" + nm.substring(index)) : nm.substring(index); result = Integer.valueOf(constant, radix); } return result;} 我们又发现decode 底层调用的是Integer.valueOf。从而得出： 所有将String转成Integer的方法都是基于parseInt方法实现的。简单看一下以上部分方法的调用栈。123456getInteger(...) ---&gt; ---&gt;Integer.decode(...)---&gt;Integer.valueOf(...)---&gt;parseInt(...)``` ## Integer转String### toString public String toString() { return toString(value);} public static String toString(int i) { if (i == Integer.MIN_VALUE) return “-2147483648”; int size = (i &lt; 0) ? stringSize(-i) + 1 : stringSize(i); char[] buf = new char[size]; getChars(i, size, buf); return new String(buf, true);}12 public static String toString(int i, int radix)public static String toBinaryString(int i)public static String toHexString(int i)public static String toOctalString(int i)public static String toUnsignedString(int i)public static String toUnsignedString(int i, int radix)```","link":"/2018/10/24/javase/Integer源码分析/"},{"title":"JavaSE源码分析-LinkedList源码剖析","text":"概述LinkedList与ArrayList一样实现List接口，只是ArrayList是List接口的大小可变数组的实现，LinkedList是List接口链表的实现。基于链表实现的方式使得LinkedList在插入和删除时更优于ArrayList，而随机访问则比ArrayList逊色些。 LinkedList实现所有可选的列表操作，并允许所有的元素包括null。 除了实现 List 接口外，LinkedList 类还为在列表的开头及结尾 get、remove 和 insert 元素提供了统一的命名方法。这些操作允许将链接列表用作堆栈、队列或双端队列。 此类实现 Deque 接口，为 add、poll 提供先进先出队列操作，以及其他堆栈和双端队列操作。 所有操作都是按照双重链接列表的需要执行的。在列表中编索引的操作将从开头或结尾遍历列表（从靠近指定索引的一端）。 同时，与ArrayList一样此实现不是同步的。 （以上摘自JDK 6.0 API）。 定义首先我们先看LinkedList的定义： 123public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable 从这段代码中我们可以清晰地看出LinkedList继承AbstractSequentialList，实现List、Deque、Cloneable、Serializable。其中AbstractSequentialList提供了 List 接口的骨干实现，从而最大限度地减少了实现受“连续访问”数据存储（如链接列表）支持的此接口所需的工作,从而以减少实现List接口的复杂度。Deque一个线性 collection，支持在两端插入和移除元素，定义了双端队列的操作。 属性在LinkedList中提供了两个基本属性size、header。12private transient Entry&lt;E&gt; header = new Entry&lt;E&gt;(null, null, null); private transient int size = 0; 其中size表示的LinkedList的大小，header表示链表的表头，Entry为节点对象。1234567891011private static class Entry&lt;E&gt; { E element; //元素节点 Entry&lt;E&gt; next; //下一个元素 Entry&lt;E&gt; previous; //上一个元素 Entry(E element, Entry&lt;E&gt; next, Entry&lt;E&gt; previous) { this.element = element; this.next = next; this.previous = previous; } } 上面为Entry对象的源代码，Entry为LinkedList的内部类，它定义了存储的元素。该元素的前一个元素、后一个元素，这是典型的双向链表定义方式。 方法构造方法LinkedList提高了两个构造方法：LinkedLis()和LinkedList(Collection&lt;? extends E&gt; c)。1234567891011121314/** * 构造一个空列表。 */ public LinkedList() { header.next = header.previous = header; } /** * 构造一个包含指定 collection 中的元素的列表，这些元素按其 collection 的迭代器返回的顺序排列。 */ public LinkedList(Collection&lt;? extends E&gt; c) { this(); addAll(c); } LinkedList()构造一个空列表。里面没有任何元素，仅仅只是将header节点的前一个元素、后一个元素都指向自身。LinkedList(Collection&lt;? extends E&gt; c)： 构造一个包含指定 collection 中的元素的列表，这些元素按其 collection 的迭代器返回的顺序排列。该构造函数首先会调用LinkedList()，构造一个空列表，然后调用了addAll()方法将Collection中的所有元素添加到列表中。以下是addAll()的源代码：12345678910111213141516171819202122232425262728293031323334353637383940/** * 添加指定 collection 中的所有元素到此列表的结尾，顺序是指定 collection 的迭代器返回这些元素的顺序。 */ public boolean addAll(Collection&lt;? extends E&gt; c) { return addAll(size, c); } /** * 将指定 collection 中的所有元素从指定位置开始插入此列表。其中index表示在其中插入指定collection中第一个元素的索引 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) { //若插入的位置小于0或者大于链表长度，则抛出IndexOutOfBoundsException异常 if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(\"Index: \" + index + \", Size: \" + size); Object[] a = c.toArray(); int numNew = a.length; //插入元素的个数 //若插入的元素为空，则返回false if (numNew == 0) return false; //modCount:在AbstractList中定义的，表示从结构上修改列表的次数 modCount++; //获取插入位置的节点，若插入的位置在size处，则是头节点，否则获取index位置处的节点 Entry&lt;E&gt; successor = (index == size ? header : entry(index)); //插入位置的前一个节点，在插入过程中需要修改该节点的next引用：指向插入的节点元素 Entry&lt;E&gt; predecessor = successor.previous; //执行插入动作 for (int i = 0; i &lt; numNew; i++) { //构造一个节点e，这里已经执行了插入节点动作同时修改了相邻节点的指向引用 // Entry&lt;E&gt; e = new Entry&lt;E&gt;((E) a[i], successor, predecessor); //将插入位置前一个节点的下一个元素引用指向当前元素 predecessor.next = e; //修改插入位置的前一个节点，这样做的目的是将插入位置右移一位，保证后续的元素是插在该元素的后面，确保这些元素的顺序 predecessor = e; } successor.previous = predecessor; //修改容量大小 size += numNew; return true; } 在addAll()方法中，涉及到了两个方法，一个是entry(int index)，该方法为LinkedList的私有方法，主要是用来查找index位置的节点元素。12345678910111213141516171819/** * 返回指定位置(若存在)的节点元素 */ private Entry&lt;E&gt; entry(int index) { if (index &lt; 0 || index &gt;= size) throw new IndexOutOfBoundsException(\"Index: \" + index + \", Size: \" + size); //头部节点 Entry&lt;E&gt; e = header; //判断遍历的方向 if (index &lt; (size &gt;&gt; 1)) { for (int i = 0; i &lt;= index; i++) e = e.next; } else { for (int i = size; i &gt; index; i--) e = e.previous; } return e; } 从该方法有两个遍历方向中我们也可以看出LinkedList是双向链表，这也是在构造方法中为什么需要将header的前、后节点均指向自己。 如果对数据结构有点了解，对上面所涉及的内容应该问题，我们只需要清楚一点：LinkedList是双向链表，其余都迎刃而解。由于篇幅有限，下面将就LinkedList中几个常用的方法进行源码分析。 增加方法add(E e): 将指定元素添加到此列表的结尾。1234public boolean add(E e) { addBefore(e, header); return true; } 该方法调用addBefore方法，然后直接返回true，对于addBefore()而已，它为LinkedList的私有方法。 123456789101112private Entry&lt;E&gt; addBefore(E e, Entry&lt;E&gt; entry) { //利用Entry构造函数构建一个新节点 newEntry， Entry&lt;E&gt; newEntry = new Entry&lt;E&gt;(e, entry, entry.previous); //修改newEntry的前后节点的引用，确保其链表的引用关系是正确的 newEntry.previous.next = newEntry; newEntry.next.previous = newEntry; //容量+1 size++; //修改次数+1 modCount++; return newEntry; } 在addBefore方法中无非就是做了这件事：构建一个新节点newEntry，然后修改其前后的引用。 LinkedList还提供了其他的增加方法： add(int index, E element)：在此列表中指定的位置插入指定的元素。 addAll(Collection&lt;? extends E&gt; c)：添加指定 collection 中的所有元素到此列表的结尾，顺序是指定 collection 的迭代器返回这些元素的顺序。 addAll(int index, Collection&lt;? extends E&gt; c)：将指定 collection 中的所有元素从指定位置开始插入此列表。 AddFirst(E e): 将指定元素插入此列表的开头。 addLast(E e): 将指定元素添加到此列表的结尾。 移除方法remove(Object o)：从此列表中移除首次出现的指定元素（如果存在）。该方法的源代码如下：123456789101112131415161718public boolean remove(Object o) { if (o==null) { for (Entry&lt;E&gt; e = header.next; e != header; e = e.next) { if (e.element==null) { remove(e); return true; } } } else { for (Entry&lt;E&gt; e = header.next; e != header; e = e.next) { if (o.equals(e.element)) { remove(e); return true; } } } return false; } 该方法首先会判断移除的元素是否为null，然后迭代这个链表找到该元素节点，最后调用remove(Entry e)，remove(Entry e)为私有方法，是LinkedList中所有移除方法的基础方法，如下： 12345678910111213141516171819private E remove(Entry&lt;E&gt; e) { if (e == header) throw new NoSuchElementException(); //保留被移除的元素：要返回 E result = e.element; //将该节点的前一节点的next指向该节点后节点 e.previous.next = e.next; //将该节点的后一节点的previous指向该节点的前节点 //这两步就可以将该节点从链表从除去：在该链表中是无法遍历到该节点的 e.next.previous = e.previous; //将该节点归空 e.next = e.previous = null; e.element = null; size--; modCount++; return result; } 其他的移除方法： clear()： 从此列表中移除所有元素。 remove()：获取并移除此列表的头（第一个元素）。 remove(int index)：移除此列表中指定位置处的元素。 remove(Objec o)：从此列表中移除首次出现的指定元素（如果存在）。 removeFirst()：移除并返回此列表的第一个元素。 removeFirstOccurrence(Object o)：从此列表中移除第一次出现的指定元素（从头部到尾部遍历列表时）。 removeLast()：移除并返回此列表的最后一个元素。 removeLastOccurrence(Object o)：从此列表中移除最后一次出现的指定元素（从头部到尾部遍历列表时）。 查找方法对于查找方法的源码就没有什么好介绍了，无非就是迭代，比对，然后就是返回当前值。 get(int index)：返回此列表中指定位置处的元素。 getFirst()：返回此列表的第一个元素。 getLast()：返回此列表的最后一个元素。 indexOf(Object o)：返回此列表中首次出现的指定元素的索引，如果此列表中不包含该元素，则返回 -1。 lastIndexOf(Object o)：返回此列表中最后出现的指定元素的索引，如果此列表中不包含该元素，则返回 -1。","link":"/2018/11/27/javase/LinkedList源码剖析/"},{"title":"JavaSE源码分析-Hashtable源码剖析","text":"Hashtable简介Hashtable同样是基于哈希表实现的，同样每个元素是一个key-value对，其内部也是通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。Hashtable也是JDK1.0引入的类，是线程安全的，能用于多线程环境中。Hashtable同样实现了Serializable接口，它支持序列化，实现了Cloneable接口，能被克隆。 HashTable源码剖析Hashtable的源码的很多实现都与HashMap差不多，源码如下（加入了比较详细的注释）：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789package java.util; import java.io.*; public class Hashtable&lt;K,V&gt; extends Dictionary&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, java.io.Serializable { // 保存key-value的数组。 // Hashtable同样采用单链表解决冲突，每一个Entry本质上是一个单向链表 private transient Entry[] table; // Hashtable中键值对的数量 private transient int count; // 阈值，用于判断是否需要调整Hashtable的容量（threshold = 容量*加载因子） private int threshold; // 加载因子 private float loadFactor; // Hashtable被改变的次数，用于fail-fast机制的实现 private transient int modCount = 0; // 序列版本号 private static final long serialVersionUID = 1421746759512286392L; // 指定“容量大小”和“加载因子”的构造函数 public Hashtable(int initialCapacity, float loadFactor) { if (initialCapacity &lt; 0) throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\"Illegal Load: \"+loadFactor); if (initialCapacity==0) initialCapacity = 1; this.loadFactor = loadFactor; table = new Entry[initialCapacity]; threshold = (int)(initialCapacity * loadFactor); } // 指定“容量大小”的构造函数 public Hashtable(int initialCapacity) { this(initialCapacity, 0.75f); } // 默认构造函数。 public Hashtable() { // 默认构造函数，指定的容量大小是11；加载因子是0.75 this(11, 0.75f); } // 包含“子Map”的构造函数 public Hashtable(Map&lt;? extends K, ? extends V&gt; t) { this(Math.max(2*t.size(), 11), 0.75f); // 将“子Map”的全部元素都添加到Hashtable中 putAll(t); } public synchronized int size() { return count; } public synchronized boolean isEmpty() { return count == 0; } // 返回“所有key”的枚举对象 public synchronized Enumeration&lt;K&gt; keys() { return this.&lt;K&gt;getEnumeration(KEYS); } // 返回“所有value”的枚举对象 public synchronized Enumeration&lt;V&gt; elements() { return this.&lt;V&gt;getEnumeration(VALUES); } // 判断Hashtable是否包含“值(value)” public synchronized boolean contains(Object value) { //注意，Hashtable中的value不能是null， // 若是null的话，抛出异常! if (value == null) { throw new NullPointerException(); } // 从后向前遍历table数组中的元素(Entry) // 对于每个Entry(单向链表)，逐个遍历，判断节点的值是否等于value Entry tab[] = table; for (int i = tab.length ; i-- &gt; 0 ;) { for (Entry&lt;K,V&gt; e = tab[i] ; e != null ; e = e.next) { if (e.value.equals(value)) { return true; } } } return false; } public boolean containsValue(Object value) { return contains(value); } // 判断Hashtable是否包含key public synchronized boolean containsKey(Object key) { Entry tab[] = table; //计算hash值，直接用key的hashCode代替 int hash = key.hashCode(); // 计算在数组中的索引值 int index = (hash &amp; 0x7FFFFFFF) % tab.length; // 找到“key对应的Entry(链表)”，然后在链表中找出“哈希值”和“键值”与key都相等的元素 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { return true; } } return false; } // 返回key对应的value，没有的话返回null public synchronized V get(Object key) { Entry tab[] = table; int hash = key.hashCode(); // 计算索引值， int index = (hash &amp; 0x7FFFFFFF) % tab.length; // 找到“key对应的Entry(链表)”，然后在链表中找出“哈希值”和“键值”与key都相等的元素 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { return e.value; } } return null; } // 调整Hashtable的长度，将长度变成原来的2倍+1 protected void rehash() { int oldCapacity = table.length; Entry[] oldMap = table; //创建新容量大小的Entry数组 int newCapacity = oldCapacity * 2 + 1; Entry[] newMap = new Entry[newCapacity]; modCount++; threshold = (int)(newCapacity * loadFactor); table = newMap; //将“旧的Hashtable”中的元素复制到“新的Hashtable”中 for (int i = oldCapacity ; i-- &gt; 0 ;) { for (Entry&lt;K,V&gt; old = oldMap[i] ; old != null ; ) { Entry&lt;K,V&gt; e = old; old = old.next; //重新计算index int index = (e.hash &amp; 0x7FFFFFFF) % newCapacity; e.next = newMap[index]; newMap[index] = e; } } } // 将“key-value”添加到Hashtable中 public synchronized V put(K key, V value) { // Hashtable中不能插入value为null的元素！！！ if (value == null) { throw new NullPointerException(); } // 若“Hashtable中已存在键为key的键值对”， // 则用“新的value”替换“旧的value” Entry tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { V old = e.value; e.value = value; return old; } } // 若“Hashtable中不存在键为key的键值对”， // 将“修改统计数”+1 modCount++; // 若“Hashtable实际容量” &gt; “阈值”(阈值=总的容量 * 加载因子) // 则调整Hashtable的大小 if (count &gt;= threshold) { rehash(); tab = table; index = (hash &amp; 0x7FFFFFFF) % tab.length; } //将新的key-value对插入到tab[index]处（即链表的头结点） Entry&lt;K,V&gt; e = tab[index]; tab[index] = new Entry&lt;K,V&gt;(hash, key, value, e); count++; return null; } // 删除Hashtable中键为key的元素 public synchronized V remove(Object key) { Entry tab[] = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; //从table[index]链表中找出要删除的节点，并删除该节点。 //因为是单链表，因此要保留带删节点的前一个节点，才能有效地删除节点 for (Entry&lt;K,V&gt; e = tab[index], prev = null ; e != null ; prev = e, e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { modCount++; if (prev != null) { prev.next = e.next; } else { tab[index] = e.next; } count--; V oldValue = e.value; e.value = null; return oldValue; } } return null; } // 将“Map(t)”的中全部元素逐一添加到Hashtable中 public synchronized void putAll(Map&lt;? extends K, ? extends V&gt; t) { for (Map.Entry&lt;? extends K, ? extends V&gt; e : t.entrySet()) put(e.getKey(), e.getValue()); } // 清空Hashtable // 将Hashtable的table数组的值全部设为null public synchronized void clear() { Entry tab[] = table; modCount++; for (int index = tab.length; --index &gt;= 0; ) tab[index] = null; count = 0; } // 克隆一个Hashtable，并以Object的形式返回。 public synchronized Object clone() { try { Hashtable&lt;K,V&gt; t = (Hashtable&lt;K,V&gt;) super.clone(); t.table = new Entry[table.length]; for (int i = table.length ; i-- &gt; 0 ; ) { t.table[i] = (table[i] != null) ? (Entry&lt;K,V&gt;) table[i].clone() : null; } t.keySet = null; t.entrySet = null; t.values = null; t.modCount = 0; return t; } catch (CloneNotSupportedException e) { throw new InternalError(); } } public synchronized String toString() { int max = size() - 1; if (max == -1) return \"{}\"; StringBuilder sb = new StringBuilder(); Iterator&lt;Map.Entry&lt;K,V&gt;&gt; it = entrySet().iterator(); sb.append('{'); for (int i = 0; ; i++) { Map.Entry&lt;K,V&gt; e = it.next(); K key = e.getKey(); V value = e.getValue(); sb.append(key == this ? \"(this Map)\" : key.toString()); sb.append('='); sb.append(value == this ? \"(this Map)\" : value.toString()); if (i == max) return sb.append('}').toString(); sb.append(\", \"); } } // 获取Hashtable的枚举类对象 // 若Hashtable的实际大小为0,则返回“空枚举类”对象； // 否则，返回正常的Enumerator的对象。 private &lt;T&gt; Enumeration&lt;T&gt; getEnumeration(int type) { if (count == 0) { return (Enumeration&lt;T&gt;)emptyEnumerator; } else { return new Enumerator&lt;T&gt;(type, false); } } // 获取Hashtable的迭代器 // 若Hashtable的实际大小为0,则返回“空迭代器”对象； // 否则，返回正常的Enumerator的对象。(Enumerator实现了迭代器和枚举两个接口) private &lt;T&gt; Iterator&lt;T&gt; getIterator(int type) { if (count == 0) { return (Iterator&lt;T&gt;) emptyIterator; } else { return new Enumerator&lt;T&gt;(type, true); } } // Hashtable的“key的集合”。它是一个Set，没有重复元素 private transient volatile Set&lt;K&gt; keySet = null; // Hashtable的“key-value的集合”。它是一个Set，没有重复元素 private transient volatile Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet = null; // Hashtable的“key-value的集合”。它是一个Collection，可以有重复元素 private transient volatile Collection&lt;V&gt; values = null; // 返回一个被synchronizedSet封装后的KeySet对象 // synchronizedSet封装的目的是对KeySet的所有方法都添加synchronized，实现多线程同步 public Set&lt;K&gt; keySet() { if (keySet == null) keySet = Collections.synchronizedSet(new KeySet(), this); return keySet; } // Hashtable的Key的Set集合。 // KeySet继承于AbstractSet，所以，KeySet中的元素没有重复的。 private class KeySet extends AbstractSet&lt;K&gt; { public Iterator&lt;K&gt; iterator() { return getIterator(KEYS); } public int size() { return count; } public boolean contains(Object o) { return containsKey(o); } public boolean remove(Object o) { return Hashtable.this.remove(o) != null; } public void clear() { Hashtable.this.clear(); } } // 返回一个被synchronizedSet封装后的EntrySet对象 // synchronizedSet封装的目的是对EntrySet的所有方法都添加synchronized，实现多线程同步 public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() { if (entrySet==null) entrySet = Collections.synchronizedSet(new EntrySet(), this); return entrySet; } // Hashtable的Entry的Set集合。 // EntrySet继承于AbstractSet，所以，EntrySet中的元素没有重复的。 private class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; { public Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() { return getIterator(ENTRIES); } public boolean add(Map.Entry&lt;K,V&gt; o) { return super.add(o); } // 查找EntrySet中是否包含Object(0) // 首先，在table中找到o对应的Entry链表 // 然后，查找Entry链表中是否存在Object public boolean contains(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry entry = (Map.Entry)o; Object key = entry.getKey(); Entry[] tab = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry e = tab[index]; e != null; e = e.next) if (e.hash==hash &amp;&amp; e.equals(entry)) return true; return false; } // 删除元素Object(0) // 首先，在table中找到o对应的Entry链表 // 然后，删除链表中的元素Object public boolean remove(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;K,V&gt; entry = (Map.Entry&lt;K,V&gt;) o; K key = entry.getKey(); Entry[] tab = table; int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index], prev = null; e != null; prev = e, e = e.next) { if (e.hash==hash &amp;&amp; e.equals(entry)) { modCount++; if (prev != null) prev.next = e.next; else tab[index] = e.next; count--; e.value = null; return true; } } return false; } public int size() { return count; } public void clear() { Hashtable.this.clear(); } } // 返回一个被synchronizedCollection封装后的ValueCollection对象 // synchronizedCollection封装的目的是对ValueCollection的所有方法都添加synchronized，实现多线程同步 public Collection&lt;V&gt; values() { if (values==null) values = Collections.synchronizedCollection(new ValueCollection(), this); return values; } // Hashtable的value的Collection集合。 // ValueCollection继承于AbstractCollection，所以，ValueCollection中的元素可以重复的。 private class ValueCollection extends AbstractCollection&lt;V&gt; { public Iterator&lt;V&gt; iterator() { return getIterator(VALUES); } public int size() { return count; } public boolean contains(Object o) { return containsValue(o); } public void clear() { Hashtable.this.clear(); } } // 重新equals()函数 // 若两个Hashtable的所有key-value键值对都相等，则判断它们两个相等 public synchronized boolean equals(Object o) { if (o == this) return true; if (!(o instanceof Map)) return false; Map&lt;K,V&gt; t = (Map&lt;K,V&gt;) o; if (t.size() != size()) return false; try { // 通过迭代器依次取出当前Hashtable的key-value键值对 // 并判断该键值对，存在于Hashtable中。 // 若不存在，则立即返回false；否则，遍历完“当前Hashtable”并返回true。 Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); while (i.hasNext()) { Map.Entry&lt;K,V&gt; e = i.next(); K key = e.getKey(); V value = e.getValue(); if (value == null) { if (!(t.get(key)==null &amp;&amp; t.containsKey(key))) return false; } else { if (!value.equals(t.get(key))) return false; } } } catch (ClassCastException unused) { return false; } catch (NullPointerException unused) { return false; } return true; } // 计算Entry的hashCode // 若 Hashtable的实际大小为0 或者 加载因子&lt;0，则返回0。 // 否则，返回“Hashtable中的每个Entry的key和value的异或值 的总和”。 public synchronized int hashCode() { int h = 0; if (count == 0 || loadFactor &lt; 0) return h; // Returns zero loadFactor = -loadFactor; // Mark hashCode computation in progress Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) for (Entry e = tab[i]; e != null; e = e.next) h += e.key.hashCode() ^ e.value.hashCode(); loadFactor = -loadFactor; // Mark hashCode computation complete return h; } // java.io.Serializable的写入函数 // 将Hashtable的“总的容量，实际容量，所有的Entry”都写入到输出流中 private synchronized void writeObject(java.io.ObjectOutputStream s) throws IOException { // Write out the length, threshold, loadfactor s.defaultWriteObject(); // Write out length, count of elements and then the key/value objects s.writeInt(table.length); s.writeInt(count); for (int index = table.length-1; index &gt;= 0; index--) { Entry entry = table[index]; while (entry != null) { s.writeObject(entry.key); s.writeObject(entry.value); entry = entry.next; } } } // java.io.Serializable的读取函数：根据写入方式读出 // 将Hashtable的“总的容量，实际容量，所有的Entry”依次读出 private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException { // Read in the length, threshold, and loadfactor s.defaultReadObject(); // Read the original length of the array and number of elements int origlength = s.readInt(); int elements = s.readInt(); // Compute new size with a bit of room 5% to grow but // no larger than the original size. Make the length // odd if it's large enough, this helps distribute the entries. // Guard against the length ending up zero, that's not valid. int length = (int)(elements * loadFactor) + (elements / 20) + 3; if (length &gt; elements &amp;&amp; (length &amp; 1) == 0) length--; if (origlength &gt; 0 &amp;&amp; length &gt; origlength) length = origlength; Entry[] table = new Entry[length]; count = 0; // Read the number of elements and then all the key/value objects for (; elements &gt; 0; elements--) { K key = (K)s.readObject(); V value = (V)s.readObject(); // synch could be eliminated for performance reconstitutionPut(table, key, value); } this.table = table; } private void reconstitutionPut(Entry[] tab, K key, V value) throws StreamCorruptedException { if (value == null) { throw new java.io.StreamCorruptedException(); } // Makes sure the key is not already in the hashtable. // This should not happen in deserialized version. int hash = key.hashCode(); int index = (hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { throw new java.io.StreamCorruptedException(); } } // Creates the new entry. Entry&lt;K,V&gt; e = tab[index]; tab[index] = new Entry&lt;K,V&gt;(hash, key, value, e); count++; } // Hashtable的Entry节点，它本质上是一个单向链表。 // 也因此，我们才能推断出Hashtable是由拉链法实现的散列表 private static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { // 哈希值 int hash; K key; V value; // 指向的下一个Entry，即链表的下一个节点 Entry&lt;K,V&gt; next; // 构造函数 protected Entry(int hash, K key, V value, Entry&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } protected Object clone() { return new Entry&lt;K,V&gt;(hash, key, value, (next==null ? null : (Entry&lt;K,V&gt;) next.clone())); } public K getKey() { return key; } public V getValue() { return value; } // 设置value。若value是null，则抛出异常。 public V setValue(V value) { if (value == null) throw new NullPointerException(); V oldValue = this.value; this.value = value; return oldValue; } // 覆盖equals()方法，判断两个Entry是否相等。 // 若两个Entry的key和value都相等，则认为它们相等。 public boolean equals(Object o) { if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; return (key==null ? e.getKey()==null : key.equals(e.getKey())) &amp;&amp; (value==null ? e.getValue()==null : value.equals(e.getValue())); } public int hashCode() { return hash ^ (value==null ? 0 : value.hashCode()); } public String toString() { return key.toString()+\"=\"+value.toString(); } } private static final int KEYS = 0; private static final int VALUES = 1; private static final int ENTRIES = 2; // Enumerator的作用是提供了“通过elements()遍历Hashtable的接口” 和 “通过entrySet()遍历Hashtable的接口”。 private class Enumerator&lt;T&gt; implements Enumeration&lt;T&gt;, Iterator&lt;T&gt; { // 指向Hashtable的table Entry[] table = Hashtable.this.table; // Hashtable的总的大小 int index = table.length; Entry&lt;K,V&gt; entry = null; Entry&lt;K,V&gt; lastReturned = null; int type; // Enumerator是 “迭代器(Iterator)” 还是 “枚举类(Enumeration)”的标志 // iterator为true，表示它是迭代器；否则，是枚举类。 boolean iterator; // 在将Enumerator当作迭代器使用时会用到，用来实现fail-fast机制。 protected int expectedModCount = modCount; Enumerator(int type, boolean iterator) { this.type = type; this.iterator = iterator; } // 从遍历table的数组的末尾向前查找，直到找到不为null的Entry。 public boolean hasMoreElements() { Entry&lt;K,V&gt; e = entry; int i = index; Entry[] t = table; /* Use locals for faster loop iteration */ while (e == null &amp;&amp; i &gt; 0) { e = t[--i]; } entry = e; index = i; return e != null; } // 获取下一个元素 // 注意：从hasMoreElements() 和nextElement() 可以看出“Hashtable的elements()遍历方式” // 首先，从后向前的遍历table数组。table数组的每个节点都是一个单向链表(Entry)。 // 然后，依次向后遍历单向链表Entry。 public T nextElement() { Entry&lt;K,V&gt; et = entry; int i = index; Entry[] t = table; /* Use locals for faster loop iteration */ while (et == null &amp;&amp; i &gt; 0) { et = t[--i]; } entry = et; index = i; if (et != null) { Entry&lt;K,V&gt; e = lastReturned = entry; entry = e.next; return type == KEYS ? (T)e.key : (type == VALUES ? (T)e.value : (T)e); } throw new NoSuchElementException(\"Hashtable Enumerator\"); } // 迭代器Iterator的判断是否存在下一个元素 // 实际上，它是调用的hasMoreElements() public boolean hasNext() { return hasMoreElements(); } // 迭代器获取下一个元素 // 实际上，它是调用的nextElement() public T next() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); return nextElement(); } // 迭代器的remove()接口。 // 首先，它在table数组中找出要删除元素所在的Entry， // 然后，删除单向链表Entry中的元素。 public void remove() { if (!iterator) throw new UnsupportedOperationException(); if (lastReturned == null) throw new IllegalStateException(\"Hashtable Enumerator\"); if (modCount != expectedModCount) throw new ConcurrentModificationException(); synchronized(Hashtable.this) { Entry[] tab = Hashtable.this.table; int index = (lastReturned.hash &amp; 0x7FFFFFFF) % tab.length; for (Entry&lt;K,V&gt; e = tab[index], prev = null; e != null; prev = e, e = e.next) { if (e == lastReturned) { modCount++; expectedModCount++; if (prev == null) tab[index] = e.next; else prev.next = e.next; count--; lastReturned = null; return; } } throw new ConcurrentModificationException(); } } } private static Enumeration emptyEnumerator = new EmptyEnumerator(); private static Iterator emptyIterator = new EmptyIterator(); // 空枚举类 // 当Hashtable的实际大小为0；此时，又要通过Enumeration遍历Hashtable时，返回的是“空枚举类”的对象。 private static class EmptyEnumerator implements Enumeration&lt;Object&gt; { EmptyEnumerator() { } // 空枚举类的hasMoreElements() 始终返回false public boolean hasMoreElements() { return false; } // 空枚举类的nextElement() 抛出异常 public Object nextElement() { throw new NoSuchElementException(\"Hashtable Enumerator\"); } } // 空迭代器 // 当Hashtable的实际大小为0；此时，又要通过迭代器遍历Hashtable时，返回的是“空迭代器”的对象。 private static class EmptyIterator implements Iterator&lt;Object&gt; { EmptyIterator() { } public boolean hasNext() { return false; } public Object next() { throw new NoSuchElementException(\"Hashtable Iterator\"); } public void remove() { throw new IllegalStateException(\"Hashtable Iterator\"); } } } 几点总结针对Hashtable，我们同样给出几点比较重要的总结，但要结合与HashMap的比较来总结。1、二者的存储结构和解决冲突的方法都是相同的。 2、HashTable在不指定容量的情况下的默认容量为11，而HashMap为16，Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。 3、Hashtable中key和value都不允许为null，而HashMap中key和value都允许为null（key只能有一个为null，而value则可以有多个为null）。但是如果在Hashtable中有类似put(null,null)的操作，编译同样可以通过，因为key和value都是Object类型，但运行时会抛出NullPointerException异常，这是JDK的规范规定的。我们来看下ContainsKey方法和ContainsValue的源码： 12345678910111213141516171819202122232425262728293031323334353637383940// 判断Hashtable是否包含“值(value)” public synchronized boolean contains(Object value) { //注意，Hashtable中的value不能是null， // 若是null的话，抛出异常! if (value == null) { throw new NullPointerException(); } // 从后向前遍历table数组中的元素(Entry) // 对于每个Entry(单向链表)，逐个遍历，判断节点的值是否等于value Entry tab[] = table; for (int i = tab.length ; i-- &gt; 0 ;) { for (Entry&lt;K,V&gt; e = tab[i] ; e != null ; e = e.next) { if (e.value.equals(value)) { return true; } } } return false; } public boolean containsValue(Object value) { return contains(value); } // 判断Hashtable是否包含key public synchronized boolean containsKey(Object key) { Entry tab[] = table; /计算hash值，直接用key的hashCode代替 int hash = key.hashCode(); // 计算在数组中的索引值 int index = (hash &amp; 0x7FFFFFFF) % tab.length; // 找到“key对应的Entry(链表)”，然后在链表中找出“哈希值”和“键值”与key都相等的元素 for (Entry&lt;K,V&gt; e = tab[index] ; e != null ; e = e.next) { if ((e.hash == hash) &amp;&amp; e.key.equals(key)) { return true; } } return false; } 很明显，如果value为null，会直接抛出NullPointerException异常，但源码中并没有对key是否为null判断，有点小不解！不过NullPointerException属于RuntimeException异常，是可以由JVM自动抛出的，也许对key的值在JVM中有所限制吧。 4、Hashtable扩容时，将容量变为原来的2倍加1，而HashMap扩容时，将容量变为原来的2倍。 5、Hashtable计算hash值，直接用key的hashCode()，而HashMap重新计算了key的hash值，Hashtable在求hash值对应的位置索引时，用取模运算，而HashMap在求位置索引时，则用与运算，且这里一般先用hash&amp;0x7FFFFFFF后，再对length取模，&amp;0x7FFFFFFF的目的是为了将负的hash值转化为正值，因为hash值有可能为负数，而&amp;0x7FFFFFFF后，只有符号外改变，而后面的位都不变。","link":"/2018/11/23/javase/Hashtable源码剖析/"},{"title":"JavaSE源码分析-TreeSet源码剖析","text":"TreeSet简介TreeSet是基于TreeMap实现的，元素的顺序取决于元素自身的自然顺序或者在构造时提供的比较器。 TreeSet中的元素支持2种排序方式：自然排序 或者 根据创建TreeSet 时提供的 Comparator 进行排序。这取决于使用的构造方法。 对于add，remove，contains操作，保证log（n）的时间复杂度。 因为Set接口的定义根据equals方法，但是TreeSet接口约定元素的顺序基于compareTo或者compare方法，所以它们要保持一致性才能保证程序不会出错。 TreeSet是不同步的，运行在多线程环境下需要外部同步化或调用 它的iterator 方法返回的迭代器是fail-fast的。1SortedSet s = Collections.synchronizedSortedSet(new TreeSet(...)); 类定义12public class TreeSet&lt;E&gt; extends AbstractSet&lt;E&gt; implements NavigableSet&lt;E&gt;, Cloneable, java.io.Serializable 它继承于AbstractSet抽象类，实现了NavigableSet, Cloneable, java.io.Serializable接口。 TreeSet 继承于AbstractSet，所以它是一个Set集合，具有Set的属性和方法。 TreeSet 实现了NavigableSet接口，意味着它支持一系列的导航方法。比如查找与指定目标最匹配项。 TreeSet 实现了Cloneable接口，意味着它能被克隆。 TreeSet 实现了java.io.Serializable接口，意味着它支持序列化。 成员变量1234567/** * The backing map. */private transient NavigableMap&lt;E,Object&gt; m;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object(); TreeSet包含了两个重要的成员变量：m, PRESENT。 m是一个NavigableMap对象，TreeSet是由一个NavigableMa实例支持的。 PRESENT是一个static final Object对象，用来作为NavigableMap中的value值。 构造函数1234567891011121314151617181920212223242526//这个构造器不是导出API,在下面构造器有使用这个构造器TreeSet(NavigableMap&lt;E,Object&gt; m) { this.m = m;}//调用第一个的构造器,创建一个空的TreeSet,不提供比较器,使用元素自然顺序public TreeSet() { this(new TreeMap&lt;E,Object&gt;());}//调用第一个构造器,提供比较器,比较器由TreeMap维护,TreeSet本身没有比较器public TreeSet(Comparator&lt;? super E&gt; comparator) { this(new TreeMap&lt;&gt;(comparator));}//通过Collection的子类构造TreeSet,不提供比较器,使用元素的自然顺序public TreeSet(Collection&lt;? extends E&gt; c) { this(); addAll(c);}//通过SortedSet的子类构造TreeSet,SortedSet本身可能有比较器,如果有,使用该比较器,否则使用元素自然顺序public TreeSet(SortedSet&lt;E&gt; s) { this(s.comparator()); addAll(s);} add 和addAll12345678910111213141516171819202122public boolean add(E e) { return m.put(e, PRESENT)==null;}public boolean addAll(Collection&lt;? extends E&gt; c) { // Use linear-time version if applicable if (m.size()==0 &amp;&amp; c.size() &gt; 0 &amp;&amp; c instanceof SortedSet &amp;&amp; m instanceof TreeMap) { SortedSet&lt;? extends E&gt; set = (SortedSet&lt;? extends E&gt;) c; TreeMap&lt;E,Object&gt; map = (TreeMap&lt;E, Object&gt;) m; Comparator&lt;?&gt; cc = set.comparator(); Comparator&lt;? super E&gt; mc = map.comparator(); // 如果两个Comparator 相等，那么添加到map里面 if (cc==mc || (cc != null &amp;&amp; cc.equals(mc))) { map.addAllForTreeSet(set, PRESENT); return true; } } return super.addAll(c);} add 调用的是的map.put方法。由构造方法来看，其实是调用了TreeMap的put()方法（TreeMap的put重写了NavigableSet的put）。","link":"/2018/12/05/javase/TreeSet源码剖析/"},{"title":"JavaSE源码分析-TreeMap源码剖析","text":"TreeMap简介TreeMap的基本概念： TreeMap集合是基于红黑树（Red-Black tree）的 NavigableMap实现。该集合最重要的特点就是可排序，该映射根据其键的自然顺序进行排序，或者根据创建映射时提供的 Comparator 进行排序，具体取决于使用的构造方法。这句话是什么意思呢？就是说TreeMap可以对添加进来的元素进行排序，可以按照默认的排序方式，也可以自己指定排序方式。 类定义及类成员变量1234567891011121314151617181920212223242526public class TreeMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, Cloneable, java.io.Serializable{ // 比较器对象 private final Comparator&lt;? super K&gt; comparator; // 根节点 private transient Entry&lt;K,V&gt; root; // 集合大小 private transient int size = 0; // 树结构被修改的次数 private transient int modCount = 0; // 静态内部类用来表示节点类型 static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { K key; // 键 V value; // 值 Entry&lt;K,V&gt; left; // 指向左子树的引用（指针） Entry&lt;K,V&gt; right; // 指向右子树的引用（指针） Entry&lt;K,V&gt; parent; // 指向父节点的引用（指针） boolean color = BLACK; // 当前节点颜色 }} 类构造方法先来看下TreeMap的构造方法。TreeMap一共有4个构造方法。 无参构造方法123public TreeMap() { comparator = null;} 采用无参构造方法，不指定比较器，这时候，排序的实现要依赖key.compareTo()方法，因此key必须实现Comparable接口，并覆写其中的compareTo方法。 带有比较器的构造方法 123public TreeMap(Comparator&lt;? super K&gt; comparator) { this.comparator = comparator;} 采用带比较器的构造方法，这时候，排序依赖该比较器，key可以不用实现Comparable接口。 带Map的构造方法1234public TreeMap(Map&lt;? extends K, ? extends V&gt; m) { comparator = null; putAll(m);} 该构造方法同样不指定比较器，调用putAll方法将Map中的所有元素加入到TreeMap中。putAll的源码如下：123456789101112131415161718192021222324// 将map中的全部节点添加到TreeMap中public void putAll(Map&lt;? extends K, ? extends V&gt; map) { // 获取map的大小 int mapSize = map.size(); // 如果TreeMap的大小是0,且map的大小不是0,且map是已排序的“key-value对” if (size==0 &amp;&amp; mapSize!=0 &amp;&amp; map instanceof SortedMap) { Comparator c = ((SortedMap)map).comparator(); // 如果TreeMap和map的比较器相等； // 则将map的元素全部拷贝到TreeMap中，然后返回！ if (c == comparator || (c != null &amp;&amp; c.equals(comparator))) { ++modCount; try { buildFromSorted(mapSize, map.entrySet().iterator(), null, null); } catch (java._01_io.IOException cannotHappen) { } catch (ClassNotFoundException cannotHappen) { } return; } } // 调用AbstractMap中的putAll(); // AbstractMap中的putAll()又会调用到TreeMap的put() super.putAll(map);} 显然，如果Map里的元素是排好序的，就调用buildFromSorted方法来拷贝Map中的元素，这在下一个构造方法中会重点提及，而如果Map中的元素不是排好序的，就调用AbstractMap的putAll(map)方法，该方法源码如下：1234public void putAll(Map&lt;? extends K, ? extends V&gt; m) { for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) put(e.getKey(), e.getValue());} 很明显它是将Map中的元素一个个put（插入）到TreeMap中的，主要因为Map中的元素是无序存放的，因此要一个个插入到红黑树中，使其有序存放，并满足红黑树的性质。带有SortedMap的构造方法12345678910public TreeMap(SortedMap&lt;K, ? extends V&gt; m) { comparator = m.comparator(); try { buildFromSorted(m.size(), m.entrySet().iterator(), null, null); } catch (java._01_io.IOException cannotHappen) { } catch (ClassNotFoundException cannotHappen) { }} 首先将比较器指定为m的比较器，这取决于生成m时调用构造方法是否传入了指定的构造器，而后调用buildFromSorted方法，将SortedMap中的元素插入到TreeMap中，由于SortedMap中的元素师有序的，实际上它是根据SortedMap创建的TreeMap，将SortedMap中对应的元素添加到TreeMap中。 put()方法详解插入操作即对应TreeMap的put方法，put操作实际上只需按照二叉排序树的插入步骤来操作即可，插入到指定位置后，再做调整，使其保持红黑树的特性。put源码的实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public V put(K key, V value) { Entry&lt;K,V&gt; t = root; // 获取根节点 // 如果根节点为空，则该元素置为根节点 if (t == null) { compare(key, key); // type (and possibly null) check root = new Entry&lt;&gt;(key, value, null); size = 1; // 集合大小为1 modCount++; // 结构修改次数自增 return null; } int cmp; Entry&lt;K,V&gt; parent; Comparator&lt;? super K&gt; cpr = comparator; // 比较器对象 // 如果比较器对象不为空，也就是自定义了比较器 if (cpr != null) { do { // 循环比较并确定元素应插入的位置(也就是找到该元素的父节点) parent = t; // t就是root // 调用比较器对象的compare()方法，该方法返回一个整数 cmp = cpr.compare(key, t.key); if (cmp &lt; 0) // 待插入元素的key\"小于\"当前位置元素的key，则查询左子树 t = t.left; else if (cmp &gt; 0) // 待插入元素的key\"大于\"当前位置元素的key，则查询右子树 t = t.right; else // \"相等\"则替换其value。 return t.setValue(value); } while (t != null); } // 如果比较器对象为空，使用默认的比较机制 else { if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; // 取出比较器对象 do { // 同样是循环比较并确定元素应插入的位置(也就是找到该元素的父节点) parent = t; cmp = k.compareTo(t.key); // 同样调用比较方法并返回一个整数 if (cmp &lt; 0) // 待插入元素的key\"小于\"当前位置元素的key，则查询左子树 t = t.left; else if (cmp &gt; 0) // 待插入元素的key\"大于\"当前位置元素的key，则查询右子树 t = t.right; else // \"相等\"则替换其value。 return t.setValue(value); } while (t != null); } Entry&lt;K,V&gt; e = new Entry&lt;&gt;(key, value, parent); // 根据key找到父节点后新建一个节点 if (cmp &lt; 0) // 根据比较的结果来确定放在左子树还是右子树 parent.left = e; else parent.right = e; fixAfterInsertion(e); size++; // 集合大小+1 modCount++; // 集合结构被修改次数+1 return null;} 这里的fixAfterInsertion便是节点插入后对树进行调整的方法，这里不做介绍。 remove删除操作及对应TreeMap的deleteEntry方法，deleteEntry方法同样也只需按照二叉排序树的操作步骤实现即可，删除指定节点后，再对树进行调整即可。deleteEntry方法的实现源码如下：123456789101112131415161718192021222324252627282930313233343536373839404142// 删除“红黑树的节点p”private void deleteEntry(Entry&lt;K,V&gt; p) { modCount++; size--; if (p.left != null &amp;&amp; p.right != null) { Entry&lt;K,V&gt; s = successor (p); p.key = s.key; p.value = s.value; p = s; } Entry&lt;K,V&gt; replacement = (p.left != null ? p.left : p.right); if (replacement != null) { replacement.parent = p.parent; if (p.parent == null) root = replacement; else if (p == p.parent.left) p.parent.left = replacement; else p.parent.right = replacement; p.left = p.right = p.parent = null; if (p.color == BLACK) fixAfterDeletion(replacement); } else if (p.parent == null) { root = null; } else { if (p.color == BLACK) fixAfterDeletion(p); if (p.parent != null) { if (p == p.parent.left) p.parent.left = null; else if (p == p.parent.right) p.parent.right = null; p.parent = null; } }} 后面的fixAfterDeletion方法便是节点删除后对树进行调整的方法，这里不做介绍。 get()一帮以getEntry()方法为基础的获取元素的方法，其中包括containsKey()，get()等。12345678910111213141516171819202122232425public V get(Object key) { Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value);}final Entry&lt;K,V&gt; getEntry(Object key) { // 如果有自定义比较器对象，就按照自定义规则遍历二叉树 if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings(\"unchecked\") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; while (p != null) { // 按照默认比较规则遍历二叉树 int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; } return null;} getFirstEntry()，getLastEntry()一帮以getFirstEntry()，getLastEntry()为基础的获取头和尾元素的方法，其中包括：firstKey()，lastKey()；firstEntry()，lastEntry()；pollFirstEntry()，pollLastEntry()；123456789101112131415final Entry&lt;K,V&gt; getFirstEntry() { // 获取第一个元素也就是最小的元素，一直遍历左子树 Entry&lt;K,V&gt; p = root; if (p != null) while (p.left != null) p = p.left; return p;}final Entry&lt;K,V&gt; getLastEntry() { // 获取最后个元素也就是最大的元素，一直遍历右子树 Entry&lt;K,V&gt; p = root; if (p != null) while (p.right != null) p = p.right; return p;}","link":"/2018/12/05/javase/TreeMap源码剖析/"},{"title":"Nginx在Linux下的安装","text":"系统平台：CentOS release 6.6 (Final) 64位。 安装编译工具及库文件1yum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 首先要安装 PCREPCRE 作用是让 Nginx 支持 Rewrite 功能。 1、下载 PCRE 安装包，下载地址： http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz1[root@ngnix src]# wget http://downloads.sourceforge.net/project/pcre/pcre/8.35/pcre-8.35.tar.gz 2、解压安装包:1[root@ngnix src]# tar zxvf pcre-8.35.tar.gz 3、进入安装包目录1[root@ngnix src]# cd pcre-8.35 4、编译安装12[root@ngnix pcre-8.35]# ./configure[root@ngnix pcre-8.35]# make &amp;&amp; make install 5、查看pcre版本1[root@ngnix pcre-8.35]# pcre-config --version 安装Nginx1、下载 Nginx，下载地址：http://nginx.org/download/nginx-1.6.2.tar.gz1[root@ngnix src]# wget http://nginx.org/download/nginx-1.6.2.tar.gz 2、解压安装包1[root@ngnix src]# tar zxvf nginx-1.6.2.tar.gz 3、进入安装包目录1[root@ngnix src]# cd nginx-1.6.2 4、编译安装123[root@ngnix nginx-1.6.2]# ./configure --prefix=/usr/local/webserver/nginx --with-http_stub_status_module --with-http_ssl_module --with-pcre=/usr/local/src/pcre-8.35[root@ngnix nginx-1.6.2]# make[root@ngnix nginx-1.6.2]# make install 常用编译选项说明: nginx大部分常用模块，编译时./configure –help以–without开头的都默认安装。 –prefix=PATH ： 指定nginx的安装目录。默认 /usr/local/nginx –conf-path=PATH ： 设置nginx.conf配置文件的路径。nginx允许使用不同的配置文件启动，通过命令行中的-c选项。默认为prefix/conf/nginx.conf –user=name： 设置nginx工作进程的用户。安装完成后，可以随时在nginx.conf配置文件更改user指令。默认的用户名是nobody。–group=name类似 –with-pcre ： 设置PCRE库的源码路径，如果已通过yum方式安装，使用–with-pcre自动找到库文件。使用–with-pcre=PATH时，需要从PCRE网站下载pcre库的源码（版本4.4 – 8.30）并解压，剩下的就交给Nginx的./configure和make来完成。perl正则表达式使用在location指令和 ngx_http_rewrite_module模块中。 –with-zlib=PATH ： 指定 zlib（版本1.1.3 – 1.2.5）的源码解压目录。在默认就启用的网络传输压缩模块ngx_http_gzip_module时需要使用zlib 。 –with-http_ssl_module ： 使用https协议模块。默认情况下，该模块没有被构建。前提是openssl与openssl-devel已安装 –with-http_stub_status_module ： 用来监控 Nginx 的当前状态 –with-http_realip_module ： 通过这个模块允许我们改变客户端请求头中客户端IP地址值(例如X-Real-IP 或 X-Forwarded-For)，意义在于能够使得后台服务器记录原始客户端的IP地址 –add-module=PATH ： 添加第三方外部模块，如nginx-sticky-module-ng或缓存模块。每次添加新的模块都要重新编译（Tengine可以在新加入module时无需重新编译）5、查看nginx版本1[root@ngnix nginx-1.6.2]# /usr/local/webserver/nginx/sbin/nginx -v 到此，nginx安装完成。 Nginx 配置创建 Nginx 运行使用的用户 www：12[root@ngnix conf]# /usr/sbin/groupadd www [root@ngnix conf]# /usr/sbin/useradd -g www www 配置nginx.conf ，将/usr/local/webserver/nginx/conf/nginx.conf替换为以下内容1[root@ngnix conf]# cat /usr/local/webserver/nginx/conf/nginx.conf 显示如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374user www www;worker_processes 2; #设置值和CPU核心数一致error_log /usr/local/webserver/nginx/logs/nginx_error.log crit; #日志位置和日志级别pid /usr/local/webserver/nginx/nginx.pid;#Specifies the value for maximum file descriptors that can be opened by this process.worker_rlimit_nofile 65535;events{ use epoll; worker_connections 65535;}http{ include mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" $http_x_forwarded_for'; #charset gb2312; server_names_hash_bucket_size 128; client_header_buffer_size 32k; large_client_header_buffers 4 32k; client_max_body_size 8m; sendfile on; tcp_nopush on; keepalive_timeout 60; tcp_nodelay on; fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_http_version 1.0; gzip_comp_level 2; gzip_types text/html text/xml text/javascript application/x-javascript application/javascript text/css text/plain image/png image/jpeg image/gif; gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #下面是server虚拟主机的配置 server { listen 80;#监听端口 server_name localhost;#域名 index index.html index.htm index.php; root /usr/local/webserver/nginx/html;#站点目录 location ~ .*\\.(php|php5)?$ { #fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|ico)$ { expires 30d; # access_log off; } location ~ .*\\.(js|css)?$ { expires 15d; # access_log off; } access_log off; }} 检查配置文件ngnix.conf的正确性命令：1[root@ngnix conf]# /usr/local/webserver/nginx/sbin/nginx -t Nginx命令1234/usr/local/webserver/nginx/sbin/nginx # 启动 Nginx/usr/local/webserver/nginx/sbin/nginx -s reload # 重新载入配置文件/usr/local/webserver/nginx/sbin/nginx -s reopen # 重启 Nginx/usr/local/webserver/nginx/sbin/nginx -s stop # 停止 Nginx","link":"/2017/12/15/server/Nginx安装/"},{"title":"JavaSE源码分析-String源码分析","text":"Integer 类在对象中包装了一个基本类型 int 的值。Integer 类型的对象包含一个 int 类型的字段。 类定义1public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence{} 从该类的声明中我们可以看出String是final类型的，表示该类不能被继承，同时该类实现了三个接口：java.io.Serializable、 Comparable、 CharSequence 属性私有属性Integer类中定义了以下几个私有属性：1234private final char value[];private int hash;private static final long serialVersionUID = -6849794470754667710L;private static final ObjectStreamField[] serialPersistentFields = new ObjectStreamField[0]; value[]这是一个字符数组，并且是final类型，他用于存储字符串内容，从fianl这个关键字中我们可以看出，String的内容一旦被初始化了是不能被更改的。 虽然有这样的例子： String s = “a”; s = “b” 但是，这并不是对s的修改，而是重新指向了新的字符串， 从这里我们也能知道，String其实就是用char[]实现的。 hash缓存字符串的hash Code，默认值为 0 因为String实现了Serializable接口，所以支持序列化和反序列化支持。Java的序列化机制是通过在运行时判断类的serialVersionUID来验证版本一致性的。在进行反序列化时，JVM会把传来的字节流中的serialVersionUID与本地相应实体（类）的serialVersionUID进行比较，如果相同就认为是一致的，可以进行反序列化，否则就会出现序列化版本不一致的异常(InvalidCastException)。 12345678public class StringLearning { public static void main(String[] args) { String s1 = \"abc\"; System.out.println(s1); String s2 = new String(\"edf\"); System.out.println(s2); }} 上面代码，编译后，我们通过jdk自带的javap命令工具对StringLearning.class 进行分析javap -v StringLearning.class12345678910111213141516171819public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=3, args_size=1 0: ldc #2 // String abc 2: astore_1 3: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 6: aload_1 7: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 10: new #5 // class java/lang/String 13: dup 14: ldc #6 // String edf 16: invokespecial #7 // Method java/lang/String.\"&lt;init&gt;\":(Ljava/lang/String;)V 19: astore_2 20: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 23: aload_2 24: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 27: return ldc表示将一个常量加载到操作数栈。 #2 从常量池中取出”abc”的引用，加载到操作数栈中 。在编译启动过程中,字符串字面量就会被存入常量池astore_1 保存一个变量，就是s116: invokespecial #7 是调用String的构造方法。 由以上可以知道，String s1 = “abc”中s1只是引用了常量池的值。String s2 = new String(“edf”);的s2是指向了一个String对象。那么：1234String s1 = \"abc\";String s2 = \"abc\";String s3 = new String(\"abc\");String s4 = new String(\"abc\"); s1 == s2 答案：trues1 == s3 答案：falses3 == s4 答案：falses1.equals(s2) 答案：trues1.equals(s3) 答案：trues3.equals(s4) 答案：true 方法构造方法String类作为一个java.lang包中比较常用的类,自然有很多重载的构造方法.在这里介绍几种典型的构造方法:1234public String(String original) { this.value = original.value; this.hash = original.hash;} 我们知道，其实String就是使用字符数组（char[]）实现的。所以我们可以使用一个字符数组来创建一个String，那么这里值得注意的是，当我们使用字符数组创建String的时候，会用到Arrays.copyOf方法和Arrays.copyOfRange方法。这两个方法是将原有的字符数组中的内容逐一的复制到String中的字符数组中。同样，我们也可以用一个String类型的对象来初始化一个String。这里将直接将源String中的value和hash两个属性直接赋值给目标String。因为String一旦定义之后是不可以改变的，所以也就不用担心改变源String的值会影响到目标String的值。 当然，在使用字符数组来创建一个新的String对象的时候，不仅可以使用整个字符数组，也可以使用字符数组的一部分，只要多传入两个参数int offset和int count就可以了。123public String(char value[]) { this.value = Arrays.copyOf(value, value.length);} 在Java中，String实例中保存有一个char[]字符数组，char[]字符数组是以unicode码来存储的，String 和 char 为内存形式，byte是网络传输或存储的序列化形式。所以在很多传输和存储的过程中需要将byte[]数组和String进行相互转化。所以，String提供了一系列重载的构造方法来将一个字符数组转化成String，提到byte[]和String之间的相互转换就不得不关注编码问题。String(byte[] bytes, Charset charset)是指通过charset来解码指定的byte数组，将其解码成unicode的char[]数组，够造成新的String。 这里的bytes字节流是使用charset进行编码的，想要将他转换成unicode的char[]数组，而又保证不出现乱码，那就要指定其解码方式 同样使用字节数组来构造String也有很多种形式，按照是否指定解码方式分的话可以分为两种：12345String(byte bytes[]) String(byte bytes[], int offset, int length)String(byte bytes[], Charset charset)String(byte bytes[], String charsetName)String(byte bytes[], int offset, int length, Charset charset)String(byte bytes[], int offset, int length, String charsetName) 如果我们在使用byte[]构造String的时候，使用的是下面这四种构造方法(带有charsetName或者charset参数)的一种的话，那么就会使用StringCoding.decode方法进行解码，使用的解码的字符集就是我们指定的charsetName或者charset。 我们在使用byte[]构造String的时候，如果没有指明解码使用的字符集的话，那么StringCoding的decode方法首先调用系统的默认编码格式，如果没有指定编码格式则默认使用ISO-8859-1编码格式进行编码操作。主要体现代码如下：123456789101112131415161718192021static char[] decode(byte[] ba, int off, int len) { String csn = Charset.defaultCharset().name(); try { // use charset name decode() variant which provides caching. return decode(csn, ba, off, len); } catch (UnsupportedEncodingException x) { warnUnsupportedCharset(csn); } try { return decode(\"ISO-8859-1\", ba, off, len); } catch (UnsupportedEncodingException x) { // If this code is hit during VM initialization, MessageUtils is // the only way we will be able to get any kind of error message. MessageUtils.err(\"ISO-8859-1 charset not available: \" + x.toString()); // If we can not find ISO-8859-1 (a required encoding) then things // are seriously wrong with the installation. System.exit(1); return null; }} 使用StringBuffer和StringBuider构造一个String作为String的两个“兄弟”，StringBuffer和StringBuider也可以被当做构造String的参数。123456789public String(StringBuffer buffer) { synchronized(buffer) { this.value = Arrays.copyOf(buffer.getValue(), buffer.length()); }}public String(StringBuilder builder) { this.value = Arrays.copyOf(builder.getValue(), builder.length());} 当然，这两个构造方法是很少用到的，至少我从来没有使用过，因为当我们有了StringBuffer或者StringBuilfer对象之后可以直接使用他们的toString方法来得到String。关于效率问题，Java的官方文档有提到说使用StringBuilder的toString方法会更快一些，原因是StringBuffer的toString方法是synchronized的，在牺牲了效率的情况下保证了线程安全。123456 public String toString() { // Create a copy, don't share the array return new String(value, 0, count); }this.value = Arrays.copyOfRange(value, offset, offset+count); 其他方法","link":"/2018/10/23/javase/String源码分析/"},{"title":"符号++的原理","text":"我们在使用java程序时，会使用带 i++ 这样一个表达式，那么他的底层原理是什么呢？ 先来看下面这段代码：12345678910111213141516171819202122232425public class IntegerPlusPlusLearning implements Runnable{ public static Integer i = new Integer(0); @Override public void run() { while (true){ synchronized (i){ if(i &lt; 200){ i++; System.out.println(i); }else{ break; } } } } public static void main(String[] args) { Thread t1 = new Thread(new IntegerPlusPlusLearning()); Thread t2 = new Thread(new IntegerPlusPlusLearning()); t1.start(); t2.start(); }} 期待的运行结果：12345123...200 顺序 输出 i=1到i=200。可惜，这是错误的答案。 它的结果可能会出现无序的，或者重复、缺少的情况。 底层原理分析1、 分析编译后的IntegerPlusPlusLearning.class文件，发现 i++ 在虚拟机中的执行原理。 通过jdk自带的javap命令工具，对IntegerPlusPlusLearning.class 进行分析javap -v IntegerPlusPlusLearning.class 可以看到输出内容中(如下图)，JVM执行 i++ 的内部逻辑。12345678910111213141516171819202122232425public void run(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=5, args_size=1 0: getstatic #2 // Field i:Ljava/lang/Integer; 3: dup 4: astore_1 5: monitorenter 6: getstatic #2 // Field i:Ljava/lang/Integer; 9: invokevirtual #3 // Method java/lang/Integer.intValue:()I 12: sipush 200 15: if_icmpge 52 18: getstatic #2 // Field i:Ljava/lang/Integer; 21: astore_2 22: getstatic #2 // Field i:Ljava/lang/Integer; 25: invokevirtual #3 // Method java/lang/Integer.intValue:()I 28: iconst_1 29: iadd 30: invokestatic #4 // Method java/lang/Integer.valueOf:(I)Ljava/lang/Integer; 33: dup 34: putstatic #2 // Field i:Ljava/lang/Integer; 37: astore_3 38: aload_2 39: pop 22: getstatic —– 获取i的值，Integer类型。25: invokevirtual —– 这里表示调用了，Integer.intValue29: iadd —– 表示i加130: invokestatic —– 调用Integer.valueOf34: putstatic —– 赋值 这个逻辑，通过代码表达出来就是这句“i = Integer.valueOf(i.intValue() + 1)；” 2、 查看Integer的源码，当变量i的值发生变化后，发现 Integer.valueOf 每次都是新对象。123456/** Integer源码取自jdk1.8 */public static Integer valueOf(int i) { if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);} 所以最后，代码中 i 变量，是一个Integer对象，当代码中两个线程执行 i++时，实际运行时的 i++ 的实现逻辑是这样的：1i = Integer.valueOf(i.intValue() + 1)； 而Integer.valueOf每次是返回一个新的Integer对象，所以，我们的synchronized实际上没有起到你预想中的效果。","link":"/2018/11/08/javase/符号++的原理/"},{"title":"mysql安装成服务及自启动","text":"windows1、解压该压缩包，生成3分tomcat 分别命名为 tomcat1,tomcat2,tomcat3 2、进入tomcat1/conf/目录，修改server.xml的端口。 3、进入tomcat1/bin目录，修改 service.bat 4、修改SERVICE_NAME，如下123rem Set default Service nameset SERVICE_NAME=Tomcat7set DISPLAYNAME=Apache Tomcat 7.0 %SERVICE_NAME% 5、打开CMD命令控制台，进入tomcat1/bin目录，执行服务安装命令1service.bat install 注意：不要在环境变量设置CATALINA_HOME和CATALINA_HOME，否则无法生效！ 同理安装其他Tomcat。 PS：删除服务1service.bat uninstall linux简单自启动：1vim /etc/rc.local 在 exit 0 之前添加启动命令：1/home/tomcat/bin/startup.sh","link":"/2017/04/20/server/tomcat安装成服务及自启动/"},{"title":"SSL-https配置","text":"主流证书格式介绍一般来说，主流的Web服务软件，通常都基于两种基础密码库：OpenSSL和Java。 Tomcat、Weblogic、JBoss等，使用Java提供的密码库。通过Java的Keytool工具，生成Java Keystore（JKS）格式的证书文件。 Apache、Nginx等，使用OpenSSL提供的密码库，生成PEM、KEY、CRT等格式的证书文件。 BM的产品，如Websphere、IBM Http Server（IHS）等，使用IBM产品自带的iKeyman工具，生成KDB格式的证书文件。 微软Windows Server中的Internet Information Services（IIS），使用Windows自带的证书库生成PFX格式的证书文件。 如果您在工作中遇到带有后缀扩展名的证书文件，可以简单用如下方法区分： .DER .CER : 这样的证书文件是二进制格式，只含有证书信息，不包含私钥。 .CRT : 这样的文件可以是二进制格式，也可以是文本格式，一般均为文本格式，功能与.DER/*.CER相同。 .PEM : 一般是文本格式，可以放证书或私钥，或者两者都包含。 .PEM如果只包含私钥，那一般用 *.KEY代替。 .PFX .P12 是二进制格式，同时含证书和私钥，一般有密码保护。 怎么判断是文本格式还是二进制？ 用记事本打开，如果是规则的数字字母，如—–BEGIN CERTIFICATE—–MIIE5zCCA8+gAwIBAgIQN+whYc2BgzAogau0dc3PtzANBgkqh……—–END CERTIFICATE—–就是文本的，上面的BEGIN CERTIFICATE，说明这是一个证书如果是—–BEGIN RSA PRIVATE KEY—–，说明这是一个私钥 这些证书格式之间是可以互相转换的以下提供了一些证书之间的转换方法： 将JKS转换成PFX 可以使用Keytool工具，将JKS格式转换为PFX格式。 keytool -importkeystore -srckeystore D:\\server.jks -destkeystore D:\\server.pfx -srcstoretype JKS -deststoretype PKCS12 将PFX转换为JKS 可以使用Keytool工具，将PFX格式转换为JKS格式。 keytool -importkeystore -srckeystore D:\\server.pfx -destkeystore D:\\server.jks -srcstoretype PKCS12 -deststoretype JKS 将PEM/KEY/CRT转换为PFX 使用OpenSSL工具，可以将密钥文件KEY和公钥文件CRT转化为PFX文件。 将密钥文件KEY和公钥文件CRT放到OpenSSL目录下，打开OpenSSL执行以下命令： openssl pkcs12 -export -out server.pfx -inkey server.key -in server.crt 将PFX转换为PEM/KEY/CRT 使用OpenSSL工具，可以将PFX文件转化为密钥文件KEY和公钥文件CRT。 将PFX文件放到OpenSSL目录下，打开OpenSSL执行以下命令： openssl pkcs12 -in server.pfx -nodes -out server.pem openssl rsa -in server.pem -out server.key openssl x509 -in server.pem -out server.crt 请注意 此步骤是专用于使用keytool生成私钥和CSR申请证书，并且获取到pem格式证书公钥的情况下做分离私钥使用的，所以在实际部署证书时请使用此步骤分离出来的私钥和申请下来的公钥证书做匹配使用。 云盾证书服务统一使用 PEM 格式的数字证书文件。 pem证书转为jks证书第一步：pem(需要私钥) 转为 .pfx1openssl pkcs12 -export -out server.pfx -inkey private.key -in server.pem 第二步：.pfx 转为 .jks1keytool -importkeystore -srckeystore server.pfx -destkeystore server.jks -srcstoretype PKCS12 -deststoretype JKS tomcat配置1234567&lt;Connector protocol=\"org.apache.coyote.http11.Http11NioProtocol\" port=\"443\" SSLEnabled=\"true\" maxThreads=\"150\" scheme=\"https\" secure=\"true\" keystoreFile=\"/home/websoft/key/server.jks\" keystorePass=\"123456\" clientAuth=\"false\" sslProtocol=\"TLS\" ciphers=\"TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA256\" /&gt; ngnix配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869server { listen 80; listen 443 ssl; # ssl on; #在同一个server{}里配置同时开启http和https时，不需要开启此项！ server_name dev.cmop.mgtv.com; root /home/websoft/nginx/html; ssl_certificate \"/home/websoft/key/dev/full_chain.pem\"; ssl_certificate_key \"/home/websoft/key/dev/private.key\"; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1;#指定密码为openssl支持的格式 ssl_ciphers HIGH:!aNULL:!MD5;#密码加密方式 ssl_prefer_server_ciphers on; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { }}server { listen 80; listen 443 ssl; # ssl on; #在同一个server{}里配置同时开启http和https时，不需要开启此项！ server_name book.cmop.mgtv.com; root /home/websoft/nginx/html; ssl_certificate \"/home/websoft/key/book/full_chain.pem\"; ssl_certificate_key \"/home/websoft/key/book/private.key\"; ssl_session_timeout 5m; ssl_protocols SSLv2 SSLv3 TLSv1;#指定密码为openssl支持的格式 ssl_ciphers HIGH:!aNULL:!MD5;#密码加密方式 ssl_prefer_server_ciphers on; location / { } error_page 404 /404.html; location = /40x.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { }}server { listen 80; listen 443 ssl; server_name mango.m.lrts.me; ssl_certificate \"/data/nginx/key/mango.m.lrts.me_bundle.crt\"; ssl_certificate_key \"/data/nginx/key/mango.m.lrts.me.key\"; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #启用TLS1.1、TLS1.2要求OpenSSL1.0.1及以上版本，若您的OpenSSL版本低于要求，请使用 ssl_protocols TLSv1; ssl_ciphers HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM; ssl_prefer_server_ciphers on; ssl_session_cache shared:SSL:1m; location / { proxy_set_header Host $host; proxy_set_header X-Forwarded-For $remote_addr; index index.html; proxy_pass http://127.0.0.1:3000/; }}","link":"/2018/01/08/server/SSL-https配置/"},{"title":"Nginx配置文件详解","text":"Nginx配置文件nginx.conf中文详解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335#定义Nginx运行的用户和用户组user www www;#nginx进程数，建议设置为等于CPU总核心数。worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]error_log /usr/local/nginx/logs/error.log info;#进程pid文件pid /usr/local/nginx/logs/nginx.pid;#指定进程可以打开的最大描述符：数目#工作模式与连接数上限#这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。#现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。#这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。worker_rlimit_nofile 65535;events{ #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #4096 #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误. open_file_cache_errors on;} #设定http服务器，利用它的反向代理功能提供负载均衡支持http{ #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; #负载均衡配置 upstream piao.jd.com { #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; } #nginx的upstream目前支持4种方式的分配 #1、轮询（默认） #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 #upstream bakend { # server 192.168.0.14; # server 192.168.0.15; #} #2、weight #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 #例如： #upstream bakend { # server 192.168.0.14 weight=10; # server 192.168.0.15 weight=10; #} #2、ip_hash #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 #例如： #upstream bakend { # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; #} #3、fair（第三方） #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 #upstream backend { # server server1; # server server2; # fair; #} #4、url_hash（第三方） #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 #upstream backend { # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; #} #tips: #upstream bakend{#定义负载均衡设备的Ip及设备状态}{ # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060; # server 127.0.0.1:7070 backup; #} #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; #每个设备的状态设置为: #1.down表示单前的server暂时不参与负载 #2.weight为weight越大，负载的权重就越大。 #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 #4.fail_timeout:max_fails次失败后，暂停的时间。 #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 #虚拟主机的配置 server { #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.jd.com jd.com; index index.html index.htm index.php; root /data/www/jd; #对******进行负载均衡 # ~ 波浪线表示执行一个正则匹配，区分大小写 # ~* 表示执行一个正则匹配，不区分大小写 # ^~ 表示普通字符匹配，如果该选项匹配，只匹配该选项，不匹配别的选项，一般用来匹配目录 # = 进行普通字符精确匹配 location ~ .*.(php|php5)?$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ { expires 10d; } #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ { expires 1h; } #日志格式设定 #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； #$remote_user：用来记录客户端用户名称； #$time_local： 用来记录访问时间与时区； #$request： 用来记录请求的url与http协议； #$status： 用来记录请求状态；成功是200， #$body_bytes_sent ：记录发送给客户端文件主体内容大小； #$http_referer：用来记录从那个页面链接访问过来的； #$http_user_agent：记录客户浏览器的相关信息； #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 log_format access '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" $http_x_forwarded_for'; #定义本虚拟主机的访问日志 access_log /usr/local/nginx/logs/host.access.log main; access_log /usr/local/nginx/logs/host.access.404.log log404; #对 \"/\" 启用反向代理 location / { proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 128k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #后端服务器连接的超时时间_发起握手等候响应超时时间 #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #后端服务器数据回传时间(代理发送超时) #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的设置 #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; } #设定查看Nginx状态的地址 location /NginxStatus { stub_status on; access_log on; auth_basic \"NginxStatus\"; auth_basic_user_file confpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 } #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; } #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| pdf|xls|mp3|wma)$ { expires 15d; } location ~ .*.(js|css)?$ { expires 1h; } }}######Nginx配置文件nginx.conf中文详解#####","link":"/2017/12/15/server/Nginx配置文件详解/"},{"title":"idea的使用","text":"1 修改对应的配置信息(缓存)地址由于我家里的电脑C盘被我设置得超级小,然后Idea默认的各种系统配置,最主要是缓存的地址,修改 ${idea.home}/bin/idea.properties 修改下面几个值. 123456789101112131415. #--------------------------------------------------------------------- 16. # Uncomment this option if you want to customize path to IDE config folder. Make sure you're using forward slashes 17. #--------------------------------------------------------------------- 18. idea.config.path=D:/dev_soft/IntelliJ IDEA 12.0.1/bin/.IntelliJIdea/config 19. 20. #--------------------------------------------------------------------- 21. # Uncomment this option if you want to customize path to IDE system folder. Make sure you're using forward slashes 22. #--------------------------------------------------------------------- 23. idea.system.path=D:/dev_soft/IntelliJ IDEA 12.0.1/bin/.IntelliJIdea/system 24. 25. #--------------------------------------------------------------------- 26. # Uncomment this option if you want to customize path to user installed plugins folder. Make sure you're using forward slashes 27. #--------------------------------------------------------------------- 28. idea.plugins.path=D:/dev_soft/IntelliJ IDEA 12.0.1/bin/.IntelliJIdea/config/plugins 2 修改快捷键 key/map 选择eclipse ,选择copy成自定义 (我还是习惯用eclipse的快捷键) 3 配置修改1、修改主题 File | Settings | Appearance &amp; Behavior | Appearance ： Theme选择 Darcula2、显示行号：Settings-&gt;Editor-&gt;Appearance标签项，勾选Show line numbers3、选择字体大小：File | Settings | Editor | Font 154、Tab换成字符串 ：File | Settings | Editor | Code Style | Java –&gt; Use tab charactor3、生成Serializable ID ，setting–&gt;Editor–&gt;Inspactions–&gt;Java | Serialization issues | Serializable class without ‘serialVersionUID’ 打上勾4、maven 工程 unable to read the metadata file for artifact 问题 :setting-&gt;maven-&gt;always update snapshot 打开,然后重新import change就搞定了. 4、代码TemplatesFile | Settings | Editor | File and Code Templates –&gt; Includes–&gt;File Header/** Company：MGTV User: huangmin DateTime: ${DATE} ${TIME}*/ File | Settings | Editor | Live Templates添加Templates group ，再添加 Live Template。private static final Logger LOGGER = LoggerFactory.getLogger($CLASS$.class);点击$CLASS$ ，点击edit variables，选择getClassName() 5、常用插件Mybatis自动转换对象插件 generateO2O 快捷键 alt+insert 快捷键提示插件Key Promoter大小写转换插件 UpperLowerCapitalize : 安装后快捷键alt+P全部大写 alt+L全部小写 alt+C开头字母大写查看maven的依赖树 Maven Helper 6、常用快捷键fori/sout/psvm+Tab.for+Tab.var+Tab Top #10切来切去：Ctrl+TabTop #9选你所想：Ctrl+WTop #8代码生成：Template/Postfix +TabTop #7发号施令：Ctrl+Shift+ATop #6无处藏身：Shift+ShiftTop #5自动完成：Ctrl+Shift+EnterTop #4创造万物：Alt+Insert 太难割舍，前三名并列吧！Top #1智能补全：Ctrl+Shift+SpaceTop #1自我修复：Alt+EnterTop #1重构一切：Ctrl+Shift+Alt+T","link":"/2017/12/05/tools/idea的使用/"},{"title":"markdown语法","text":"链接1.语法：1[淘宝网](http://www.taobao.com/) 2.例子：淘宝网 图片1.语法：1![图片标题](http://mat1.gtimg.com/pingjs/ext2020/qqindex2018/dist/img/qq_logo_2x.png) 2.例子： 标题1.语法：123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 tips：几个 # 就是几级标题，最小到六级 斜体1234 *这是斜体* *[这是斜体链接](http://www.taobao.com)* *斜体,[这是斜体链接](http://www.taobao.com)*tips：斜体和链接可以混用 为字体加颜色12345 这是&lt;label style=\"color:red\"&gt;红色&lt;/label&gt;字体 这是&lt;label style=\"color:green\"&gt;绿色&lt;/label&gt;字体 这是&lt;label style=\"color:yellow\"&gt;黄色&lt;/label&gt;字体 这是&lt;label style=\"color:blue\"&gt;蓝色&lt;/label&gt;字体&lt;label style=\"color:red\"&gt;tips:修改color为对应的颜色英文字母即可，复杂的颜色不要想了，况且大家也用不到&lt;/label&gt; 例子： 这是红色字体 这是绿色字体 这是黄色字体 这是蓝色字体 为自体加粗1**加粗**字体 例子： 加粗字体 Email1Email:&lt;yabing.zyb@alibaba-inc.com&gt; 例子： Email:yabing.zyb@alibaba-inc.com 无序排列123* list1* list2* list3 有序排列1231. list12. list23. list3 分割线123***---- - - - tips: 三种都一样 内容块1&gt; 这里的内容在内容块中 例子： 这里的内容在内容块中 代码块1234567```java public class Demo{ public static void main(String[] args) } }``` 例子： 12345public class Demo{ public static void main(String[] args){ }} 内容框1在上一行内容缩进的基础上再缩进四个空格 换行1需要换行&lt;br&gt;这是换行后的下一行 例子：需要换行这是换行后的下一行 中划线1~~中划线~~ 例子：中划线 添加脚注12这是脚注[^1][^1]: 这是脚注说明，会在文章的末尾显示. 这是脚注[^1][^1]: 这是脚注说明，会在文章的末尾显示. 表格默认表格：1234First Header | Second Header | Third Header------------ | ------------- | ------------Content Cell | Content Cell | Content CellContent Cell | Content Cell | Content Cell 例子： First Header Second Header Third Header Content Cell Content Cell Content Cell Content Cell Content Cell Content Cell 左右浮动表格：1234First Header | Second Header | Third Header:----------- | :-----------: | -----------:Left | Center | RightLeft | Center | Right 例子： First Header Second Header Third Header Left Center Right Left Center Right tips:默认向左对齐","link":"/2018/11/06/tools/markdown语法/"},{"title":"Too Many Open Files 错误和解决方案","text":"Nginx: 24: Too Many Open Files 错误和解决方案在我的nginx配置文件中获取以下错误日志:12010/04/16 13:24:16 [crit] 21974#0: *3188937 open() \"/usr/local/nginx/html/50x.html\" failed (24: Too many open files), client: 88.x.y.z, server: example.com, request: \"GET /file/images/background.jpg HTTP/1.1\", upstream: \"http://10.8.4.227:81//file/images/background.jpg\", host: \"example.com\" 像 CentOS / RHEL / Fedora Linux or UNIX这样的操作系统碰到这样的问题如何修复？ Linux / Unix 设置了软硬文件句柄和打开文件的数目，你可以使用’ulimit’命令来查看这些限制1su - nginx 要看这些值，请使用下面的命令：12ulimit -Hnulimit -Sn 内核级别的检查与设置在Linux系统级别上提高打开文件的限制 nginx服务器可以打开的文件数量受你操作系统的限制，你可以轻松地修复这个问题by setting or increasing system open file limits 。编辑文件／etc/sysctl.conf, 键入：1vi /etc/sysctl.conf 追加或者修改下面的行：1fs.file-max=70000 保存并关闭文件 系统级别的检查与设置编辑 /etc/security/limits.conf, 键入：1vi /etc/security/limits.conf 像下面这样为所有用户或者nginx用户设置软硬限制：12* soft nofile 10000* hard nofile 30000 保存并关闭，最后重新载入sysctl命令，以使以上改变生效：1sysctl -p nginx.conf参数规划与设置nginx worker_rlimit_nofile Option （在nginx级别上提高打开的文件句柄限制） nginx也有同样的限制，可以通过worker_rlimit_nofile来增加此限制数量。 来设置被nginx进程最大文件打开的数量,编辑nginx.conf文件，键入：1vi /usr/local/nginx/conf/nginx.conf （视你的配置文件的位置而定） 追加或者编辑：1worker_rlimit_nofile 30000; 保存并关闭文件，重新加载nginx配置，并重新执行开始查看软硬限制的命令：123su - nginxulimit -Hnulimit -Sn 输入示例： 3000010000 其实在国内的网站是上搜到的内容大都讲的差不多了，原先我也如此，但一直没有解决，最后发现最关键的参数没设置，故一直不能超过1024个active conntions. 通过设置此参数即可解决：1worker_rlimit_nofile 30000; 验证在合理规划以上3个层次的设置后，必须做验证： 1.验证nginx程序的限制1ps -ef |grep nginx 将得出的PID XXX带入下面1cat /proc/XXX/limits 查看Max open files 那一行 2.验证系统级别的限制1ulimit -n 3.验证内核级别的限制1cat /proc/sys/fs/file-max","link":"/2019/01/07/server/Too Many Open Files 错误和解决方案/"}],"tags":[{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"Oracle","slug":"Oracle","link":"/tags/Oracle/"},{"name":"memcached","slug":"memcached","link":"/tags/memcached/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"计划任务","slug":"计划任务","link":"/tags/计划任务/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"存储过程","slug":"存储过程","link":"/tags/存储过程/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"centos","slug":"centos","link":"/tags/centos/"},{"name":"防火墙","slug":"防火墙","link":"/tags/防火墙/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Server","slug":"Server","link":"/tags/Server/"},{"name":"源码分析","slug":"源码分析","link":"/tags/源码分析/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"IOC","slug":"IOC","link":"/tags/IOC/"},{"name":"Java线程","slug":"Java线程","link":"/tags/Java线程/"},{"name":"Thread","slug":"Thread","link":"/tags/Thread/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"JavaSE","slug":"JavaSE","link":"/tags/JavaSE/"},{"name":"ArrayList","slug":"ArrayList","link":"/tags/ArrayList/"},{"name":"HashMap","slug":"HashMap","link":"/tags/HashMap/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","link":"/tags/ConcurrentHashMap/"},{"name":"HashSet","slug":"HashSet","link":"/tags/HashSet/"},{"name":"Integer","slug":"Integer","link":"/tags/Integer/"},{"name":"LinkedList","slug":"LinkedList","link":"/tags/LinkedList/"},{"name":"Hashtable","slug":"Hashtable","link":"/tags/Hashtable/"},{"name":"TreeSet","slug":"TreeSet","link":"/tags/TreeSet/"},{"name":"TreeMap","slug":"TreeMap","link":"/tags/TreeMap/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"String","slug":"String","link":"/tags/String/"},{"name":"tomcat","slug":"tomcat","link":"/tags/tomcat/"},{"name":"ssl","slug":"ssl","link":"/tags/ssl/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"md","slug":"md","link":"/tags/md/"}],"categories":[{"name":"JavaSE源码分析","slug":"JavaSE源码分析","link":"/categories/JavaSE源码分析/"},{"name":"Spring源码分析","slug":"Spring源码分析","link":"/categories/Spring源码分析/"},{"name":"Java线程","slug":"Java线程","link":"/categories/Java线程/"}]}